Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5992788529396058,0.002460251451382982,0.049600921880374176,999.0,0.0,0.0,480717.54,1471360.7083999997,1212.9965821880949,2,14
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6267312908172606,0.0028547523925991753,0.05342988295513266,999.0,0.0,0.0,480595.26,1513777.0124,1230.3564574545053,2,11
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6064863991737366,0.0030289475398627198,0.05503587502586581,999.0,0.0,0.0,479524.58,1837564.9036,1355.568111014714,2,20
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.4454562592506408,0.0016956020733715094,0.041177689024173146,999.0,0.0,0.0,454415.36,1644560.6304,1282.4042382961777,5,5
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.481111500263214,0.001990887934771189,0.04461936726099093,999.0,0.0,0.0,476308.84,768573.2944,876.683120859527,5,5
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.27237543821334836,0.00017429087424363274,0.013201926914039206,999.0,0.0,0.0,80861.96,18063.3584,134.39999404761892,31,31
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.2583369469642639,0.00032766743204512637,0.018101586451057996,999.0,0.0,0.0,77595.32,19911.057600000004,141.10654697780683,31,33
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.191421320438385,0.00011805057609000756,0.010865108195043783,999.0,0.0,0.0,54723.42,20455.783600000002,143.02371691436355,31,31
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16787848711013795,0.00014241182462466781,0.01193364255475535,999.0,0.0,0.0,46976.28,21281.5216,145.88187550206501,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16177138328552246,0.0001328508671025702,0.01152609505004059,999.0,0.0,0.0,45897.24,18828.3024,137.21626142698977,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.16379663944244385,0.00010915624615392971,0.010447786662921947,999.0,0.0,0.0,45906.18,18872.1676,137.3760080945723,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5874560427665712,0.004745759061459984,0.06888946988807494,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.15436097383499145,0.00010907418960002247,0.010443858941982244,999.0,0.0,0.0,46358.96,22647.7184,150.4915891337453,31,31
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.29783196926116945,0.00038161912221696645,0.019535074154375928,999.0,0.0,0.0,86378.78,21073.831599999998,145.16828717044228,2,35
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.15246137857437134,8.071895746031146e-05,0.008984372958660579,999.0,0.0,0.0,45891.12,23157.7056,152.17656061299323,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.16902664422988892,4.5627997538593946e-05,0.006754849927170399,999.0,0.0,0.0,46281.68,21636.137600000005,147.09227579992094,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.1485314154624939,6.3786412534852845e-06,0.0025255972072928184,999.0,0.0,0.0,46335.56,16172.206400000001,127.16999017063735,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.16836159706115722,6.218105978755375e-05,0.007885496800300774,999.0,0.0,0.0,46371.76,24458.9824,156.39367762157138,31,31
No-discounting Q-learning,True,False,0.16183073282241822,0.00010742115296158661,0.010364417637358436,999.0,0.0,0.0,46400.2,25765.080000000005,160.51504602372952,31,31
"No-discounting, no stochastic approximation Q-learning",True,False,0.16491106748580933,0.00011557721750293124,0.010750684513226645,999.0,0.0,0.0,46759.24,26489.662399999997,162.75645117782582,31,31
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6354702639579772,0.003812819149773595,0.061748029521383065,999.0,0.0,0.0,480692.2,1411346.84,1188.0011952855941,2,11
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6343643522262574,0.002167908983509301,0.046560809523775476,999.0,0.0,0.0,480380.76,1420050.3824,1191.6586685792204,2,14
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6417269110679626,0.003231265127621299,0.056844218066759426,999.0,0.0,0.0,479965.66,1296599.9244000001,1138.6834171094265,2,10
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.4913298630714416,0.0024871888756418854,0.04987172420963491,999.0,0.0,0.0,454449.58,1882549.3836,1372.0602696674807,5,5
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5259540128707885,0.002957196906664854,0.05438011499311907,999.0,0.0,0.0,476252.68,1062962.3376,1031.0006486903876,5,5
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,0.2809337282180786,0.00024213641052872392,0.01556073296887791,999.0,0.0,0.0,80899.8,21643.4,147.1169602731106,3,33
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,0.2658014106750488,0.00022759231031918716,0.015086162875933269,999.0,0.0,0.0,77598.18,16847.687599999994,129.79864251986612,4,31
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1911730122566223,0.00014650275349123947,0.012103832182050422,999.0,0.0,0.0,54714.44,19781.6464,140.64724099675757,31,31
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16552067041397095,0.00013287787834371443,0.011527266733433144,999.0,0.0,0.0,46958.66,21295.284400000004,145.92903891960643,31,31
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.15755106687545775,8.820598435960961e-05,0.009391804105687554,999.0,0.0,0.0,45870.92,22757.473599999994,150.85580399838778,31,31
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.15780083894729613,9.307403839295033e-05,0.009647488709138266,999.0,0.0,0.0,45928.3,16458.51,128.29072452831497,31,31
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.2925712990760803,0.00018907439937431152,0.013750432697712154,999.0,0.0,0.0,86360.18,16452.967599999996,128.26912177137567,2,35
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1632140111923218,0.00010786039146794336,0.010385585754686318,999.0,0.0,0.0,45893.64,17168.8304,131.02988361438776,31,31
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5390323185920716,0.007211680415963537,0.08492161336175577,999.0,0.0,0.0,487252.38,1635553.5756000003,1278.887632124105,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.565806772708893,0.002763199116464983,0.05256614039916744,999.0,0.0,0.0,498309.6,178235.92,422.17996162774,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6069546627998352,0.005690151524405171,0.07543309303220418,999.0,0.0,0.0,499835.58,25643.543600000005,160.13601593645325,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.6125231885910034,0.003553653159606733,0.05961252519065715,999.0,0.0,0.0,499979.4,3231.320000000001,56.84470072047175,5,5
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5935091304779052,0.007460271021419748,0.08637286044481651,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.15828112840652467,0.00010732561464130299,0.010359807654647986,999.0,0.0,0.0,46370.02,16733.399599999997,129.35764221722656,31,31
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1625470209121704,0.0001279293633089992,0.011310586337984393,999.0,0.0,0.0,46251.34,25443.9244,159.51151807941645,31,31
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.16844231605529786,0.0002381907237862379,0.015433428776076879,999.0,0.0,0.0,46340.56,21134.566399999996,145.37732422905574,31,31
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.16429107904434204,0.00011961715025774423,0.010936962570007461,999.0,0.0,0.0,46362.58,21912.743599999998,148.02953624192708,31,31
cost-based no-discounting Q-learning,True,False,0.17142107963562012,9.567796312726384e-05,0.009781511290555454,999.0,0.0,0.0,46351.72,15650.6416,125.1025243550265,31,31
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.16729120969772338,0.00013080913777423008,0.01143718224801153,999.0,0.0,0.0,46725.98,23811.4796,154.3096873174202,31,31
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1706312608718872,0.00010065118275722396,0.010032506304868388,999.0,0.0,0.0,46729.4,23420.999999999996,153.03921066184313,31,31
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.8701869201660157,0.008790479204761231,0.09375755545427382,999.0,0.0,0.0,500000.0,0.0,0.0,31,31
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7262860655784606,0.0069485475127810045,0.08335794810802989,999.0,0.0,0.0,500000.0,0.0,0.0,2,14
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.756113083362579,0.04968291980181295,0.22289665722440288,999.0,0.0,0.0,1583984.54,887726034.5484002,29794.731657600143,31,31
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,5.885717513561249,0.036201439433867215,0.19026675861502243,999.0,0.0,0.0,1592742.46,479943142.46840006,21907.6046720859,31,31
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.14474086284637452,7.483583270116014e-05,0.008650770642038785,999.0,0.0,0.0,42170.0,0.0,0.0,31,31
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1952937865257263,0.004093023944744227,0.06397674534347794,0.0,0.0,0.0,400000.0,0.0,0.0,31,31
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.0666503643989563,2.1927113659666015e-05,0.004682639603862977,203.0,0.0,0.0,18290.0,0.0,0.0,31,31
Don't care Q-learning,True,False,1.802631709575653,0.018832693921844904,0.1372322626857289,14999.0,0.0,0.0,535596.47,962700401.6491001,31027.413711895166,31,41
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.31641206026077273,0.00046080591588481076,0.021466390378561802,999.0,0.0,0.0,74490.16,2334612.1344,1527.9437602215598,32,3096
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.32023274660110473,0.00033542463295872836,0.018314601632542497,999.0,0.0,0.0,73020.58,2150877.7436000006,1466.587107402762,31,2380
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.3070241951942444,0.0004661523510261474,0.02159056161905353,999.0,0.0,0.0,73209.75,2583982.9275,1607.4771934618545,32,827
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.41856908082962035,0.0005593087911784722,0.023649710171130475,999.0,0.0,0.0,98689.53,199426.7891,446.5722663802579,31,650
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.2522825074195862,0.00030315329716751755,0.017411297974806977,999.0,0.0,0.0,59813.14,156748.24040000004,395.9144357054944,31,270
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.2560517716407776,0.00029106173270458267,0.017060531430895774,999.0,0.0,0.0,60542.95,667080.5875,816.7500153045606,31,2221
Value Iteration,True,False,0.022350826263427735,7.866733086211751e-06,0.0028047697028832424,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.8,True,False,0.025019400119781494,8.144828971643391e-06,0.002853914674905925,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.6,True,False,0.024018981456756593,6.416299346426513e-07,0.0008010180613710601,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.5,True,False,0.027230212688446043,6.496707352761177e-06,0.00254886393374797,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.3,True,False,0.02496032476425171,5.310315236437188e-06,0.0023044121238262024,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.1,False,True,0.019478862285614015,3.3811853698523465e-06,0.0018387999809256978,25.0,0.0,0.0,25350.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.01,False,True,0.010605485439300536,2.3586738264555158e-06,0.0015357974561951572,13.0,0.0,0.0,13182.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.0001,False,True,0.006040172576904297,2.001078866987882e-06,0.001414594948028545,8.0,0.0,0.0,8112.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.00001,False,True,0.004238688945770263,1.87744637258902e-07,0.00043329509258575963,6.0,0.0,0.0,6084.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.000001,False,True,0.004419760704040527,4.2312230150400856e-07,0.0006504785173270587,6.0,0.0,0.0,6084.0,0.0,0.0,5,5
Stochastic Value Iteration,True,True,0.2953133845329285,0.0004769902430881472,0.021840106297546888,105.0,0.0,0.0,279090.0,0.0,0.0,31,44
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.24526187896728516,0.00048020478349517366,0.02191357532433203,91.0,0.0,0.0,241878.0,0.0,0.0,31,44
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.20859833002090455,0.000220084248873826,0.01483523673130382,75.0,0.0,0.0,199350.0,0.0,0.0,31,41
Random Action Value Iteration,True,False,0.09172922849655152,9.12579141195863e-05,0.009552900822241708,154.0,184.0,13.564659966250536,44950.0,15474400.0,3933.7513902126557,31,31
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.09189131736755371,4.505448127733871e-05,0.006712263498801184,151.0,49.0,7.0,44080.0,4120900.0,2030.0,31,31
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.09010730743408203,1.9621926033232742e-05,0.004429664325119088,150.0,0.0,0.0,43790.0,0.0,0.0,31,31
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.09656081199645997,4.475333534287528e-05,0.006689793370715965,150.0,0.0,0.0,43790.0,0.0,0.0,31,31
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.09499100208282471,5.7908841758967354e-05,0.007609785920705481,150.0,0.0,0.0,43790.0,0.0,0.0,31,31
Q-factor Value Iteration,True,False,0.027927544116973877,3.4155065167112752e-06,0.001848108902827773,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Q-factor Stochastic Value Iteration,True,True,0.30814948558807376,0.0006499965645341263,0.025495030192845942,105.0,0.0,0.0,279090.0,0.0,0.0,31,44
Model-free Dijkstra,True,False,0.002524383068084717,6.057683327242104e-07,0.0007783112055753858,238.0,0.0,0.0,836.0,0.0,0.0,31,31
