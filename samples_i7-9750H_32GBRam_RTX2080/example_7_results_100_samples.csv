Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.24261115074157716,0.00023927793249945347,0.015468611201379828,999.0,0.0,0.0,68063.66,52632.5644,229.41788160472584,29,29
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17434100866317748,0.0001191426976091577,0.010915250689249317,999.0,0.0,0.0,49287.78,42095.75159999999,205.17249230830137,29,29
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14824076890945434,0.00011796286011460212,0.0108610708548744,999.0,0.0,0.0,42929.26,50745.572400000005,225.26777932052335,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1470111584663391,8.170881353918843e-05,0.009039292756581592,999.0,0.0,0.0,42128.14,43152.98039999999,207.73295453538418,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.13335687637329102,2.221087426969462e-05,0.00471284142208229,999.0,0.0,0.0,42123.76,50156.50239999999,223.9564743426722,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5614120435714722,0.007938584849318408,0.08909873651920328,999.0,0.0,0.0,500000.0,0.0,0.0,8,8
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.14478081464767456,0.00010848368130311314,0.010415549976026861,999.0,0.0,0.0,42192.24,34631.94239999999,186.09659427297424,29,29
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.2641021680831909,0.00028566671556584417,0.016901677892027293,999.0,0.0,0.0,76182.4,51698.8,227.3737012057463,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.14759076595306397,7.528792367841109e-05,0.008676861395597552,999.0,0.0,0.0,42163.62,52105.57559999999,228.2664574570692,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14089109659194946,0.00010377659507764179,0.010187079811096102,999.0,0.0,0.0,42126.62,37638.05560000001,194.00529786580574,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14227261066436767,9.704439939603162e-05,0.009851111581747089,999.0,0.0,0.0,42195.38,40914.615600000005,202.27361567935648,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.1442832088470459,0.00014646030844178313,0.012102078682680225,999.0,0.0,0.0,42209.14,48434.4204,220.07821427846966,29,29
No-discounting Q-learning,True,False,0.14849129676818848,9.520977809247598e-05,0.009757549799641095,999.0,0.0,0.0,42150.94,32783.63639999999,181.06252069381998,29,29
"No-discounting, no stochastic approximation Q-learning",True,False,0.14271109104156493,0.00010010849727621009,0.010005423393150841,999.0,0.0,0.0,41381.54,49613.66840000001,222.74125886328292,29,29
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5801682782173156,0.0067616070699701315,0.08222899166334299,999.0,0.0,0.0,500000.0,0.0,0.0,8,8
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14180432319641112,9.288761187158342e-05,0.009637821946455715,999.0,0.0,0.0,42177.64,32776.8304,181.04372510529052,29,29
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14449085235595704,0.00011192867808781558,0.010579635064018777,999.0,0.0,0.0,42153.54,49895.1084,223.3721298640455,29,29
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.14320056438446044,0.00011541035458151327,0.010742921138196691,999.0,0.0,0.0,42179.8,51235.79999999999,226.35326372729858,29,29
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.15017156362533568,7.628483599398236e-05,0.008734119073723599,999.0,0.0,0.0,42181.92,44645.51360000002,211.2948499135746,29,29
cost-based no-discounting Q-learning,True,False,0.13916122674942016,6.189118605504405e-05,0.007867095147196584,999.0,0.0,0.0,42151.92,39762.23359999999,199.40469803893788,29,29
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.1491109037399292,7.781859450735736e-05,0.008821484824413481,999.0,0.0,0.0,41332.7,45760.27,213.91650240222233,29,29
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1393012547492981,7.270190310770772e-05,0.008526541098693404,999.0,0.0,0.0,41389.74,51087.0124,226.02436240370196,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.8721365571022033,0.008305616200987487,0.09113515348638794,999.0,0.0,0.0,500000.0,0.0,0.0,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7869857501983644,0.008995060786391468,0.09484229429105703,999.0,0.0,0.0,500000.0,0.0,0.0,2,10
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,5.903315944671631,0.0634862251696687,0.25196473001130276,999.0,0.0,0.0,1977511.38,876421218.9355999,29604.412153184192,29,29
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.12442062139511108,3.9614519796595005e-05,0.006294006656859762,999.0,0.0,0.0,37740.0,0.0,0.0,29,29
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.151260187625885,0.005141323129488791,0.07170302036517563,0.0,0.0,0.0,400000.0,0.0,0.0,29,29
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.049400293827056886,1.6545234322364875e-05,0.004067583351618609,138.0,0.0,0.0,13632.0,0.0,0.0,29,29
Don't care Q-learning,True,False,1.923945825099945,0.044994966425405264,0.21212016977507175,14999.0,0.0,0.0,578376.96,3422807594.5983996,58504.76557168997,29,41
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.30070133686065675,0.0003373702757578258,0.018367642084868317,999.0,0.0,0.0,69187.38,2899152.7355999993,1702.6898530266747,31,31076
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.2845916676521301,0.001470297184411555,0.038344454415359136,965.39,27340.817900000005,165.35059086680036,66825.07,87888939.10510002,9374.910085174151,29,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.28779146671295164,0.0003713644983969288,0.019270819868312007,999.0,0.0,0.0,68898.92,2819078.3135999995,1679.0111118155232,30,13753
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.3852579092979431,0.0005426100857429163,0.023293992481816343,999.0,0.0,0.0,88527.94,294999.43639999995,543.1385057239082,29,252
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.2413717293739319,0.00022725692116420642,0.015075042990459643,999.0,0.0,0.0,56749.68,609214.5375999999,780.521964841477,29,726
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.25947419881820677,0.0002259409444480468,0.015031332091602753,999.0,0.0,0.0,60132.68,905914.0376,951.795165778856,30,558
Value Iteration,True,False,0.02103048324584961,3.3810164883561803e-06,0.0018387540586919666,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.8,True,False,0.024260129928588867,1.4726450848684184e-06,0.0012135258896572494,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.6,True,False,0.022109935283660887,6.158158854867679e-06,0.0024815637922220896,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.5,True,False,0.023560450077056885,5.649214121712022e-06,0.002376807548311815,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Stochastic Value Iteration,True,True,0.34346627235412597,0.00031837586851556803,0.017843090217660392,105.0,0.0,0.0,328440.0,0.0,0.0,29,42
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.26693211317062376,0.00042312506533364165,0.020570004018804704,86.0,0.0,0.0,269008.0,0.0,0.0,29,43
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.2231016182899475,0.00024661213635965285,0.015703889211263967,71.0,0.0,0.0,222088.0,0.0,0.0,29,41
Random Action Value Iteration,True,False,0.09192061424255371,5.71700061118463e-05,0.007561084982451017,149.5,24.75,4.9749371855331,48611.5,2582142.75,1606.9047109271912,29,29
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.10446998119354248,6.274422183653315e-05,0.007921125036037062,151.0,49.0,7.0,49096.0,5112121.0,2261.0,29,29
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.1076006031036377,6.648344373561486e-05,0.008153738022257943,150.0,0.0,0.0,48773.0,0.0,0.0,29,29
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.10340054750442505,6.717276971136812e-05,0.008195899566940052,149.5,24.75,4.9749371855331,48611.5,2582142.75,1606.9047109271912,29,29
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.10041054248809815,4.137601358504526e-05,0.00643241895285477,150.0,0.0,0.0,48773.0,0.0,0.0,29,29
Q-factor Value Iteration,True,False,0.025589871406555175,4.719825494134966e-06,0.002172515936451322,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Q-factor Stochastic Value Iteration,True,True,0.36504435777664185,0.0006205357897005058,0.024910555788671313,105.0,0.0,0.0,328440.0,0.0,0.0,29,42
Model-free Dijkstra,True,False,0.002814488410949707,6.017767684397769e-07,0.0007757427205200039,0.0,0.0,0.0,0.0,0.0,0.0,29,29
