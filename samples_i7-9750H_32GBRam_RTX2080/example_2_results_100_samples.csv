Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5797724390029908,0.00039540446804610385,0.01988477980884133,999.0,0.0,0.0,485496.68,963316.0576000002,981.4866568629449,2,15
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.5985989451408387,0.0035108672830919826,0.05925257195339273,999.0,0.0,0.0,485579.17,813989.1211,902.2134565057207,2,18
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5842013049125672,0.0003245157002955409,0.01801431931257856,999.0,0.0,0.0,485274.8,860010.0799999998,927.3672843054147,2,32
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.4257223033905029,0.0016619466025782916,0.04076697931633262,999.0,0.0,0.0,443326.3,6463046.650000001,2542.2522789841296,4,4
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.4398800945281982,0.00024913513890251123,0.01578401529720848,999.0,0.0,0.0,454333.96,2592552.6584,1610.1405710061467,4,4
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.29732249975204467,1.6154095496148104e-05,0.004019215781237442,999.0,0.0,0.0,91771.14,14720.3604,121.32749235025011,30,30
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.29220353841781616,8.181225638219874e-05,0.009045012790604485,999.0,0.0,0.0,87774.32,11267.0376,106.14630280890616,30,30
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.19664480209350585,4.043916425948737e-05,0.00635917952722577,999.0,0.0,0.0,59206.38,12917.3356,113.65445701775184,30,30
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1646974802017212,0.00012795457374693343,0.0113117007451105,999.0,0.0,0.0,49433.16,11760.514400000002,108.44590540910248,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.15891704559326172,4.149776373424174e-05,0.006441875793139895,999.0,0.0,0.0,48023.4,12817.200000000003,113.21307345002167,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.17139525651931764,0.00016204405753080097,0.012729652686966797,999.0,0.0,0.0,48115.77,13821.6371,117.56545878785997,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.6636787676811218,0.007232651601543119,0.08504499751039515,999.0,0.0,0.0,500000.0,0.0,0.0,4,4
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.17097267389297485,0.0004076651393923554,0.020190719140049357,999.0,0.0,0.0,48203.35,14609.4075,120.86938197906035,30,30
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.357174768447876,0.0012683492480417952,0.035613891222973586,999.0,0.0,0.0,99228.35,8833.027499999998,93.98418749981296,3,34
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.17474208354949952,0.00043537180675591564,0.020865565095532775,999.0,0.0,0.0,48012.36,15694.850399999998,125.27909003500942,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1611781597137451,8.725536795623157e-05,0.00934105818182456,999.0,0.0,0.0,48158.77,14193.6971,119.13730356189869,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.17087676763534546,0.00037258299899223745,0.019302409149954246,999.0,0.0,0.0,48207.66,14157.944399999997,118.98716065189554,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.15908639907836913,3.655124533006529e-05,0.006045762592929472,999.0,0.0,0.0,48237.18,12646.6276,112.45722564602063,30,30
No-discounting Q-learning,True,False,0.16140886545181274,3.1647004928316846e-05,0.005625567076154798,999.0,0.0,0.0,48259.98,11824.899599999997,108.74235421398599,30,30
"No-discounting, no stochastic approximation Q-learning",True,False,0.16213802814483644,7.438598210499093e-05,0.008624730842466385,999.0,0.0,0.0,47827.69,12618.913900000001,112.3339392169615,30,30
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.690798318386078,0.004704027427440423,0.06858591274773868,999.0,0.0,0.0,485459.07,1156160.3851,1075.2489874908044,2,21
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6898006463050843,0.004403044586207324,0.06635544127053428,999.0,0.0,0.0,485484.62,1185499.0956000001,1088.8062709224264,2,12
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6826784372329713,0.006315834420578881,0.07947222420807713,999.0,0.0,0.0,485303.45,1119198.7074999998,1057.9218815678216,2,12
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5114280891418457,0.004601234203022705,0.06783239788642817,999.0,0.0,0.0,443069.46,5136198.4884,2266.3182672343264,4,4
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5561050987243652,0.0057059333169794625,0.07553762848395137,999.0,0.0,0.0,454528.93,3147425.8650999987,1774.0986063632424,4,4
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.3168920016288757,0.00030008894091667455,0.017323075388529445,999.0,0.0,0.0,91767.66,14278.864399999997,119.49420236982209,30,30
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.3010111737251282,0.0003541857607827467,0.01881982361189251,999.0,0.0,0.0,87756.73,15641.3771,125.06549124358806,30,30
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.2011913228034973,0.0001394776253769749,0.011810064579712292,999.0,0.0,0.0,59205.5,10926.09,104.52793884890298,30,30
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16732059717178344,7.576531014976239e-05,0.008704327093449694,999.0,0.0,0.0,49458.21,13526.645900000003,116.30410955765925,30,30
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16720032691955566,0.0001017236526911347,0.010085814428747671,999.0,0.0,0.0,48003.81,15808.013899999998,125.72992444124031,30,30
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.16480150938034058,0.00013623867406255955,0.011672132369989622,999.0,0.0,0.0,48148.76,14712.702400000002,121.29592903308834,30,30
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.3443913197517395,0.00035495472279978345,0.01884024211096512,999.0,0.0,0.0,99232.1,11056.209999999997,105.14851401707966,2,38
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.16430049419403076,0.00010208571910150113,0.010103747775033834,999.0,0.0,0.0,48006.28,14016.021600000002,118.38927992010088,30,30
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6080190134048462,0.0037906017752154415,0.06156786316915215,999.0,0.0,0.0,482771.63,1730448.9130999998,1315.4652838824748,4,4
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6702900409698487,0.0035973062565199145,0.059977547936873134,999.0,0.0,0.0,497234.47,281207.6491,530.2901555752285,4,4
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6773340678215027,0.003035797206428578,0.05509806899001614,999.0,0.0,0.0,499290.76,55014.34240000001,234.55136409750426,4,4
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.6819274044036865,0.0031232816403106876,0.05588632784779018,999.0,0.0,0.0,499607.49,42398.369900000005,205.9086445489844,4,4
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.6242933773994446,0.0005589322258211894,0.02364174752046027,999.0,0.0,0.0,500000.0,0.0,0.0,4,4
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.16062129497528077,1.1697986906733609e-05,0.0034202319960396853,999.0,0.0,0.0,48215.2,10688.18,103.38365441403201,30,30
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1723238444328308,0.0003300729862702326,0.018167910894492868,999.0,0.0,0.0,48151.5,14378.23,119.9092573573867,30,30
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.17373773574829102,0.0003536235238815152,0.018804880320850627,999.0,0.0,0.0,48214.78,17094.7716,130.74697549083115,30,30
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.1737847876548767,0.00041283508508610685,0.0203183435615728,999.0,0.0,0.0,48243.63,10946.1131,104.62367370724468,30,30
cost-based no-discounting Q-learning,True,False,0.18261533498764038,0.000609105473734934,0.02468006227169887,999.0,0.0,0.0,48268.17,13792.761100000002,117.4425863986314,30,30
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.17133944988250732,0.00025563407992378875,0.015988560908467928,999.0,0.0,0.0,47805.21,12639.025899999999,112.42342238163718,30,30
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.16664040327072144,0.0001338541128262648,0.011569533820611131,999.0,0.0,0.0,47802.62,14033.295600000001,118.46221169638865,30,30
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.8172934913635255,0.0024291631093814434,0.04928654085428844,999.0,0.0,0.0,500000.0,0.0,0.0,30,30
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.6866157412528993,0.0032345831171532776,0.056873395512781526,999.0,0.0,0.0,500000.0,0.0,0.0,2,12
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,5.46550213098526,0.06579820997828661,0.2565116176282989,999.0,0.0,0.0,1808495.85,1139907392.6875002,33762.51460847518,30,30
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.145993971824646,2.716586921508224e-05,0.005212088757406405,999.0,0.0,0.0,43093.0,0.0,0.0,30,30
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1335618162155152,0.0010544206589043143,0.032471844094604706,0.0,0.0,0.0,400000.0,0.0,0.0,30,30
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.06983449459075927,1.965684842218707e-05,0.004433604450352678,205.0,0.0,0.0,20067.0,0.0,0.0,30,30
Don't care Q-learning,True,False,1.9829885578155517,0.05220279693283909,0.2284793140151622,14999.0,0.0,0.0,630675.67,5341561735.9411,73085.98864311202,30,46
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.0903726053237915,2.1644327480748876e-05,0.0046523464489168125,189.02,100.67960000000001,10.033922463324101,22003.64,189380.43040000004,435.1786189600772,30,6910
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.08676208972930909,2.3980838025386214e-05,0.004897023384198427,174.06,90.3964,9.507702140896084,21118.47,179826.78909999997,424.0598885770735,30,4814
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.08824588775634766,1.6374595648676402e-05,0.004046553551934832,183.6,93.42,9.665402216152208,21665.55,194207.1675,440.6894229499955,30,2894
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.42669450044631957,0.00012981551134927824,0.01139366101607724,999.0,0.0,0.0,104407.91,39136.6219,197.82978011411728,30,10675
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.1354088306427002,3.9763376857990816e-05,0.0063058208710675264,308.71,96.86589999999998,9.842047551195838,33200.9,165298.83,406.5695881395951,30,174
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.10954515218734741,3.0318063506973653e-05,0.005506184114881526,275.83,64.6211,8.03872502328572,26782.36,124079.2504,352.2488472656795,30,295
Value Iteration,True,False,0.0125907301902771,8.39154774823214e-06,0.0028968168302866753,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.8,True,False,0.015030932426452637,9.326561001330448e-06,0.003053941879167062,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.6,True,False,0.014906370639801025,8.159991337350902e-06,0.0028565698551498617,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.5,True,False,0.014912950992584228,9.529647322068515e-06,0.0030870126857641053,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.3,True,False,0.014899504184722901,4.308647525874676e-07,0.0006564028889237673,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.1,False,True,0.015389680862426758,2.9852222507997794e-06,0.001727779572399147,20.0,0.0,0.0,19960.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.01,False,True,0.009120192527770996,2.694921693318974e-07,0.0005191263519913986,13.0,0.0,0.0,12974.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.0001,False,True,0.005387775897979737,2.8921464997324616e-07,0.0005377868071766414,8.0,0.0,0.0,7984.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.00001,False,True,0.004238455295562744,2.4308143185294283e-07,0.0004930328912485888,6.0,0.0,0.0,5988.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.000001,False,True,0.004358189105987549,6.10079080450987e-07,0.0007810755920210201,6.0,0.0,0.0,5988.0,0.0,0.0,4,4
Stochastic Value Iteration,True,True,0.27308093070983885,0.00104654757760195,0.03235038759585347,97.0,0.0,0.0,244052.0,0.0,0.0,30,43
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.2254753065109253,0.0008825041256920712,0.02970697099490406,80.0,0.0,0.0,201280.0,0.0,0.0,30,41
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.18657416105270386,0.0007613329314078498,0.027592262165466785,64.0,0.0,0.0,161024.0,0.0,0.0,30,42
Random Action Value Iteration,True,False,0.0956332802772522,0.00019292602013180728,0.013889781140529439,154.0,184.0,13.564659966250536,45570.0,15904224.0,3988.0100300776576,30,30
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.10912128686904907,0.0003769970523607923,0.019416411933227837,159.5,384.75,19.615045245933032,47187.0,33256251.0,5766.823302304311,30,30
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.10536057233810425,0.0003397129015432426,0.018431302220495507,159.0,369.0,19.209372712298546,47040.0,31894884.0,5647.555577415773,30,30
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1057243275642395,0.00033502843390858177,0.01830378195643135,156.5,282.75,16.815171720800237,46305.0,24439779.0,4943.660485915269,30,30
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.11001786470413208,0.0004467474352607667,0.021136400716791084,161.5,442.75,21.041625412500814,47775.0,38269539.0,6186.237871275239,30,30
Q-factor Value Iteration,True,False,0.017742972373962402,1.3391123242377037e-05,0.0036593883699843938,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Q-factor Stochastic Value Iteration,True,True,0.2970775055885315,0.001184548636490098,0.034417272356915475,97.0,0.0,0.0,244052.0,0.0,0.0,30,42
Model-free Dijkstra,True,False,0.0025020837783813477,5.308366581857626e-07,0.0007285853815344929,0.0,0.0,0.0,0.0,0.0,0.0,30,30
