Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.29220353841781616,8.181225638219874e-05,0.009045012790604485,999.0,0.0,0.0,87774.32,11267.0376,106.14630280890616,30,30
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.19664480209350585,4.043916425948737e-05,0.00635917952722577,999.0,0.0,0.0,59206.38,12917.3356,113.65445701775184,30,30
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1646974802017212,0.00012795457374693343,0.0113117007451105,999.0,0.0,0.0,49433.16,11760.514400000002,108.44590540910248,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.15891704559326172,4.149776373424174e-05,0.006441875793139895,999.0,0.0,0.0,48023.4,12817.200000000003,113.21307345002167,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.17139525651931764,0.00016204405753080097,0.012729652686966797,999.0,0.0,0.0,48115.77,13821.6371,117.56545878785997,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.6636787676811218,0.007232651601543119,0.08504499751039515,999.0,0.0,0.0,500000.0,0.0,0.0,4,4
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.17097267389297485,0.0004076651393923554,0.020190719140049357,999.0,0.0,0.0,48203.35,14609.4075,120.86938197906035,30,30
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.357174768447876,0.0012683492480417952,0.035613891222973586,999.0,0.0,0.0,99228.35,8833.027499999998,93.98418749981296,3,34
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.17474208354949952,0.00043537180675591564,0.020865565095532775,999.0,0.0,0.0,48012.36,15694.850399999998,125.27909003500942,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1611781597137451,8.725536795623157e-05,0.00934105818182456,999.0,0.0,0.0,48158.77,14193.6971,119.13730356189869,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.17087676763534546,0.00037258299899223745,0.019302409149954246,999.0,0.0,0.0,48207.66,14157.944399999997,118.98716065189554,30,30
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.15908639907836913,3.655124533006529e-05,0.006045762592929472,999.0,0.0,0.0,48237.18,12646.6276,112.45722564602063,30,30
No-discounting Q-learning,True,False,0.16140886545181274,3.1647004928316846e-05,0.005625567076154798,999.0,0.0,0.0,48259.98,11824.899599999997,108.74235421398599,30,30
"No-discounting, no stochastic approximation Q-learning",True,False,0.16213802814483644,7.438598210499093e-05,0.008624730842466385,999.0,0.0,0.0,47827.69,12618.913900000001,112.3339392169615,30,30
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.6242933773994446,0.0005589322258211894,0.02364174752046027,999.0,0.0,0.0,500000.0,0.0,0.0,4,4
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.16062129497528077,1.1697986906733609e-05,0.0034202319960396853,999.0,0.0,0.0,48215.2,10688.18,103.38365441403201,30,30
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1723238444328308,0.0003300729862702326,0.018167910894492868,999.0,0.0,0.0,48151.5,14378.23,119.9092573573867,30,30
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.17373773574829102,0.0003536235238815152,0.018804880320850627,999.0,0.0,0.0,48214.78,17094.7716,130.74697549083115,30,30
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.1737847876548767,0.00041283508508610685,0.0203183435615728,999.0,0.0,0.0,48243.63,10946.1131,104.62367370724468,30,30
cost-based no-discounting Q-learning,True,False,0.18261533498764038,0.000609105473734934,0.02468006227169887,999.0,0.0,0.0,48268.17,13792.761100000002,117.4425863986314,30,30
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.17133944988250732,0.00025563407992378875,0.015988560908467928,999.0,0.0,0.0,47805.21,12639.025899999999,112.42342238163718,30,30
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.16664040327072144,0.0001338541128262648,0.011569533820611131,999.0,0.0,0.0,47802.62,14033.295600000001,118.46221169638865,30,30
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.8172934913635255,0.0024291631093814434,0.04928654085428844,999.0,0.0,0.0,500000.0,0.0,0.0,30,30
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.6866157412528993,0.0032345831171532776,0.056873395512781526,999.0,0.0,0.0,500000.0,0.0,0.0,2,12
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,5.46550213098526,0.06579820997828661,0.2565116176282989,999.0,0.0,0.0,1808495.85,1139907392.6875002,33762.51460847518,30,30
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.145993971824646,2.716586921508224e-05,0.005212088757406405,999.0,0.0,0.0,43093.0,0.0,0.0,30,30
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1335618162155152,0.0010544206589043143,0.032471844094604706,0.0,0.0,0.0,400000.0,0.0,0.0,30,30
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.06983449459075927,1.965684842218707e-05,0.004433604450352678,205.0,0.0,0.0,20067.0,0.0,0.0,30,30
Don't care Q-learning,True,False,1.9829885578155517,0.05220279693283909,0.2284793140151622,14999.0,0.0,0.0,630675.67,5341561735.9411,73085.98864311202,30,46
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.0903726053237915,2.1644327480748876e-05,0.0046523464489168125,189.02,100.67960000000001,10.033922463324101,22003.64,189380.43040000004,435.1786189600772,30,6910
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.08676208972930909,2.3980838025386214e-05,0.004897023384198427,174.06,90.3964,9.507702140896084,21118.47,179826.78909999997,424.0598885770735,30,4814
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.08824588775634766,1.6374595648676402e-05,0.004046553551934832,183.6,93.42,9.665402216152208,21665.55,194207.1675,440.6894229499955,30,2894
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.42669450044631957,0.00012981551134927824,0.01139366101607724,999.0,0.0,0.0,104407.91,39136.6219,197.82978011411728,30,10675
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.1354088306427002,3.9763376857990816e-05,0.0063058208710675264,308.71,96.86589999999998,9.842047551195838,33200.9,165298.83,406.5695881395951,30,174
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.10954515218734741,3.0318063506973653e-05,0.005506184114881526,275.83,64.6211,8.03872502328572,26782.36,124079.2504,352.2488472656795,30,295
Value Iteration,True,False,0.0125907301902771,8.39154774823214e-06,0.0028968168302866753,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.8,True,False,0.015030932426452637,9.326561001330448e-06,0.003053941879167062,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.6,True,False,0.014906370639801025,8.159991337350902e-06,0.0028565698551498617,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Discounted Value Iteration - gamma = 0.5,True,False,0.014912950992584228,9.529647322068515e-06,0.0030870126857641053,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Stochastic Value Iteration,True,True,0.27308093070983885,0.00104654757760195,0.03235038759585347,97.0,0.0,0.0,244052.0,0.0,0.0,30,43
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.2254753065109253,0.0008825041256920712,0.02970697099490406,80.0,0.0,0.0,201280.0,0.0,0.0,30,41
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.18657416105270386,0.0007613329314078498,0.027592262165466785,64.0,0.0,0.0,161024.0,0.0,0.0,30,42
Random Action Value Iteration,True,False,0.0956332802772522,0.00019292602013180728,0.013889781140529439,154.0,184.0,13.564659966250536,45570.0,15904224.0,3988.0100300776576,30,30
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.10912128686904907,0.0003769970523607923,0.019416411933227837,159.5,384.75,19.615045245933032,47187.0,33256251.0,5766.823302304311,30,30
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.10536057233810425,0.0003397129015432426,0.018431302220495507,159.0,369.0,19.209372712298546,47040.0,31894884.0,5647.555577415773,30,30
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1057243275642395,0.00033502843390858177,0.01830378195643135,156.5,282.75,16.815171720800237,46305.0,24439779.0,4943.660485915269,30,30
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.11001786470413208,0.0004467474352607667,0.021136400716791084,161.5,442.75,21.041625412500814,47775.0,38269539.0,6186.237871275239,30,30
Q-factor Value Iteration,True,False,0.017742972373962402,1.3391123242377037e-05,0.0036593883699843938,20.0,0.0,0.0,19960.0,0.0,0.0,30,30
Q-factor Stochastic Value Iteration,True,True,0.2970775055885315,0.001184548636490098,0.034417272356915475,97.0,0.0,0.0,244052.0,0.0,0.0,30,42
Model-free Dijkstra,True,False,0.0025020837783813477,5.308366581857626e-07,0.0007285853815344929,0.0,0.0,0.0,0.0,0.0,0.0,30,30
