Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.014990172386169433,8.441307338921434e-06,0.0029053928028618496,999.0,0.0,0.0,4507.0,973.48,31.200641019056004,5,5
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.014730353355407715,6.039650302432165e-06,0.0024575699994979117,999.0,0.0,0.0,4473.46,1563.6684000000002,39.543247211123166,5,5
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.014439575672149658,1.8263689883667665e-06,0.0013514321989529354,999.0,0.0,0.0,4458.8,950.7199999999997,30.833747744962817,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.013899986743927001,3.1296360644489597e-06,0.0017690777440375423,999.0,0.0,0.0,4458.26,1147.9724,33.88174139562487,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.01430037260055542,1.4087828991421248e-06,0.0011869216061484956,999.0,0.0,0.0,4459.02,1001.1596000000001,31.641106175353606,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.4236915850639342,0.0033629420222106255,0.057990878784603926,999.0,0.0,0.0,499258.82,1468869.1276,1211.969111652603,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.013369967937469482,1.8533882999861358e-06,0.0013613920449253903,999.0,0.0,0.0,4458.58,903.5436000000002,30.05900197944037,5,5
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.013950016498565674,1.347146484130235e-06,0.0011606663965714847,999.0,0.0,0.0,4514.02,847.2396000000002,29.107380507355867,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.014549615383148194,2.0481251243552376e-06,0.0014311272215827766,999.0,0.0,0.0,4461.54,957.7484,30.94751040067682,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.013377106189727784,1.925162394371682e-06,0.0013875022141862268,999.0,0.0,0.0,4461.02,1155.8796000000002,33.99822936565962,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.013099627494812012,7.298154689351577e-07,0.0008542923790688746,999.0,0.0,0.0,4456.34,782.4844000000002,27.972922621706875,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.01414034366607666,2.799896877263564e-06,0.001673289238973216,999.0,0.0,0.0,4463.44,959.6064000000001,30.977514425789558,5,5
No-discounting Q-learning,True,False,0.014920072555541992,2.4146208328147626e-06,0.001553905026961031,999.0,0.0,0.0,4457.18,915.4475999999999,30.256364619696132,5,5
"No-discounting, no stochastic approximation Q-learning",True,False,0.014449996948242188,2.848252250805671e-06,0.0016876765835922684,999.0,0.0,0.0,4459.84,996.4543999999999,31.566665962689182,5,5
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.4367002415657044,0.005167669514520702,0.07188650439770111,999.0,0.0,0.0,499264.6,1170340.2000000002,1081.8226287150774,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.01444995880126953,1.6450858657663044e-06,0.0012826090073620661,999.0,0.0,0.0,4455.4,1092.76,33.05692060673529,5,5
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.01481997013092041,1.2680470927307397e-06,0.0011260759711186185,999.0,0.0,0.0,4454.8,1034.8,32.16830738475371,5,5
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.01413999080657959,2.1814355047581555e-06,0.0014769683492743355,999.0,0.0,0.0,4455.04,1098.9183999999998,33.14993815982165,5,5
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.014190003871917725,2.173322949028034e-06,0.0014742194372033065,999.0,0.0,0.0,4454.52,801.0095999999998,28.302112995322446,5,5
cost-based no-discounting Q-learning,True,False,0.014119939804077148,1.9848029272452548e-06,0.0014088303401209297,999.0,0.0,0.0,4460.14,928.9804,30.479179778990115,5,5
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.013700134754180908,1.0092660482484916e-06,0.0010046223411055975,999.0,0.0,0.0,4457.98,752.7596000000002,27.43646478684891,5,5
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.015230011940002442,1.8358288758690828e-06,0.0013549276275392286,999.0,0.0,0.0,4459.86,953.2203999999999,30.874267602649297,5,5
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.6313419270515441,0.006220269141101035,0.07886868289188703,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.488860924243927,0.005557939368127397,0.07455158863583926,999.0,0.0,0.0,500000.0,0.0,0.0,2,5
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.06823072910308838,3.58482015396703e-05,0.0059873367651795155,999.0,0.0,0.0,23970.54,357946.06840000005,598.2859420043229,5,5
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.013380317687988282,4.197113630971217e-06,0.002048685830226591,999.0,0.0,0.0,4020.0,0.0,0.0,5,5
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1119883370399475,0.0047211747507295235,0.06871080519634101,0.0,0.0,0.0,400000.0,0.0,0.0,5,5
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.00022000551223754882,1.7161558486691317e-07,0.00041426511422869435,5.0,0.0,0.0,44.0,0.0,0.0,5,5
Don't care Q-learning,True,False,0.18968356370925904,0.0001081338633474104,0.010398743354242877,14999.0,0.0,0.0,60069.5,1449.55,38.07295628132914,5,5
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.000350344181060791,2.480203583843377e-07,0.0004980164238098355,6.21,8.1659,2.85760389137473,68.02,603.8796000000001,24.57396182954633,5,11
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.00034967660903930666,2.2709052970526503e-07,0.00047654016588873706,6.45,10.6075,3.2569157188972513,69.57,738.5850999999998,27.176922195127243,5,71
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.0003499913215637207,2.2749273563817948e-07,0.00047696198552733685,6.5,8.19,2.8618176042508368,69.91,619.4019,24.887786161087128,5,13
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.021220316886901857,2.7110642650086444e-06,0.0016465309790613246,999.0,0.0,0.0,5307.99,2675.1299,51.72165793939711,5,9
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.013029930591583251,5.7053401015417654e-05,0.007553370175982219,609.99,127824.50990000002,357.52553740956745,3267.21,3607003.0458999993,1899.211164115249,5,11
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.0024699950218200683,3.8283384459361966e-06,0.0019566140257946114,111.86,8793.7004,93.77473220436302,648.95,273396.6075000001,522.8734144130873,5,11
Value Iteration,True,False,0.001450035572052002,3.267836005932167e-07,0.0005716498933728727,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.8,True,False,0.0011900806427001954,1.54451844673531e-07,0.00039300361916085584,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.6,True,False,0.001230001449584961,1.770914434473525e-07,0.00042082234190612137,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.5,True,False,0.0012000060081481933,1.5898300909498174e-07,0.0003987267348635927,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Stochastic Value Iteration,True,True,0.01568964958190918,5.954979656871728e-06,0.002440282700195149,75.0,0.0,0.0,750.0,0.0,0.0,5,9
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.013150041103363036,3.466986513393522e-06,0.0018619845631458714,61.0,0.0,0.0,610.0,0.0,0.0,5,11
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.009799768924713135,3.4009541456612164e-07,0.0005831770010606743,51.0,0.0,0.0,510.0,0.0,0.0,5,12
Random Action Value Iteration,True,False,0.013010308742523194,3.08461490811851e-07,0.0005553930957545754,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.013159642219543457,3.928042229063067e-07,0.0006267409535895247,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.013119933605194091,2.470736211478197e-07,0.0004970650069636965,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.014699642658233642,3.571283219633869e-06,0.001889783908184708,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.013870368003845215,8.736882857874662e-07,0.0009347129429870254,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Q-factor Value Iteration,True,False,0.0008899974822998047,1.1745734545911547e-07,0.00034272050633003484,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Q-factor Stochastic Value Iteration,True,True,0.009989998340606689,2.4839145691544214e-07,0.0004983888611470387,75.0,0.0,0.0,750.0,0.0,0.0,5,11
