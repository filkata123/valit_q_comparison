Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.28102234125137326,0.00028543303646831077,0.01689476358130858,999.0,0.0,0.0,83038.6,46469.2,215.56715890877254,22,22
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.18377843379974365,0.0001437319808028178,0.011988827332263061,999.0,0.0,0.0,53048.24,17179.8424,131.07189782710861,22,22
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1463949775695801,0.00010037736294398202,0.01001885038035712,999.0,0.0,0.0,42683.04,22290.058399999998,149.2985545810809,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1443863320350647,9.937460833845649e-05,0.009968681374106432,999.0,0.0,0.0,41182.84,28281.5344,168.17114615771638,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.13868094444274903,7.462815734006653e-05,0.008638759016205194,999.0,0.0,0.0,41057.33,25317.241100000003,159.1139249091669,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.4924349236488341,0.0031891590920245733,0.056472640207666694,999.0,0.0,0.0,500000.0,0.0,0.0,9,9
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.13691211462020875,5.1602927227867225e-05,0.007183517747445692,999.0,0.0,0.0,41146.65,20577.4675,143.44848378424916,22,22
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.32373259544372557,0.0003595085480500074,0.01896071064200937,999.0,0.0,0.0,95826.75,34960.0275,186.97600781918518,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.14102185487747193,0.0002831729923291903,0.016827744719040347,999.0,0.0,0.0,41187.7,34754.87,186.42658072281432,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14341042757034303,0.00012031940982780612,0.010969020458901795,999.0,0.0,0.0,41253.8,29688.360000000008,172.30310502135475,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14111124038696288,8.91068141053438e-05,0.009439640570770891,999.0,0.0,0.0,41144.53,27593.98910000001,166.114385590171,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.13583203315734863,3.6429762424995716e-05,0.006035707284568704,999.0,0.0,0.0,41104.99,20288.289900000003,142.43696816486934,22,22
No-discounting Q-learning,True,False,0.14061426639556884,7.426473310258641e-05,0.008617698828723734,999.0,0.0,0.0,41125.33,31936.861100000002,178.7088724714025,22,22
"No-discounting, no stochastic approximation Q-learning",True,False,0.13640209913253784,5.9235479424904715e-05,0.007696458888664625,999.0,0.0,0.0,40276.28,56838.48159999999,238.40822469034075,22,22
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5277347874641418,0.00471167335306024,0.06864162988347698,999.0,0.0,0.0,500000.0,0.0,0.0,9,9
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14315166234970092,9.419545853358498e-05,0.009705434484534167,999.0,0.0,0.0,41145.85,36089.3875,189.97207031561243,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14004866123199464,6.964271297104007e-05,0.008345220965980473,999.0,0.0,0.0,41231.75,28336.7075,168.33510477615772,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.14083088874816896,7.785278527003357e-05,0.008823422537203666,999.0,0.0,0.0,41163.12,28056.165600000004,167.49974805951203,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.14201083421707153,7.385916507120669e-05,0.008594135504587224,999.0,0.0,0.0,41147.02,29472.959600000006,171.67690467852688,22,22
cost-based no-discounting Q-learning,True,False,0.14580153226852416,9.92119651350265e-05,0.009960520324512495,999.0,0.0,0.0,41130.35,39364.747500000005,198.4055127762331,22,22
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.13753257274627687,5.4189557120230346e-05,0.007361355657773258,999.0,0.0,0.0,40236.12,63691.84559999999,252.3724343108811,22,22
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.13757076978683472,4.930127894414796e-05,0.007021486946804641,999.0,0.0,0.0,40301.55,60406.4675,245.77727213882085,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.831209623813629,0.004816379884951374,0.06940014326319056,999.0,0.0,0.0,500000.0,0.0,0.0,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7546630263328553,0.005940115988627639,0.07707214794351873,999.0,0.0,0.0,500000.0,0.0,0.0,2,13
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.732934558391571,0.040999833568494494,0.20248415633943928,999.0,0.0,0.0,1607381.17,834134375.2011,28881.38457901733,22,22
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.12776000738143922,7.88257361440344e-05,0.008878385897449739,999.0,0.0,0.0,35473.0,0.0,0.0,22,22
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1857778573036193,0.004083850227218245,0.06390500940629182,0.0,0.0,0.0,400000.0,0.0,0.0,22,22
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.06134026527404785,1.8967126207007823e-05,0.004355126428360929,119.0,0.0,0.0,16993.0,0.0,0.0,22,22
Don't care Q-learning,True,False,1.607185471057892,0.07346904000035281,0.27105172938085603,14999.0,0.0,0.0,502145.75,6227448766.6275,78914.1860924099,22,46
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.3072319269180298,0.0003608618365655047,0.018996363772193475,999.0,0.0,0.0,71356.24,2691588.2423999994,1640.60605947924,25,1305
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.2874636960029602,0.0003632990322246939,0.019060404828457708,999.0,0.0,0.0,68277.28,1684987.1416,1298.0705456946475,26,2014
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.29559132575988767,0.0003511761408370602,0.018739694256765776,999.0,0.0,0.0,69111.77,1896027.1371000002,1376.963012248332,26,477
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.46390413522720336,0.0008739382834010656,0.029562447182211855,999.0,0.0,0.0,111952.22,364982.9515999999,604.1381891587387,22,665
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.2574918818473816,0.00026271924123518034,0.016208616265282496,999.0,0.0,0.0,60720.57,974421.7050999999,987.128008466987,22,386
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.25701154470443727,0.00021646022353635885,0.014712587248215687,999.0,0.0,0.0,62273.77,979413.3171000001,989.6531296873668,25,275
Value Iteration,True,False,0.024329419136047362,4.903004274842715e-06,0.0022142728546506447,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.8,True,False,0.02817018985748291,5.661663675732599e-06,0.002379425072519116,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.6,True,False,0.029080119132995606,4.3516371966006775e-06,0.002086057812382168,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.5,True,False,0.029670112133026123,5.662264159644792e-06,0.0023795512517373507,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Stochastic Value Iteration,True,True,0.32885746717453,0.0006277112860274144,0.025054167039185605,102.0,0.0,0.0,349248.0,0.0,0.0,22,32
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.28132181882858276,0.0005795834302760511,0.02407453904597243,87.0,0.0,0.0,297888.0,0.0,0.0,22,32
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.23019182443618774,0.00028140416178359256,0.016775105417957663,73.0,0.0,0.0,249952.0,0.0,0.0,22,32
Random Action Value Iteration,True,False,0.11546579122543335,0.0003358232014950942,0.018325479570671384,172.0,616.0,24.819347291981714,60896.0,76324864.0,8736.410246777563,22,22
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.117360520362854,0.00020765249697944914,0.014410152566140621,157.5,318.75,17.853571071357123,55792.0,39494400.0,6284.457017117708,22,22
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.12053066253662109,0.00019799098714784125,0.014070927018069607,157.0,301.0,17.349351572897472,55616.0,37295104.0,6106.97175365991,22,22
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1181006383895874,0.0002702736515095467,0.016440001566591977,157.5,318.75,17.853571071357123,55792.0,39494400.0,6284.457017117708,22,22
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.11264074563980103,0.00016471483197330483,0.012834127628058902,160.5,414.75,20.3654118544163,56848.0,51389184.0,7168.624972754538,22,22
Q-factor Value Iteration,True,False,0.03351017713546753,9.048561003800157e-06,0.0030080826125291433,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Q-factor Stochastic Value Iteration,True,True,0.3547135853767395,0.0008747035118266183,0.029575386926067735,102.0,0.0,0.0,349248.0,0.0,0.0,22,32
Model-free Dijkstra,True,False,0.0032321310043334963,6.535896709692682e-07,0.0008084489291039157,0.0,0.0,0.0,0.0,0.0,0.0,22,22
