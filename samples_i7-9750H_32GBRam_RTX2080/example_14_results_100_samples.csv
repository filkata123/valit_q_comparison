Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6666173839569092,0.0026638237519317955,0.05161224420553514,999.0,0.0,0.0,472330.08,1444432.2736,1201.845361766646,2,11
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.7118707537651061,0.00538097065927488,0.07335509974960759,999.0,0.0,0.0,472138.14,2212196.1804000004,1487.3453467167606,2,13
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6837038064002992,0.000985999496218591,0.0314006289143799,999.0,0.0,0.0,471315.08,2406403.4135999996,1551.2586546414493,2,20
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.3669571352005006,0.005333423677672704,0.0730302928768104,999.0,0.0,0.0,383044.63,4795642.273100001,2189.895493648042,2,2
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.4341719627380372,0.003253486259247893,0.057039339575839176,999.0,0.0,0.0,440234.35,2841869.9875,1685.784680052586,2,2
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.2698219347000122,0.0002538278543074512,0.015931975844428438,999.0,0.0,0.0,78698.93,17394.125099999997,131.88678895173692,24,24
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.2585099053382873,0.0003533392610889393,0.018797320582703784,999.0,0.0,0.0,75266.95,12701.2875,112.69998890860637,24,24
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17823019981384278,0.00010009508623797955,0.010004753182261896,999.0,0.0,0.0,49828.59,13351.6019,115.54913197423856,24,24
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14533982753753663,8.509770808038865e-05,0.009224841900021303,999.0,0.0,0.0,41168.94,20584.736399999998,143.47381782053475,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14307959794998168,8.572072503517915e-05,0.009258548754269168,999.0,0.0,0.0,39998.45,23453.0475,153.1438784280978,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.1401703977584839,8.664467181872623e-05,0.009308311974720563,999.0,0.0,0.0,40170.91,21921.0419,148.057562792314,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.6241238903999329,0.006894979168339493,0.08303601127426277,999.0,0.0,0.0,500000.0,0.0,0.0,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.141780047416687,0.00010998579811232503,0.010487411411417263,999.0,0.0,0.0,40312.23,25221.9971,158.81434790345614,24,24
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.2954994034767151,0.0004496762464339725,0.021205571117844775,999.0,0.0,0.0,85752.62,28290.195599999992,168.19689533401024,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.14093064546585082,9.376674081005945e-05,0.009683322818643373,999.0,0.0,0.0,40017.5,25843.81,160.76010077130456,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14237930297851562,8.463381508554447e-05,0.00919966385720394,999.0,0.0,0.0,40202.86,27420.3204,165.59082220944492,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14700023889541625,5.420013016839107e-05,0.007362073768198134,999.0,0.0,0.0,40286.98,19853.539600000004,140.90258904647567,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.14635985374450683,5.827176422949379e-05,0.007633594450158705,999.0,0.0,0.0,40304.83,22857.321099999997,151.1863786853829,24,24
No-discounting Q-learning,True,False,0.1470795226097107,0.0001020699995465918,0.010102969837953186,999.0,0.0,0.0,40185.73,36710.8771,191.60082750343224,24,24
"No-discounting, no stochastic approximation Q-learning",True,False,0.14234009742736817,0.00010091615797537087,0.01004570345846277,999.0,0.0,0.0,40342.87,33709.3731,183.60112499655332,24,24
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6894398188591004,0.0022265741087411414,0.047186588229507984,999.0,0.0,0.0,472206.4,2050209.22,1431.8551672568005,2,9
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.690999183654785,0.0010490524871960589,0.03238907975222604,999.0,0.0,0.0,472137.16,1374101.3744,1172.220702086429,2,14
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6851744151115418,0.0025719032677083025,0.050713935636157274,999.0,0.0,0.0,471006.64,2032304.1704000002,1425.5890608446741,2,14
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.357580804824829,0.0016485188327294508,0.040601956020978235,999.0,0.0,0.0,382674.39,4071547.8779,2017.807691010221,2,2
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5278526639938355,0.0012810826497609694,0.03579221493231411,999.0,0.0,0.0,440467.82,2638062.9875999996,1624.2114971887127,2,2
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.2826720905303955,0.0001559064150750146,0.012486249039443936,999.0,0.0,0.0,78689.15,18769.9675,137.0035309763949,24,24
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.27335198402404787,6.962118376791295e-05,0.008343930954167402,999.0,0.0,0.0,75308.64,16333.570400000004,127.80285755803743,24,24
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17983187675476076,6.116300804060302e-05,0.00782067823405381,999.0,0.0,0.0,49814.44,18465.9064,135.88931672504648,24,24
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1501113796234131,5.651448340258867e-05,0.0075176115490618875,999.0,0.0,0.0,41180.87,18843.1531,137.27036497365336,24,24
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14524053573608398,7.297505917667877e-05,0.008542544069343674,999.0,0.0,0.0,40001.5,23649.09,153.78260629863183,24,24
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.14418073415756225,6.668801621351008e-05,0.008166273092023685,999.0,0.0,0.0,40157.64,23383.190400000007,152.91563164045724,24,24
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.3094722008705139,0.00014262230962060017,0.011942458273764249,999.0,0.0,0.0,85781.52,24575.229600000006,156.76488637446846,24,24
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1444610071182251,7.455064125199441e-05,0.008634271321425706,999.0,0.0,0.0,40001.6,21276.64,145.86514319740684,24,24
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6270182919502258,0.0021835048143203214,0.046727987484165436,999.0,0.0,0.0,474682.28,1946934.1216,1395.3258119880102,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7065526509284974,0.0025100022211698787,0.050099922366904706,999.0,0.0,0.0,495860.62,301882.7156,549.4385457901548,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.7052577805519105,0.0019489985289220105,0.04414746344833427,999.0,0.0,0.0,498933.07,76005.2851,275.6905604114874,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.7117119574546813,0.0033768865745488425,0.058110984973142925,999.0,0.0,0.0,499668.51,35996.8899,189.72846359995646,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.614835000038147,0.008841429559000176,0.09402887619768821,999.0,0.0,0.0,500000.0,0.0,0.0,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.1476098871231079,3.122368650522276e-05,0.0055878158975777616,999.0,0.0,0.0,40319.07,23859.6651,154.46574086184935,24,24
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1412069606781006,0.000228879127422124,0.015128751680893041,999.0,0.0,0.0,40196.02,21698.1996,147.30308754401585,24,24
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.13353893518447876,4.444479484492946e-05,0.006666692946651245,999.0,0.0,0.0,40348.8,25171.840000000004,158.65635820855087,24,24
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.13867926359176636,0.00010047575702798781,0.01002375962540941,999.0,0.0,0.0,40283.44,26425.9064,162.56046997963557,24,24
cost-based no-discounting Q-learning,True,False,0.14597951173782348,8.380992415524702e-05,0.009154776029769763,999.0,0.0,0.0,40189.46,23914.668400000002,154.64368205652633,24,24
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.13799217462539673,8.034657191917061e-05,0.008963624931866048,999.0,0.0,0.0,40288.62,27264.715600000007,165.1203064435141,24,24
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.14036987781524657,9.110190735534615e-05,0.00954473191636864,999.0,0.0,0.0,40287.58,28694.103600000002,169.39333989268883,24,24
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.7311605429649353,0.008377262885837917,0.09152738871965002,999.0,0.0,0.0,500000.0,0.0,0.0,24,24
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.675739176273346,0.006092231804037119,0.07805275013756478,999.0,0.0,0.0,500000.0,0.0,0.0,2,12
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.540069932937622,0.05297820748603213,0.2301699534822739,999.0,0.0,0.0,1539033.31,897340474.9538999,29955.641788382698,24,24
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.691737332344055,0.029153868598156368,0.17074503974685873,999.0,0.0,0.0,1542837.65,342471986.7475,18505.998669282886,24,24
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.12582510232925415,6.779454917347607e-05,0.008233744541426826,999.0,0.0,0.0,36119.0,0.0,0.0,24,24
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1679596948623656,0.0051492296270759874,0.07175813282880197,0.0,0.0,0.0,400000.0,0.0,0.0,24,24
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.055929856300354006,2.02259728944e-05,0.004497329529220646,132.0,0.0,0.0,16178.0,0.0,0.0,24,24
Don't care Q-learning,True,False,1.48131187915802,0.019877572125203892,0.1409878438916061,14999.0,0.0,0.0,456428.57,1143975314.9250998,33822.704133837375,24,38
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.29385035037994384,0.00038489806609334217,0.01961881918193198,999.0,0.0,0.0,69690.79,2813632.0859,1677.3884719706405,25,3406
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.28335723400115964,0.0005591829108536558,0.023647048671105994,999.0,0.0,0.0,68303.9,2771808.6699999995,1664.8749712816273,24,1985
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.29243183612823487,0.000343432728258199,0.018531938059960135,999.0,0.0,0.0,68364.82,2721233.5076,1649.6161697801099,24,1655
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",False,True,0.4266306185722351,0.0010584984013411202,0.03253457240138742,999.0,0.0,0.0,99903.79,444747.86590000003,666.8941939318411,24,100002
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.24056988716125488,0.0002443736642216209,0.01563245547639976,999.0,0.0,0.0,56477.42,465743.6635999999,682.4541476172593,24,243
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.24804944038391114,0.0006933949887174094,0.026332394283798227,999.0,0.0,0.0,58965.83,788416.0810999998,887.9279706710448,25,73155
Value Iteration,True,False,0.02255955457687378,1.509122438694135e-05,0.0038847425123090655,30.0,0.0,0.0,32400.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.8,True,False,0.15999178647994994,0.000163793316130176,0.012798176281415097,224.0,0.0,0.0,241920.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.6,True,False,0.07265986442565918,3.434117875863194e-05,0.005860134704819672,101.0,0.0,0.0,109080.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.5,True,False,0.055439848899841306,2.6364842296038656e-05,0.005134670612224182,75.0,0.0,0.0,81000.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.3,True,False,0.03691019535064697,2.529889513218677e-05,0.005029800705016727,45.0,0.0,0.0,48600.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.1,False,True,0.022740092277526856,8.090801951561844e-06,0.002844433502749158,25.0,0.0,0.0,27000.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.01,False,True,0.010780038833618165,2.3924394555706383e-06,0.0015467512584674493,13.0,0.0,0.0,14040.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.0001,False,True,0.006209776401519775,1.206630826908395e-06,0.001098467490146338,8.0,0.0,0.0,8640.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.00001,False,True,0.005149204730987549,5.877538911306602e-07,0.0007666510882602725,6.0,0.0,0.0,6480.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.000001,False,True,0.0046103310585021975,5.584738262939481e-07,0.0007473110639445586,6.0,0.0,0.0,6480.0,0.0,0.0,2,2
Stochastic Value Iteration,True,True,0.28735965967178345,0.0003887525536400233,0.019716808911180918,104.0,0.0,0.0,259584.0,0.0,0.0,24,34
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.6540933775901795,0.0018141662033513682,0.042593029985566515,242.0,0.0,0.0,604032.0,0.0,0.0,24,35
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.30986916065216064,0.0006939719282779834,0.026343346945253244,111.0,0.0,0.0,277056.0,0.0,0.0,24,37
Random Action Value Iteration,True,False,0.10020963191986083,0.00016998532093591624,0.013037841881842111,158.0,336.0,18.33030277982336,54855.0,39992400.0,6323.95445903906,24,24
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.11163128614425659,8.592789171572691e-05,0.009269729862068631,150.0,0.0,0.0,52095.0,0.0,0.0,24,24
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.13243045091629027,0.00025160144700209344,0.015861949659549845,187.5,468.75,21.650635094610966,65032.5,55792968.75,7469.4691076407835,24,24
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1119605016708374,6.95056971457916e-05,0.008337007685362392,150.0,0.0,0.0,52095.0,0.0,0.0,24,24
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1188696002960205,9.733312844687136e-05,0.00986575534091898,150.5,24.75,4.9749371855331,52267.5,2945868.75,1716.3533290089194,24,24
Q-factor Value Iteration,True,False,0.0318797492980957,6.445141072526894e-06,0.0025387282392030255,30.0,0.0,0.0,32400.0,0.0,0.0,24,24
Q-factor Stochastic Value Iteration,True,True,0.34243184328079224,0.00045883848374089097,0.02142051548728207,104.0,0.0,0.0,259584.0,0.0,0.0,24,38
Model-free Dijkstra,True,False,0.002732868194580078,9.144803487743048e-07,0.000956284658861735,279.0,0.0,0.0,882.0,0.0,0.0,24,24
