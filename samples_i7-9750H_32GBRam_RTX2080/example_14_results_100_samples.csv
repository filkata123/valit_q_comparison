Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.2585099053382873,0.0003533392610889393,0.018797320582703784,999.0,0.0,0.0,75266.95,12701.2875,112.69998890860637,24,24
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17823019981384278,0.00010009508623797955,0.010004753182261896,999.0,0.0,0.0,49828.59,13351.6019,115.54913197423856,24,24
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14533982753753663,8.509770808038865e-05,0.009224841900021303,999.0,0.0,0.0,41168.94,20584.736399999998,143.47381782053475,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14307959794998168,8.572072503517915e-05,0.009258548754269168,999.0,0.0,0.0,39998.45,23453.0475,153.1438784280978,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.1401703977584839,8.664467181872623e-05,0.009308311974720563,999.0,0.0,0.0,40170.91,21921.0419,148.057562792314,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.6241238903999329,0.006894979168339493,0.08303601127426277,999.0,0.0,0.0,500000.0,0.0,0.0,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.141780047416687,0.00010998579811232503,0.010487411411417263,999.0,0.0,0.0,40312.23,25221.9971,158.81434790345614,24,24
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.2954994034767151,0.0004496762464339725,0.021205571117844775,999.0,0.0,0.0,85752.62,28290.195599999992,168.19689533401024,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.14093064546585082,9.376674081005945e-05,0.009683322818643373,999.0,0.0,0.0,40017.5,25843.81,160.76010077130456,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14237930297851562,8.463381508554447e-05,0.00919966385720394,999.0,0.0,0.0,40202.86,27420.3204,165.59082220944492,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14700023889541625,5.420013016839107e-05,0.007362073768198134,999.0,0.0,0.0,40286.98,19853.539600000004,140.90258904647567,24,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.14635985374450683,5.827176422949379e-05,0.007633594450158705,999.0,0.0,0.0,40304.83,22857.321099999997,151.1863786853829,24,24
No-discounting Q-learning,True,False,0.1470795226097107,0.0001020699995465918,0.010102969837953186,999.0,0.0,0.0,40185.73,36710.8771,191.60082750343224,24,24
"No-discounting, no stochastic approximation Q-learning",True,False,0.14234009742736817,0.00010091615797537087,0.01004570345846277,999.0,0.0,0.0,40342.87,33709.3731,183.60112499655332,24,24
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.614835000038147,0.008841429559000176,0.09402887619768821,999.0,0.0,0.0,500000.0,0.0,0.0,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.1476098871231079,3.122368650522276e-05,0.0055878158975777616,999.0,0.0,0.0,40319.07,23859.6651,154.46574086184935,24,24
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1412069606781006,0.000228879127422124,0.015128751680893041,999.0,0.0,0.0,40196.02,21698.1996,147.30308754401585,24,24
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.13353893518447876,4.444479484492946e-05,0.006666692946651245,999.0,0.0,0.0,40348.8,25171.840000000004,158.65635820855087,24,24
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.13867926359176636,0.00010047575702798781,0.01002375962540941,999.0,0.0,0.0,40283.44,26425.9064,162.56046997963557,24,24
cost-based no-discounting Q-learning,True,False,0.14597951173782348,8.380992415524702e-05,0.009154776029769763,999.0,0.0,0.0,40189.46,23914.668400000002,154.64368205652633,24,24
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.13799217462539673,8.034657191917061e-05,0.008963624931866048,999.0,0.0,0.0,40288.62,27264.715600000007,165.1203064435141,24,24
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.14036987781524657,9.110190735534615e-05,0.00954473191636864,999.0,0.0,0.0,40287.58,28694.103600000002,169.39333989268883,24,24
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.7311605429649353,0.008377262885837917,0.09152738871965002,999.0,0.0,0.0,500000.0,0.0,0.0,24,24
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.675739176273346,0.006092231804037119,0.07805275013756478,999.0,0.0,0.0,500000.0,0.0,0.0,2,12
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.540069932937622,0.05297820748603213,0.2301699534822739,999.0,0.0,0.0,1539033.31,897340474.9538999,29955.641788382698,24,24
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.12582510232925415,6.779454917347607e-05,0.008233744541426826,999.0,0.0,0.0,36119.0,0.0,0.0,24,24
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1679596948623656,0.0051492296270759874,0.07175813282880197,0.0,0.0,0.0,400000.0,0.0,0.0,24,24
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.055929856300354006,2.02259728944e-05,0.004497329529220646,132.0,0.0,0.0,16178.0,0.0,0.0,24,24
Don't care Q-learning,True,False,1.48131187915802,0.019877572125203892,0.1409878438916061,14999.0,0.0,0.0,456428.57,1143975314.9250998,33822.704133837375,24,38
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.29385035037994384,0.00038489806609334217,0.01961881918193198,999.0,0.0,0.0,69690.79,2813632.0859,1677.3884719706405,25,3406
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.28335723400115964,0.0005591829108536558,0.023647048671105994,999.0,0.0,0.0,68303.9,2771808.6699999995,1664.8749712816273,24,1985
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.29243183612823487,0.000343432728258199,0.018531938059960135,999.0,0.0,0.0,68364.82,2721233.5076,1649.6161697801099,24,1655
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",False,True,0.4266306185722351,0.0010584984013411202,0.03253457240138742,999.0,0.0,0.0,99903.79,444747.86590000003,666.8941939318411,24,100002
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.24056988716125488,0.0002443736642216209,0.01563245547639976,999.0,0.0,0.0,56477.42,465743.6635999999,682.4541476172593,24,243
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.24804944038391114,0.0006933949887174094,0.026332394283798227,999.0,0.0,0.0,58965.83,788416.0810999998,887.9279706710448,25,73155
Value Iteration,True,False,0.02255955457687378,1.509122438694135e-05,0.0038847425123090655,30.0,0.0,0.0,32400.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.8,True,False,0.15999178647994994,0.000163793316130176,0.012798176281415097,224.0,0.0,0.0,241920.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.6,True,False,0.07265986442565918,3.434117875863194e-05,0.005860134704819672,101.0,0.0,0.0,109080.0,0.0,0.0,24,24
Discounted Value Iteration - gamma = 0.5,True,False,0.055439848899841306,2.6364842296038656e-05,0.005134670612224182,75.0,0.0,0.0,81000.0,0.0,0.0,24,24
Stochastic Value Iteration,True,True,0.28735965967178345,0.0003887525536400233,0.019716808911180918,104.0,0.0,0.0,259584.0,0.0,0.0,24,34
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.6540933775901795,0.0018141662033513682,0.042593029985566515,242.0,0.0,0.0,604032.0,0.0,0.0,24,35
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.30986916065216064,0.0006939719282779834,0.026343346945253244,111.0,0.0,0.0,277056.0,0.0,0.0,24,37
Random Action Value Iteration,True,False,0.10020963191986083,0.00016998532093591624,0.013037841881842111,158.0,336.0,18.33030277982336,54855.0,39992400.0,6323.95445903906,24,24
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.11163128614425659,8.592789171572691e-05,0.009269729862068631,150.0,0.0,0.0,52095.0,0.0,0.0,24,24
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.13243045091629027,0.00025160144700209344,0.015861949659549845,187.5,468.75,21.650635094610966,65032.5,55792968.75,7469.4691076407835,24,24
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1119605016708374,6.95056971457916e-05,0.008337007685362392,150.0,0.0,0.0,52095.0,0.0,0.0,24,24
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1188696002960205,9.733312844687136e-05,0.00986575534091898,150.5,24.75,4.9749371855331,52267.5,2945868.75,1716.3533290089194,24,24
Q-factor Value Iteration,True,False,0.0318797492980957,6.445141072526894e-06,0.0025387282392030255,30.0,0.0,0.0,32400.0,0.0,0.0,24,24
Q-factor Stochastic Value Iteration,True,True,0.34243184328079224,0.00045883848374089097,0.02142051548728207,104.0,0.0,0.0,259584.0,0.0,0.0,24,38
