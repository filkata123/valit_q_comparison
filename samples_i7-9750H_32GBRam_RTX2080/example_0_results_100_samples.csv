Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.288227859,6.555462358348905e-05,0.008096581,999,0,0,78510.82,42092.66760000001,205.16497654326872,18,18
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.18132274150848388,4.076601785584443e-05,0.006384827,999,0,0,49067.67,19429.2611,139.38888442053047,18,18
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.14340917587280275,4.7355573421873484e-05,0.006881539,999,0,0,38642.96,26161.0984,161.74392847955684,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.138486168,3.959733383350681e-05,0.006292641,999,0,0,37334.64,23204.6704,152.33079268486722,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",TRUE,FALSE,0.13750399112701417,2.0760009855871434e-05,0.004556315,999,0,0,37364.68,27911.4576,167.06722479289587,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",FALSE,TRUE,1.5803507804870605,0.006061097,0.07785305,999,0,0,500000,0,0,11,11
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",TRUE,FALSE,0.13084466695785524,0.000105548,0.010273641554743071,999,0,0,37407.42,25698.623600000003,160.3079024876815,18,18
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",TRUE,FALSE,0.3197795414924622,0.000646085,0.025418209,999,0,0,90904.65,36760.2475,191.72962082057117,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",TRUE,FALSE,0.12638132333755492,1.8382009756913933e-05,0.004287425,999,0,0,37353.39,26159.9179,161.7402791514841,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",TRUE,FALSE,0.1268461012840271,3.8838302748325754e-05,0.006232038,999,0,0,37389.65,35965.9475,189.6469021629407,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",TRUE,FALSE,0.1249741244316101,1.4827424822050261e-05,0.00385064,999,0,0,37379.41,34380.8019,185.4206080779588,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",TRUE,FALSE,0.1255717968940735,3.126970334084831e-05,0.005591932,999,0,0,37376.05,28882.3475,169.94807295171074,18,18
No-discounting Q-learning,TRUE,FALSE,0.12806288480758668,3.5927305929197926e-05,0.005993939,999,0,0,37414.57,22312.965099999998,149.3752492884949,18,18
"No-discounting, no stochastic approximation Q-learning",TRUE,FALSE,0.12612841606140138,3.342631996893033e-05,0.00578155,999,0,0,36862.73,36225.397099999995,190.32970629935832,18,18
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",FALSE,TRUE,1.5261319017410278,0.000819346,0.028624225,999,0,0,500000,0,0,11,11
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",TRUE,FALSE,0.12903554916381835,3.349509718859735e-05,0.005787495,999,0,0,37406.99,24847.549899999998,157.6310562674754,18,18
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",TRUE,FALSE,0.12876226425170897,2.8889137709802526e-05,0.005374862,999,0,0,37387.74,27706.772399999998,166.4535142314514,18,18
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",TRUE,FALSE,0.13006002426147462,5.813060859177313e-05,0.007624343,999,0,0,37405.81,23245.3339,152.4642053073442,18,18
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",TRUE,FALSE,0.12929075956344604,3.603364968668643e-05,0.006002803,999,0,0,37394.63,28403.853100000004,168.53442704682033,18,18
cost-based no-discounting Q-learning,TRUE,FALSE,0.13154289722442628,3.8418763661525194e-05,0.006198287,999,0,0,37339.62,24239.4156,155.69012685459538,18,18
"cost-based Q-learning (No discounting, no stochastic approximation)",TRUE,FALSE,0.1292031478881836,3.4484462617092505e-05,0.005872347,999,0,0,36861.71,38953.365900000004,197.36607079232238,18,18
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",TRUE,FALSE,0.12982667684555055,6.166153060505053e-05,0.007852486,999,0,0,36857.11,38002.15790000001,194.94142171431912,18,18
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",TRUE,FALSE,1.810106747150421,0.000646206,0.025420574,999,0,0,500000,0,0,18,18
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",FALSE,TRUE,1.7131019091606141,0.000647592,0.025447821395456014,999,0,0,500000,0,0,2,15
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",TRUE,FALSE,3.948530607223511,0.056309610001193375,0.23729646015310338,999,0,0,1303577.02,856045386.4995999,29258.253305684535,18,18
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",TRUE,FALSE,0.11905464172363281,0.000168659,0.012986862603227263,999,0,0,32129,0,0,18,18
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",TRUE,FALSE,1.1863316869735718,0.006075347,0.077944511,0,0,0,400000,0,0,18,18
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",TRUE,FALSE,0.060044457912445066,2.425145149720151e-05,0.004924576,112,0,0,17050,0,0,18,18
Don't care Q-learning,TRUE,FALSE,1.2781029343605042,0.022155105196739414,0.14884591091709376,14999,0,0,385613.84,1524892962,39049.877875281505,18,30
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",TRUE,TRUE,0.080342207,6.757113677795133e-05,0.008220166,101.53,36.8891,6.073639765,18425,105692.16,325.10330665805293,18,6034
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",TRUE,TRUE,0.081959555,0.000186621,0.013660928353306661,99.31,33.4539,5.7839346469336945,18243.44,109845.02639999999,331.4287651969877,18,3616
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",TRUE,TRUE,0.080850561,0.000105579,0.010275143,99.16,35.27440000000001,5.939225538738196,18248.2,93020.22000000002,304.9921638337615,18,11105
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",TRUE,TRUE,0.40079148769378664,0.001682972,0.041024042,740.21,101.54589999999999,10.076998561079582,87550.81,149634.3939,386.82605121682275,18,799
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",TRUE,TRUE,0.12593619108200074,0.000266885,0.016336614314347825,218.52,75.4296,8.685021588919627,27905.74,130603.3924,361.3909135548374,18,651
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",TRUE,TRUE,0.088621488,1.9372282148901834e-05,0.004401395,170,77.14,8.782938005018593,21391.46,99934.86839999998,316.12476714107675,18,1474
Value Iteration,TRUE,FALSE,0.024290454387664796,1.9999119499942706e-05,0.004472038,28,0,0,42560,0,0,18,18
Discounted Value Iteration - gamma = 0.8,TRUE,FALSE,0.028552913665771486,3.2111136105186235e-05,0.005666669,28,0,0,42560,0,0,18,18
Discounted Value Iteration - gamma = 0.6,TRUE,FALSE,0.028177754878997804,2.949044956978355e-05,0.005430511,28,0,0,42560,0,0,18,18
Discounted Value Iteration - gamma = 0.5,TRUE,FALSE,0.027371912002563475,1.4520475972722126e-05,0.003810574,28,0,0,42560,0,0,18,18
Stochastic Value Iteration,TRUE,TRUE,0.3806141996383667,0.000112671,0.010614679510975226,97,0,0,419816,0,0,18,30
Discounted Stochastic Value Iteration - gamma = 0.8,TRUE,TRUE,0.33511526107788087,0.00105074,0.032415117,83,0,0,359224,0,0,18,37
Discounted Stochastic Value Iteration - gamma = 0.6,TRUE,TRUE,0.29169180393218996,0.000995297,0.031548323,70,0,0,302960,0,0,18,29
Random Action Value Iteration,TRUE,FALSE,0.11322603464126586,0.000337394,0.018368288495588428,149,199,14.106735979665885,60000,31840000,5642.694391866354,18,18
Random Action Discounted Value Iteration - gamma = 0.8,TRUE,FALSE,0.12250683069229126,9.838455994769788e-05,0.009918899,152,96,9.797958971132712,61200,15360000,3919.183588453085,18,18
Random Action Discounted Value Iteration - gamma = 0.6,TRUE,FALSE,0.1332096290588379,0.000286676,0.016931513696976902,150.5,24.75,4.974937186,60600,3960000,1989.974874,18,18
Random Action Discounted Value Iteration - gamma = 0.5,TRUE,FALSE,0.12614277362823487,0.000129867,0.011395939258086947,150.5,24.75,4.974937186,60600,3960000,1989.974874,18,18
Random Action Discounted Value Iteration - gamma = 0.5,TRUE,FALSE,0.12781668901443483,0.000296902,0.017230847438527503,151,49,7,60800,7840000,2800,18,18
Q-factor Value Iteration,TRUE,FALSE,0.036789281368255614,4.6923915026872006e-05,0.006850103,28,0,0,42560,0,0,18,18
Q-factor Stochastic Value Iteration,TRUE,TRUE,0.4432192635536194,0.001598951,0.03998688,97,0,0,419816,0,0,18,27
Model-free Dijkstra,TRUE,FALSE,0.003593664,7.861030754611421e-07,0.000886625,0,0,0,0,0,0,18,18
