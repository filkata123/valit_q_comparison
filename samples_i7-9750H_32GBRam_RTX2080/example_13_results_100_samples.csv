Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.35161877870559693,0.0005952256732625472,0.024397247247641425,999.0,0.0,0.0,98320.02,25773.7996,160.54220504278618,33,33
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.23965850353240967,0.00014059891349095323,0.01185744127082033,999.0,0.0,0.0,66064.48,26282.0896,162.11751786898293,33,33
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.20070961236953735,0.00012931104207684142,0.011371501311473407,999.0,0.0,0.0,55113.62,23705.415600000004,153.96563122983,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.185227689743042,0.00014437744404644946,0.012015716543196641,999.0,0.0,0.0,53440.52,21967.889600000006,148.21568607944303,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.19202937602996825,0.0001174363619033784,0.01083680589026944,999.0,0.0,0.0,53565.26,27426.692399999996,165.61006128855817,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.6133163905143737,0.005559705887941271,0.07456343532819065,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.1933987021446228,0.00013960422169972733,0.011815423043620881,999.0,0.0,0.0,53479.42,27289.303600000007,165.19474446846064,33,33
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.38890852928161623,0.0006029347588116706,0.024554729866395814,999.0,0.0,0.0,112260.72,19921.481599999996,141.1434787724888,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1874398422241211,0.0001979364305542276,0.014068988256240304,999.0,0.0,0.0,53450.76,20672.862399999995,143.78060508983816,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.19289904117584228,0.00013481058091836077,0.011610795877904355,999.0,0.0,0.0,53460.44,25127.886399999992,158.5177794444522,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.18720920085906984,0.0001431948034684183,0.01196640311323408,999.0,0.0,0.0,53453.16,24248.094399999994,155.7179963909117,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.17760944843292237,0.00010174869339605265,0.010087055734754947,999.0,0.0,0.0,53466.28,20170.6416,142.02338399010213,33,33
No-discounting Q-learning,True,False,0.18871865034103394,0.00016989904106014256,0.013034532636812974,999.0,0.0,0.0,53502.4,19744.4,140.51476790714918,33,33
"No-discounting, no stochastic approximation Q-learning",True,False,0.19520895719528197,0.00010509214072760075,0.010251445787185374,999.0,0.0,0.0,52986.62,33862.1356,184.01667207076648,33,33
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.6054389238357545,0.00754344904424272,0.08685303128989062,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.19013904809951782,0.000173586290529164,0.013175215008840046,999.0,0.0,0.0,53462.38,22818.295599999998,151.0572593422772,33,33
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1941214394569397,0.0001363027758077976,0.01167487797828301,999.0,0.0,0.0,53468.56,24259.846400000002,155.7557267004973,33,33
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.18810933589935303,0.00015617624465064637,0.01249704943779316,999.0,0.0,0.0,53457.08,23624.513600000002,153.7026792219316,33,33
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.18914865732192992,0.00012060033939343954,0.010981818583160056,999.0,0.0,0.0,53419.34,30111.764400000004,173.52741685393696,33,33
cost-based no-discounting Q-learning,True,False,0.18940901517868042,0.00016861589693405106,0.012985218401476775,999.0,0.0,0.0,53450.46,24891.988400000002,157.77195061226823,33,33
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.19689900875091554,7.884447542721773e-05,0.008879441166380784,999.0,0.0,0.0,52986.02,32614.119599999998,180.59379723567474,33,33
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.19348918914794921,0.00011555957323421354,0.010749863870496852,999.0,0.0,0.0,53000.0,29636.08,172.1513287779098,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.894187777042389,0.004410772413845797,0.06641364629235318,999.0,0.0,0.0,500000.0,0.0,0.0,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7841018271446227,0.007738253863432481,0.08796734543813677,999.0,0.0,0.0,500000.0,0.0,0.0,2,10
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,7.075192446708679,0.08064948116932212,0.2839885229535203,999.0,0.0,0.0,2317991.82,560583724.7276,23676.649356013193,33,33
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1753295588493347,8.541901236009722e-05,0.009242240656902266,999.0,0.0,0.0,47708.0,0.0,0.0,33,33
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1929496026039124,0.004661440560506996,0.06827474321084626,0.0,0.0,0.0,400000.0,0.0,0.0,33,33
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.07190956115722656,2.8061302805690503e-05,0.005297292025713752,130.0,0.0,0.0,19900.0,0.0,0.0,33,33
Don't care Q-learning,True,False,2.407799165248871,0.0693793531134782,0.26339960727662104,14999.0,0.0,0.0,719287.25,5212780772.2475,72199.58983434393,33,49
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",False,True,0.4037388825416565,0.0017431820673549456,0.04175143191981499,999.0,0.0,0.0,89561.9,3897199.5499999993,1974.1326069947781,38,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.4006805396080017,0.0007408027051184206,0.027217691032091988,999.0,0.0,0.0,92818.16,3408184.5543999993,1846.1269063636983,37,1924
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.39159995555877686,0.000776764053257034,0.027870487137060126,999.0,0.0,0.0,92256.24,3323580.6224,1823.0690119685542,39,33330
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.5543508291244507,0.0011472787754695448,0.03387150388556057,999.0,0.0,0.0,129751.86,375990.72039999993,613.1808219440657,33,73
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.32796918153762816,0.00040104904683786914,0.02002620899815712,999.0,0.0,0.0,75502.25,715461.1275,845.8493527218661,33,237
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.3381300282478332,0.0005151180945593466,0.022696213220697117,999.0,0.0,0.0,79994.61,1391996.3179000006,1179.8289358631616,35,4826
Value Iteration,True,False,0.021650171279907225,5.789147437553766e-06,0.002406064720150679,28.0,0.0,0.0,30184.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.8,True,False,0.16234917879104616,0.0001657502003862362,0.012874400971937925,224.0,0.0,0.0,241472.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.6,True,False,0.07115959882736206,5.541478093447836e-05,0.0074441104864502354,101.0,0.0,0.0,108878.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.5,True,False,0.05678021192550659,2.777532250826766e-05,0.005270229834482332,75.0,0.0,0.0,80850.0,0.0,0.0,33,33
Stochastic Value Iteration,True,True,0.30099010467529297,0.0005432454348488136,0.023307626109254746,104.0,0.0,0.0,284336.0,0.0,0.0,33,46
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.67183025598526,0.0022384860253885732,0.047312641285269345,247.0,0.0,0.0,675298.0,0.0,0.0,33,45
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.319268479347229,0.0005838677524989407,0.02416335557200077,113.0,0.0,0.0,308942.0,0.0,0.0,33,52
Random Action Value Iteration,True,False,0.10337978839874268,0.00022258223863175317,0.014919190280700665,160.5,414.75,20.3654118544163,52003.0,43002939.0,6557.662617122049,33,33
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.12790975570678711,0.00027710223063081684,0.016646387915425283,159.5,384.75,19.615045245933032,51681.0,39892419.0,6316.044569190436,33,33
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.14793960571289064,0.000130448472609578,0.011421404143518344,199.5,24.75,4.9749371855331,64561.0,2566179.0,1601.9297737416582,33,33
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.12144954442977905,0.00025327622449787556,0.015914654394547045,160.5,414.75,20.3654118544163,52003.0,43002939.0,6557.662617122049,33,33
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.12194902181625367,0.00017898870580422107,0.013378666069688004,157.0,301.0,17.349351572897472,50876.0,31208884.0,5586.491206472986,33,33
Q-factor Value Iteration,True,False,0.028170700073242187,1.0198234680819953e-05,0.003193467501137275,28.0,0.0,0.0,30184.0,0.0,0.0,33,33
Q-factor Stochastic Value Iteration,True,True,0.3465148067474365,0.0006753951171916925,0.025988365034986187,104.0,0.0,0.0,284336.0,0.0,0.0,33,48
Model-free Dijkstra,True,False,0.0027094531059265138,5.260417945635255e-07,0.0007252873875668359,0.0,0.0,0.0,0.0,0.0,0.0,33,33
