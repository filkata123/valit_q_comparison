Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.504125747680664,0.0002636113472186935,0.016236112441674378,999.0,0.0,0.0,499972.22,2428.3115999999995,49.277901741044126,2,8
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.516366057395935,0.0015238819950077185,0.03903693116790456,999.0,0.0,0.0,499929.66,6465.364400000002,80.4074897008979,19,20
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5092803359031677,0.00039212742394553854,0.019802207552329577,999.0,0.0,0.0,499929.94,6559.956400000001,80.99355776850403,18,27
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5021147298812867,0.0007126179363903932,0.026694904689666777,999.0,0.0,0.0,499989.38,766.2955999999999,27.682044722166026,2,2
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.4958191585540772,0.00039022125436199534,0.019754018688914805,999.0,0.0,0.0,499992.42,500.9036000000001,22.38087576481314,2,2
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.4961484122276305,0.00463603336897433,0.06808842316410572,999.0,0.0,0.0,490783.92,419136.9536,647.4078726737882,2,2
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.4609447741508483,0.004113997468679707,0.06414045111066578,999.0,0.0,0.0,461933.42,1162183.7836000002,1078.046280824715,2,2
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.491406624317169,0.0056935939869059175,0.07545590756796924,999.0,0.0,0.0,474218.96,1440515.3984,1200.2147301212397,2,2
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.527483503818512,0.0070617921070516496,0.08403446975528345,999.0,0.0,0.0,481634.5,805756.27,897.6392761014861,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.542704644203186,0.006139396566204162,0.0783543015169184,999.0,0.0,0.0,484531.58,1036511.9436000001,1018.0923060312361,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.364474515914917,0.0005332512212631626,0.023092232920684883,999.0,0.0,0.0,115672.38,35137.495599999995,187.44998159509112,91,91
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5423420906066894,0.006400830814762912,0.0800051924237603,999.0,0.0,0.0,500000.0,0.0,0.0,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.36807807922363284,0.000521304105930676,0.022832085010587096,999.0,0.0,0.0,115545.62,29100.455599999998,170.58855647434268,91,91
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.5274983739852905,0.0008981648856233959,0.02996939915352652,999.0,0.0,0.0,170546.1,25291.870000000006,159.03417871640048,91,91
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.5609075379371644,0.006968264835290772,0.0834761333273815,999.0,0.0,0.0,494357.38,429863.7356000001,655.6399435665891,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.36933533430099486,0.0005152632093677938,0.022699409890298772,999.0,0.0,0.0,115584.08,26443.5936,162.61486278935269,91,91
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.3572950553894043,0.0009343795677030811,0.030567622866410158,999.0,0.0,0.0,115519.08,32557.313599999998,180.4364530797477,91,91
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.3627473020553589,0.000527135736988862,0.02295943677420816,999.0,0.0,0.0,115564.66,32335.68440000001,179.82125680797589,91,91
No-discounting Q-learning,True,False,0.34912399291992186,0.00033169074305274075,0.018212378841127284,999.0,0.0,0.0,115561.2,31416.960000000006,177.24830041498285,91,91
"No-discounting, no stochastic approximation Q-learning",True,False,0.3548868989944458,0.0004324873720695677,0.020796330735722773,999.0,0.0,0.0,115302.92,44533.07359999998,211.02860848709585,91,91
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6122664165496827,0.003880220674469092,0.062291417341950824,999.0,0.0,0.0,499960.34,3438.9644000000008,58.64268411319524,2,11
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6263633608818053,0.003089719529840949,0.0555852456128508,999.0,0.0,0.0,499925.48,6436.609600000002,80.22848371993578,20,20
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6188805651664735,0.002427078791715593,0.049265391419490344,999.0,0.0,0.0,499944.26,4201.0924,64.81583448510094,18,38
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.605372087955475,0.004695761578974788,0.06852562716951073,999.0,0.0,0.0,499992.62,935.4155999999999,30.584564734519272,2,2
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5901686668395996,0.002902433210999789,0.05387423513145954,999.0,0.0,0.0,499996.98,88.75959999999999,9.421231342027431,2,2
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.585439190864563,0.0024016332567257676,0.04900646137730991,999.0,0.0,0.0,490636.34,595946.1644,771.9754946887887,2,2
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.4761687588691712,0.0027877576317808404,0.052799219992163145,999.0,0.0,0.0,461928.8,1159059.84,1076.5964146327071,2,2
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.5144258689880372,0.0038507117725106125,0.06205410359122604,999.0,0.0,0.0,474033.06,1313830.7563999998,1146.2245663045264,2,2
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.528709387779236,0.002900842983595453,0.05385947440883037,999.0,0.0,0.0,481389.06,947234.3564,973.2596551794387,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.5505880045890807,0.0024627226071714347,0.04962582600996617,999.0,0.0,0.0,484750.3,825367.7100000002,908.4975013724585,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.36858153104782104,0.00033763841794040556,0.01837493994385847,999.0,0.0,0.0,115629.68,29727.657600000013,172.41710355994272,91,91
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.5378204870223999,0.0004338435147585415,0.020828910551407665,999.0,0.0,0.0,170559.28,33559.4816,183.1924714610292,91,91
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.5826802253723145,0.004298837841806744,0.0655655232710511,999.0,0.0,0.0,494221.46,526398.7084,725.5333957854731,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5928498339653014,0.0031293255881658464,0.05594037529518234,999.0,0.0,0.0,499832.54,12847.348399999995,113.34614417791191,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5858042383193969,0.002131107956208575,0.046163924835401236,999.0,0.0,0.0,499981.46,1166.8684,34.15945549917328,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.5843834972381592,0.002317263557185834,0.04813796378312894,999.0,0.0,0.0,499999.9,0.9900000000000001,0.99498743710662,2,2
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.600207233428955,0.002911835370225617,0.05396142483502096,999.0,0.0,0.0,499999.58,17.46360000000001,4.178947235847805,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5720190358161927,0.007354491863959372,0.0857583340787318,999.0,0.0,0.0,500000.0,0.0,0.0,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.37423498153686524,0.0005469130143866096,0.023386171434987165,999.0,0.0,0.0,115549.74,40234.6924,200.58587288241412,91,91
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.3750650024414062,0.0005046163163325673,0.022463666582563213,999.0,0.0,0.0,115595.26,31266.29239999999,176.8227711580157,91,91
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.36316452741622923,0.0004846640123483383,0.022015086017282293,999.0,0.0,0.0,115571.92,28656.953600000004,169.28364835387973,91,91
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.3650500249862671,0.0005444116786993391,0.023332631199659826,999.0,0.0,0.0,115527.96,24261.678399999993,155.76160759314214,91,91
cost-based no-discounting Q-learning,True,False,0.3608440494537353,0.0003945910154192461,0.019864315125854355,999.0,0.0,0.0,115547.5,36719.95,191.62450260861735,91,91
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.3715820026397705,0.0005900668754273738,0.02429129217286256,999.0,0.0,0.0,115293.28,37345.961599999995,193.25103259749997,91,91
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.37599748611450196,0.00044063389917951097,0.020991281503984244,999.0,0.0,0.0,115313.92,39868.9536,199.67211522894226,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.6351820468902587,0.0066197434109699325,0.0813618056029359,999.0,0.0,0.0,500000.0,0.0,0.0,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.5710034561157227,0.006425045073959471,0.08015637887254808,999.0,0.0,0.0,500000.0,0.0,0.0,2,6
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,8.20729751586914,0.09379228624094026,0.30625526320528806,999.0,0.0,0.0,2945677.12,69122615.7856,8314.001189896475,91,91
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,10.16040864944458,0.5780624396079331,0.7603041757138607,999.0,0.0,0.0,2940099.84,18403458.1344,4289.92518983723,91,91
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.3176527428627014,0.00027753805814888325,0.016659473525561463,999.0,0.0,0.0,103820.0,0.0,0.0,91,91
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.0871039342880249,0.003351725099599639,0.057894085186654766,0.0,0.0,0.0,400000.0,0.0,0.0,91,91
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.08291082382202149,5.6432628811080575e-05,0.007512165387628295,129.0,0.0,0.0,25520.0,0.0,0.0,91,91
Don't care Q-learning,True,False,7.191102712154389,0.23017441337760217,0.4797649563876067,14999.0,0.0,0.0,2448703.17,21526995164.7011,146720.80685676826,91,101
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",False,True,0.8710803723335266,0.005597842897225171,0.07481873359811145,999.0,0.0,0.0,210920.87,11827546.4331,3439.1200085341598,128,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.28068352460861207,0.002989096041089232,0.05467262606724897,23.05,24.1275,4.911975162803656,11570.0,4436912.54,2106.398001328334,5176,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.8561841630935669,0.005178252834006367,0.07196007805725593,999.0,0.0,0.0,210566.31,11857825.4939,3443.519347106968,137,100002
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.8098467850685119,0.001589906426757915,0.039873630719535876,999.0,0.0,0.0,198119.41,218540.0619,467.48268620345715,95,120
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",False,True,0.24403141260147096,0.006593566952377905,0.08120078172270206,83.95,2327.9475,48.248808275438265,30171.62,126625684.61559999,11252.807854735634,189,100002
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.699006712436676,0.0025937705260257364,0.050929073484854766,999.0,0.0,0.0,171490.25,4104306.3275,2025.9087658381854,117,96429
Value Iteration,True,False,0.017980587482452393,3.036336267285833e-06,0.0017425086132601562,56.0,0.0,0.0,18928.0,0.0,0.0,91,91
Discounted Value Iteration - gamma = 0.8,True,False,0.02408994197845459,4.987283021387157e-06,0.0022332225642302554,56.0,0.0,0.0,18928.0,0.0,0.0,91,91
Discounted Value Iteration - gamma = 0.6,False,True,0.02386000156402588,6.782239677818327e-06,0.0026042733492892653,58.0,0.0,0.0,19604.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.5,False,True,0.024899511337280272,9.3308913884357e-06,0.0030546507801114844,58.0,0.0,0.0,19604.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.3,False,True,0.018200693130493165,8.195847875867911e-07,0.0009053092220820415,45.0,0.0,0.0,15210.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.1,False,True,0.010280296802520753,7.030657544589757e-07,0.000838490163602994,25.0,0.0,0.0,8450.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.01,False,True,0.005000417232513428,2.7962115252080367e-07,0.0005287921638231827,13.0,0.0,0.0,4394.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.0001,False,True,0.002969696521759033,2.2930624106152207e-07,0.00047885931238884986,8.0,0.0,0.0,2704.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.00001,False,True,0.0026503705978393553,5.679296071093631e-07,0.0007536110449756977,6.0,0.0,0.0,2028.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.000001,False,True,0.002399551868438721,2.5908165341093086e-07,0.0005090006418570913,6.0,0.0,0.0,2028.0,0.0,0.0,2,2
Stochastic Value Iteration,True,True,0.15850380897521973,0.00022681947814444357,0.015060527153604005,163.0,0.0,0.0,89976.0,0.0,0.0,95,119
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.114323890209198,6.244577179401744e-05,0.007902263713267068,127.0,0.0,0.0,70104.0,0.0,0.0,95,122
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.08953140735626221,3.5105555472023294e-05,0.005924994132657288,98.0,0.0,0.0,54096.0,0.0,0.0,254,3913
Random Action Value Iteration,True,False,0.08737238883972168,7.254929069349601e-05,0.008517587140352367,254.5,254.75,15.960889699512368,35003.5,4781402.75,2186.6418888331946,91,91
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.11022699356079102,0.00027073408320516136,0.016453999003438687,256.0,414.0,20.346989949375804,35209.0,7770366.0,2787.537623064485,91,91
Random Action Discounted Value Iteration - gamma = 0.6,False,True,0.09255038738250733,0.00014291235356884047,0.011954595500009211,226.0,624.0,24.979991993593593,31099.0,11711856.0,3422.2589031223224,2,2
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.06837085485458375,8.226099459577087e-05,0.009069784705039633,161.0,429.0,20.71231517720798,22194.0,8051901.0,2837.5871792774933,2,2
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.06985071897506714,8.973576142281557e-05,0.009472896147578921,160.0,400.0,20.0,22057.0,7507600.0,2740.0,2,2
Q-factor Value Iteration,True,False,0.022959837913513182,6.59846198507239e-06,0.002568747162542937,56.0,0.0,0.0,18928.0,0.0,0.0,91,91
Q-factor Stochastic Value Iteration,True,True,0.16411139488220214,0.00017400774766165338,0.013191199629360984,163.0,0.0,0.0,89976.0,0.0,0.0,93,130
Model-free Dijkstra,True,False,0.0008628582954406739,1.4490071873183294e-07,0.000380658270279043,136.0,0.0,0.0,333.0,0.0,0.0,91,91
