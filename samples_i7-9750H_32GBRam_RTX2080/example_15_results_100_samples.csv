Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.2510096025466919,0.0002886002362529097,0.016988238173892833,999.0,0.0,0.0,70424.43,12609.105100000002,112.29027161780313,22,26
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16994787693023683,9.553773540781097e-05,0.009774340663584985,999.0,0.0,0.0,46620.43,16884.0851,129.93877442857462,22,22
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.13680599212646485,8.360959057504261e-05,0.009143828004454294,999.0,0.0,0.0,38466.35,27246.9475,165.06649417734658,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.13136905193328857,6.853479656072067e-05,0.008278574548841164,999.0,0.0,0.0,37424.58,22337.803600000003,149.45836744725938,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.12384033203125,1.6351058280861253e-05,0.004043644183266037,999.0,0.0,0.0,37327.85,23503.9275,153.30990672490805,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5617081141471862,0.007883266867144238,0.08878776304843049,999.0,0.0,0.0,500000.0,0.0,0.0,7,7
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.13163007974624633,7.979926030913587e-05,0.008933043171794027,999.0,0.0,0.0,37511.17,22669.301099999997,150.56327938777102,22,22
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.2823510718345642,0.0002791524168695275,0.016707854945190528,999.0,0.0,0.0,79486.73,14712.5571,121.29533008323115,2,24
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.12703952312469483,9.052983563240104e-05,0.009514716792022822,999.0,0.0,0.0,37458.13,25027.2331,158.19997819216033,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.13799961805343627,6.225978077893047e-05,0.007890486726364254,999.0,0.0,0.0,37452.11,27005.037899999996,164.33209637803566,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.12616947650909424,3.741686346190819e-05,0.006116932520627327,999.0,0.0,0.0,37525.27,19515.1971,139.6968041867816,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.12891011714935302,7.726255169311573e-05,0.008789911927494821,999.0,0.0,0.0,37523.65,24841.7875,157.61277708358546,22,22
No-discounting Q-learning,True,False,0.13495151519775392,9.682145312226567e-05,0.009839789282411778,999.0,0.0,0.0,37495.74,21531.412399999997,146.7358592846343,22,22
"No-discounting, no stochastic approximation Q-learning",True,False,0.13691986083984375,7.715034255679712e-05,0.008783526772134136,999.0,0.0,0.0,37010.96,43249.3584,207.96480086783916,22,22
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.608757984638214,0.007674949154114046,0.08760678714639662,999.0,0.0,0.0,500000.0,0.0,0.0,7,7
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.13143971920013428,9.027613885111804e-05,0.009501375629408514,999.0,0.0,0.0,37554.73,18588.157099999997,136.3383918784434,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.13545960903167725,7.259754394974608e-05,0.008520419235562654,999.0,0.0,0.0,37468.44,29357.306399999998,171.33973969864667,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.14103912591934203,5.0754020767789136e-05,0.007124185621373796,999.0,0.0,0.0,37564.43,22371.865100000003,149.5722738344243,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.13459975242614747,9.472039254328591e-05,0.009732440215243344,999.0,0.0,0.0,37532.9,27775.93,166.66112324114462,22,22
cost-based no-discounting Q-learning,True,False,0.1365075922012329,9.537771501779843e-05,0.00976615149471881,999.0,0.0,0.0,37523.36,22624.2904,150.4137307562046,22,22
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.13928942441940306,5.9811094203831775e-05,0.007733763262722216,999.0,0.0,0.0,36990.14,41740.3404,204.30452858417016,22,22
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1342501449584961,9.150300419682937e-05,0.009565720265449402,999.0,0.0,0.0,37009.05,38958.00749999999,197.37782930207737,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,2.000145206451416,0.5968236664093216,0.7725436339840758,999.0,0.0,0.0,500000.0,0.0,0.0,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.773892216682434,0.006655854466027428,0.08158342028885175,999.0,0.0,0.0,500000.0,0.0,0.0,2,15
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,3.5300783705711365,0.03347084141639136,0.18295037965631927,999.0,0.0,0.0,1139969.52,805773112.7696,28386.142970992027,22,22
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.12402063369750976,5.8741961334044384e-05,0.007664330455691768,999.0,0.0,0.0,33572.0,0.0,0.0,22,22
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.2205360412597657,0.003707099495466082,0.06088595482922216,0.0,0.0,0.0,400000.0,0.0,0.0,22,22
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.06228020191192627,1.7085765154547513e-05,0.004133493093564753,239.0,0.0,0.0,17612.0,0.0,0.0,22,22
Don't care Q-learning,True,False,1.4517656826972962,0.17890372656774153,0.4229701249116083,14999.0,0.0,0.0,385513.21,1233879963.0259,35126.627549850265,22,30
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.2349596381187439,0.0002073163083120733,0.01439848284758062,999.0,0.0,0.0,56595.46,1436155.7484,1198.397158040689,22,451
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.24846557140350342,0.0003158280699324451,0.017771552265698265,999.0,0.0,0.0,57214.18,1432400.8876,1196.8295148432796,22,972
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.24074481964111327,0.0002808327196361461,0.01675806431650583,999.0,0.0,0.0,56473.39,1581884.1779000002,1257.7297714135577,22,1198
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",False,True,0.3987902569770813,0.0013685344709679778,0.036993708532235284,999.0,0.0,0.0,91055.14,251707.50040000002,501.7045947567154,22,100002
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.21318895101547242,0.0002379646152813336,0.015426101752592377,999.0,0.0,0.0,50241.86,169424.70040000003,411.6123180858416,22,123
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.22201984167098998,0.00014918989879481045,0.012214331696609949,999.0,0.0,0.0,50122.65,526875.1675000001,725.8616724280186,22,431
Value Iteration,True,False,0.022200088500976562,5.820357183301895e-06,0.002412541643848225,28.0,0.0,0.0,34888.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.8,True,False,0.026500816345214843,6.8221935629026125e-06,0.002611932916998944,28.0,0.0,0.0,34888.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.6,True,False,0.028620107173919676,7.096905276574717e-06,0.002664001741098289,28.0,0.0,0.0,34888.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.5,True,False,0.0250650954246521,7.3978976839214305e-06,0.0027199076609181845,28.0,0.0,0.0,34888.0,0.0,0.0,22,22
Stochastic Value Iteration,True,True,0.33850950717926026,0.0006390573083781191,0.02527958283631514,99.0,0.0,0.0,320364.0,0.0,0.0,22,33
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.28413111209869385,0.00040279488289527305,0.02006975044426993,84.0,0.0,0.0,271824.0,0.0,0.0,22,34
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.24424965620040895,0.00026043835656388984,0.01613810263209061,71.0,0.0,0.0,229756.0,0.0,0.0,22,31
Random Action Value Iteration,True,False,0.09941036939620972,4.746486931787217e-05,0.006889475257076708,150.0,0.0,0.0,54360.0,0.0,0.0,22,22
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.12003182411193848,7.503150122479384e-05,0.008662072570972483,150.0,0.0,0.0,54360.0,0.0,0.0,22,22
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.12225026607513428,5.829262988479514e-05,0.0076349610270645876,150.0,0.0,0.0,54360.0,0.0,0.0,22,22
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.11908977270126343,0.0001439649871341487,0.011998541041899582,150.5,74.75,8.645808232895291,54540.0,9687600.0,3112.490963842305,22,22
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.12114950656890869,0.00019432449704029293,0.013940032175009243,150.5,24.75,4.9749371855331,54540.0,3207600.0,1790.977386791916,22,22
Q-factor Value Iteration,True,False,0.030899875164031983,7.246419897290934e-06,0.00269191751309191,28.0,0.0,0.0,34888.0,0.0,0.0,22,22
Q-factor Stochastic Value Iteration,True,True,0.38972987651824953,0.00030616475205895314,0.01749756417502028,99.0,0.0,0.0,320364.0,0.0,0.0,22,30
Model-free Dijkstra,True,False,0.0034850215911865234,1.2390649273129387e-06,0.0011131329333520497,0.0,0.0,0.0,0.0,0.0,0.0,22,22
