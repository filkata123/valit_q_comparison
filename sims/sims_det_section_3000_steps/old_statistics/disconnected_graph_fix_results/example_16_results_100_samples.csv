Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Avg convergence action,Var convergence actio ,STD convergence action,Convergence rate,Shortest Path,Longest Path
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0",True,False,0.01554018497467041,4.184281256834765e-06,0.002045551577652044,999.0,0.0,0.0,4020.0,0.0,0.0,0.0,0.0,0.0,0.0,5,5
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.25",True,False,0.019589309692382813,6.522970762125625e-06,0.002554010720832163,943.53,93.00909999999998,9.644122562472958,5002.1,4.35,2.085665361461421,5000.0,0.0,0.0,1.0,5,5
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.5",True,False,0.01962420463562012,7.764426540325076e-06,0.002786472059850067,660.94,163.63640000000004,12.792044402674657,5003.44,13.9264,3.731809212700992,5000.0,0.0,0.0,1.0,5,5
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.75",True,False,0.018888673782348632,6.436495900425144e-06,0.0025370250098146734,411.0,170.32,13.05067048086036,5007.8,71.16,8.435638683585257,5000.0,0.0,0.0,1.0,5,5
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.9",True,False,0.01886953115463257,6.84378105958672e-06,0.0026160621283881466,282.11,139.87789999999998,11.82699877399165,5012.94,168.99640000000002,12.999861537724163,5000.0,0.0,0.0,1.0,5,5
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.018679797649383545,6.994909138455796e-06,0.002644789053678156,209.4,179.48,13.39701459281134,5020.32,382.2176000000001,19.55038618544401,5000.0,0.0,0.0,1.0,5,5
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.02123088836669922,1.5146670236845238e-05,0.003891872330491487,211.82,111.58760000000002,10.563503206796504,5018.48,334.7296,18.29561696144735,5000.0,0.0,0.0,1.0,5,5
Model-free Dijkstra,True,False,2.9621124267578124e-05,2.8378382376104136e-08,0.00016845884475474754,6.0,0.0,0.0,28.0,0.0,0.0,9.0,0.0,0.0,1.0,5,5
Model-free Value Iteration,True,False,7.997989654541015e-05,7.35700125687799e-08,0.0002712379261253483,5.0,0.0,0.0,69.0,0.0,0.0,50.0,0.0,0.0,1.0,5,5
Model-free Synchronous Value Iteration,True,False,0.00010001420974731446,9.002588910220766e-08,0.0003000431454011367,6.0,0.0,0.0,79.0,0.0,0.0,60.0,0.0,0.0,1.0,5,5
