Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.1869780468940735,33,False,33,33
No-discounting Q-learning,0.18154457092285156,33,False,33,33
"No-discounting, no stochastic approximation Q-learning",0.1782397747039795,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation)",0.17814297199249268,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.1771109676361084,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.856738986968994,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.7047547435760497,0,False,0,0
Fully-random exploration Q-learning,6.708814833164215,33,False,33,33
Fully-greedy exploration Q-learning,0.1600097370147705,33,False,33,33
One-episode random-exploration Q-learning,1.125396637916565,33,False,33,33
Fully-greedy Q-learning with convergence,0.0687663745880127,33,False,33,33
Don't care Q-learning,2.2839480304718016,37,True,35,49
Stochastic Q-learning (converging),0.07152945518493653,36,True,103036,87
Value Iteration,0.016921024322509765,33,False,33,33
Random Action Value Iteration,0.09504651069641114,33,False,33,33
Stochastic Value Iteration,0.29723951816558836,40,True,33,48
