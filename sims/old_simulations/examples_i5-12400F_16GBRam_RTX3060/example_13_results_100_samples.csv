Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.13512809991836547,33,False,33,33
No-discounting Q-learning,0.13111862659454346,33,False,33,33
"No-discounting, no stochastic approximation Q-learning",0.13574759244918824,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation)",0.13837315797805785,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.14023520469665526,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.3402546763420105,33,False,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.215508062839508,0,False,0,0
Fully-random exploration Q-learning,4.78764586687088,33,False,33,33
Fully-greedy exploration Q-learning,0.1186893081665039,33,False,33,33
One-episode random-exploration Q-learning,0.8778689289093018,33,False,33,33
Fully-greedy Q-learning with convergence,0.045814330577850344,33,False,33,33
Don't care Q-learning,1.7375912499427795,41,True,33,51
Stochastic Q-learning (converging),0.1274872875213623,37,True,108,98
Value Iteration,0.011924982070922852,33,False,33,33
Random Action Value Iteration,0.06681066274642944,33,False,33,33
Stochastic Value Iteration,0.19856707096099854,35,True,33,49
