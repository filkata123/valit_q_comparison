Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.11487388610839844,29,False,29,29
No-discounting Q-learning,0.10461653232574462,29,False,29,29
"No-discounting, no stochastic approximation Q-learning",0.10212956190109253,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation)",0.09972580432891846,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.10088333129882812,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.3095015144348146,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.2109817743301392,0,False,0,0
Fully-random exploration Q-learning,4.283622984886169,29,False,29,29
Fully-greedy exploration Q-learning,0.08965451240539551,29,False,29,29
One-episode random-exploration Q-learning,0.8512453937530517,29,False,29,29
Fully-greedy Q-learning with convergence,0.029659156799316407,29,False,29,29
Don't care Q-learning,1.3608527565002442,37,True,29,43
Stochastic Q-learning (converging),0.035279674530029295,29,True,103324,99
Value Iteration,0.011417076587677003,29,False,29,29
Random Action Value Iteration,0.06324564695358276,29,False,29,29
Stochastic Value Iteration,0.21976094961166381,36,True,29,41
