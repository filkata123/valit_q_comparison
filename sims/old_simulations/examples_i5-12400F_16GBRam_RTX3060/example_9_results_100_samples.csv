Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.24489282131195067,73,False,73,73
No-discounting Q-learning,0.24532982110977172,73,False,73,73
"No-discounting, no stochastic approximation Q-learning",0.24506059646606446,73,False,73,73
"cost-based Q-learning (No discounting, no stochastic approximation)",0.24724245548248291,73,False,73,73
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.24726797819137572,73,False,73,73
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.1509063291549682,73,False,73,73
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.2102039074897766,0,False,0,0
Fully-random exploration Q-learning,6.184115283489227,73,False,73,73
Fully-greedy exploration Q-learning,0.2170017957687378,73,False,73,73
One-episode random-exploration Q-learning,0.8149658870697022,73,False,73,73
Fully-greedy Q-learning with convergence,0.07779695749282838,73,False,73,73
Don't care Q-learning,4.733746054172516,77,True,73,91
Stochastic Q-learning (converging),0.08498987913131714,85,True,103,98
Value Iteration,0.009308125972747803,73,False,73,73
Random Action Value Iteration,0.0718074893951416,73,False,73,73
Stochastic Value Iteration,0.14828960180282594,83,True,75,94
