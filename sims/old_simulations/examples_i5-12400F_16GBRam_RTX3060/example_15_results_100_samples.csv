Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.088165442943573,22,False,22,22
No-discounting Q-learning,0.08990867614746094,22,False,22,22
"No-discounting, no stochastic approximation Q-learning",0.0877770733833313,22,False,22,22
"cost-based Q-learning (No discounting, no stochastic approximation)",0.09036765098571778,22,False,22,22
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.0984272289276123,22,False,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.3525806713104247,22,False,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.2624797081947328,0,False,0,0
Fully-random exploration Q-learning,2.460115807056427,22,False,22,22
Fully-greedy exploration Q-learning,0.0844562029838562,22,False,22,22
One-episode random-exploration Q-learning,0.8561696243286133,22,False,22,22
Fully-greedy Q-learning with convergence,0.03754557847976685,22,False,22,22
Don't care Q-learning,0.9124745607376099,24,True,22,32
Stochastic Q-learning (converging),0.13339334964752197,23,True,101,99712
Value Iteration,0.013357653617858886,22,False,22,22
Random Action Value Iteration,0.06939047336578369,22,False,22,22
Stochastic Value Iteration,0.2179862380027771,22,True,22,31
