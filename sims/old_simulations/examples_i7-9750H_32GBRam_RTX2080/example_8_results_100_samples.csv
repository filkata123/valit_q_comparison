Algorithm,Avg Time,Inconsistent?,Best Path,Worst Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.01939180612564087,False,6,6
No-discounting Q-learning,0.01990469217300415,False,6,6
"No-discounting, no stochastic approximation Q-learning",0.019853436946868898,False,6,6
"cost-based Q-learning (No discounting, no stochastic approximation)",0.01999354124069214,False,6,6
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.020043673515319823,False,6,6
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.880303030014038,False,6,6
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.7334724903106689,True,0,6
Fully-random exploration Q-learning,0.14310650110244752,False,6,6
Fully-greedy exploration Q-learning,0.01863729476928711,False,6,6
One-episode random-exploration Q-learning,1.232133276462555,False,6,6
Fully-greedy Q-learning with convergence,0.0005387234687805176,False,6,6
Don't care Q-learning,0.30061983346939086,True,6,10
Stochastic Q-learning (converging),0.0007165813446044922,True,6,97
Value Iteration,0.0014447641372680663,False,6,6
Random Action Value Iteration,0.014973688125610351,False,6,6
Stochastic Value Iteration,0.01781538248062134,True,6,14
