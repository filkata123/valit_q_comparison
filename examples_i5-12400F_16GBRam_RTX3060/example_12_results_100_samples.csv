Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.3202529811859131,91,False,91,91
No-discounting Q-learning,0.3145876288414001,91,False,91,91
"No-discounting, no stochastic approximation Q-learning",0.3173933863639832,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation)",0.31761812210083007,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.33171079874038695,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.1769903922080993,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.1632464337348938,0,False,0,0
Fully-random exploration Q-learning,6.119415888786316,91,True,0,91
Fully-greedy exploration Q-learning,0.28989208698272706,91,False,91,91
One-episode random-exploration Q-learning,0.8141144013404846,91,True,0,91
Fully-greedy Q-learning with convergence,0.12620245218276976,91,False,91,91
Don't care Q-learning,8.568406021595,101,True,101,99
Stochastic Q-learning (converging),0.13812182188034058,106,True,100,99
Value Iteration,0.019455642700195314,91,False,91,91
Random Action Value Iteration,0.10877971649169922,91,False,91,91
Stochastic Value Iteration,0.24509377479553224,105,True,100,99
