Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.24539503574371338,91,False,91,91
No-discounting Q-learning,0.24514652013778687,91,False,91,91
"No-discounting, no stochastic approximation Q-learning",0.24493613004684447,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation)",0.24503058195114136,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.24523229122161866,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.0673739051818847,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.0471172475814818,0,False,0,0
Fully-random exploration Q-learning,5.375467681884766,91,False,91,91
Fully-greedy exploration Q-learning,0.21279895544052124,91,False,91,91
One-episode random-exploration Q-learning,0.7204749131202698,91,False,91,91
Fully-greedy Q-learning with convergence,0.055544567108154294,91,False,91,91
Don't care Q-learning,4.804698739051819,95,True,101,99
Stochastic Q-learning (converging),0.05811763286590576,288,True,100,99
Value Iteration,0.01030820608139038,91,False,91,91
Random Action Value Iteration,0.05343615055084228,91,False,91,91
Stochastic Value Iteration,0.09085413217544555,98,True,100,99
