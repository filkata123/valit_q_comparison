Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.10934154748916626,30,False,30,30
No-discounting Q-learning,0.10928825855255127,30,False,30,30
"No-discounting, no stochastic approximation Q-learning",0.10889477014541626,30,False,30,30
"cost-based Q-learning (No discounting, no stochastic approximation)",0.10858037710189819,30,False,30,30
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.11023123025894165,30,False,30,30
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.205660216808319,30,False,30,30
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.1286738443374633,0,False,0,0
Fully-random exploration Q-learning,3.5019738411903383,30,False,30,30
Fully-greedy exploration Q-learning,0.09662056684494019,30,False,30,30
One-episode random-exploration Q-learning,0.7635927081108094,30,False,30,30
Fully-greedy Q-learning with convergence,0.043504643440246585,30,False,30,30
Don't care Q-learning,1.33121155500412,34,True,30,48
Stochastic Q-learning (converging),0.047238249778747556,34,True,105,851
Value Iteration,0.007397668361663818,30,False,30,30
Random Action Value Iteration,0.05685548067092896,30,False,30,30
Stochastic Value Iteration,0.15527633666992188,36,True,30,47
