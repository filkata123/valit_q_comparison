Algorithm,Avg Time,Inconsistent?,Best Path,Worst Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.3705482506752014,False,64,64
No-discounting Q-learning,0.37894017457962037,False,64,64
"No-discounting, no stochastic approximation Q-learning",0.3743315815925598,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation)",0.3820233011245728,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.37972036123275754,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.7656541323661805,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.9004159283638,False,0,0
Fully-random exploration Q-learning,9.534327306747436,False,64,64
Fully-greedy exploration Q-learning,0.3778527855873108,False,64,64
One-episode random-exploration Q-learning,1.2887660336494446,False,64,64
Fully-greedy Q-learning with convergence,0.19910796165466307,False,64,64
Don't care Q-learning,6.3206426978111265,True,70,94
Stochastic Q-learning (converging),0.25976722717285156,True,65,11022
Value Iteration,0.02780019998550415,False,64,64
Random Action Value Iteration,0.15307185888290406,False,64,64
Stochastic Value Iteration,0.36962371587753295,True,66,87
