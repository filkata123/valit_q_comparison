Algorithm,Avg Time,Inconsistent?,Best Path,Worst Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.4354767537117004,False,91,91
No-discounting Q-learning,0.44170164108276366,False,91,91
"No-discounting, no stochastic approximation Q-learning",0.44613749742507935,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation)",0.4681504464149475,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.46717621564865114,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.6358861637115478,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.6623569083213807,False,0,0
Fully-random exploration Q-learning,8.835270566940308,True,0,91
Fully-greedy exploration Q-learning,0.4151690936088562,False,91,91
One-episode random-exploration Q-learning,1.111855206489563,True,0,91
Fully-greedy Q-learning with convergence,0.18092316389083862,False,91,91
Don't care Q-learning,11.33149337053299,True,91,105
Stochastic Q-learning (converging),0.27858486890792844,True,96,810611
Value Iteration,0.029624719619750977,False,91,91
Random Action Value Iteration,0.14875463962554933,False,91,91
Stochastic Value Iteration,0.3327630853652954,True,96,121
