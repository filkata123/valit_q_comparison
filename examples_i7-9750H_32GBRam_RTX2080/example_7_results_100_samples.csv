Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.14517703533172607,29,False,29,29
No-discounting Q-learning,0.1447261619567871,29,False,29,29
"No-discounting, no stochastic approximation Q-learning",0.13995304346084594,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation)",0.13823092460632325,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.14624624252319335,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.8102612137794494,29,False,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.7578409290313721,0,False,0,0
Fully-random exploration Q-learning,5.764040882587433,29,False,29,29
Fully-greedy exploration Q-learning,0.12722557067871093,29,False,29,29
One-episode random-exploration Q-learning,1.1109240341186524,29,False,29,29
Fully-greedy Q-learning with convergence,0.042735261917114256,29,False,29,29
Don't care Q-learning,1.7776425576210022,35,True,29,41
Stochastic Q-learning (converging),0.053584170341491696,67,True,129,93
Value Iteration,0.017586562633514404,29,False,29,29
Random Action Value Iteration,0.08765007495880127,29,False,29,29
Stochastic Value Iteration,0.30627299785614015,32,True,29,42
