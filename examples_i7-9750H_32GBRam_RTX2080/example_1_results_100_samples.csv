Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.14912213563919066,28,False,28,28
No-discounting Q-learning,0.14856598854064942,28,False,28,28
"No-discounting, no stochastic approximation Q-learning",0.15525017261505128,28,False,28,28
"cost-based Q-learning (No discounting, no stochastic approximation)",0.16318676233291626,28,False,28,28
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.16371797323226928,28,False,28,28
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.8395221877098082,28,False,28,28
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.7032668662071229,0,False,0,0
Fully-random exploration Q-learning,4.2362353515625,28,False,28,28
Fully-greedy exploration Q-learning,0.1407538104057312,28,False,28,28
One-episode random-exploration Q-learning,1.1423115086555482,28,False,28,28
Fully-greedy Q-learning with convergence,0.061087348461151124,28,False,28,28
Don't care Q-learning,1.7425540781021118,32,True,28,42
Stochastic Q-learning (converging),0.07195887327194214,33,True,101,95
Value Iteration,0.015718624591827393,28,False,28,28
Random Action Value Iteration,0.08157478094100952,28,False,28,28
Stochastic Value Iteration,0.2587453055381775,30,True,28,39
