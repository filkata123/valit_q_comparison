Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.0969236421585083,18,False,18,18
No-discounting Q-learning,0.09068027973175048,18,False,18,18
"No-discounting, no stochastic approximation Q-learning",0.08687893629074096,18,False,18,18
"cost-based Q-learning (No discounting, no stochastic approximation)",0.0884683871269226,18,False,18,18
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.08904984474182129,18,False,18,18
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.2343988990783692,18,False,18,18
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.1568681836128234,0,False,0,0
Fully-random exploration Q-learning,2.5552444791793825,18,False,18,18
Fully-greedy exploration Q-learning,0.07349177360534669,18,False,18,18
One-episode random-exploration Q-learning,0.8220955967903137,18,False,18,18
Fully-greedy Q-learning with convergence,0.0432590913772583,18,False,18,18
Don't care Q-learning,0.8651569533348084,24,True,20,32
Stochastic Q-learning (converging),0.04045480489730835,18,True,1073,96
Value Iteration,0.013961329460144042,18,False,18,18
Random Action Value Iteration,0.06909056186676026,18,False,18,18
Stochastic Value Iteration,0.23449898719787599,26,True,18,28
