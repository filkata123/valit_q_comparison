Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.009772288799285888,5,False,5,5
No-discounting Q-learning,0.009728817939758301,5,False,5,5
"No-discounting, no stochastic approximation Q-learning",0.009795362949371339,5,False,5,5
"cost-based Q-learning (No discounting, no stochastic approximation)",0.00981811285018921,5,False,5,5
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.009839255809783936,5,False,5,5
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.142671411037445,5,False,5,5
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.054834303855896,0,True,0,5
Fully-random exploration Q-learning,0.046972947120666506,5,False,5,5
Fully-greedy exploration Q-learning,0.008620972633361817,5,False,5,5
One-episode random-exploration Q-learning,0.7791596174240112,5,False,5,5
Fully-greedy Q-learning with convergence,0.00012559890747070311,5,False,5,5
Don't care Q-learning,0.13279976367950438,5,False,5,5
Stochastic Q-learning (converging),0.00016726493835449217,6,True,10,93
Value Iteration,0.0007867741584777832,5,False,5,5
Random Action Value Iteration,0.009273345470428468,5,False,5,5
Stochastic Value Iteration,0.00996246337890625,5,True,10,8
