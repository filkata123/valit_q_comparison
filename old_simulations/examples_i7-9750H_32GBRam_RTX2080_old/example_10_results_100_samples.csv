Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.36633944034576416,64,False,64,64
No-discounting Q-learning,0.3883278441429138,64,False,64,64
"No-discounting, no stochastic approximation Q-learning",0.3828243637084961,64,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation)",0.39180927276611327,64,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.38739593982696535,64,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.753343665599823,64,False,64,64
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.703875274658203,0,False,0,0
Fully-random exploration Q-learning,8.516360340118409,64,False,64,64
Fully-greedy exploration Q-learning,0.35240134716033933,64,False,64,64
One-episode random-exploration Q-learning,1.1354341006278992,64,False,64,64
Fully-greedy Q-learning with convergence,0.1821780562400818,64,False,64,64
Don't care Q-learning,6.168712739944458,76,True,70,92
Stochastic Q-learning (converging),0.1979909372329712,72,True,102,99
Value Iteration,0.028968279361724854,64,False,64,64
Random Action Value Iteration,0.17102938175201415,64,False,64,64
Stochastic Value Iteration,0.39598647356033323,75,True,65,90
