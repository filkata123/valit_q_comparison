Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.17266342401504517,31,False,31,31
No-discounting Q-learning,0.17641211271286011,31,False,31,31
"No-discounting, no stochastic approximation Q-learning",0.16167261123657226,31,False,31,31
"cost-based Q-learning (No discounting, no stochastic approximation)",0.16696380615234374,31,False,31,31
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.16915682077407837,31,False,31,31
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.8468673872947692,31,False,31,31
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.7068135499954225,0,False,0,0
Fully-random exploration Q-learning,4.563717658519745,31,False,31,31
Fully-greedy exploration Q-learning,0.15209428310394288,31,False,31,31
One-episode random-exploration Q-learning,1.1671815156936645,31,False,31,31
Fully-greedy Q-learning with convergence,0.054129295349121094,31,False,31,31
Don't care Q-learning,1.7032025527954102,35,True,31,39
Stochastic Q-learning (converging),0.0618139386177063,31,True,108,98
Value Iteration,0.018759973049163818,31,False,31,31
Random Action Value Iteration,0.09077387094497681,31,False,31,31
Stochastic Value Iteration,0.27312480449676513,39,True,31,42
