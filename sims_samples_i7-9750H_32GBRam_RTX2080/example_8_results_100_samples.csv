Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.14505271434783937,9.917537959652237e-05,0.009958683627695097,999.0,0.0,0.0,45166.92,179.15360000000004,13.38482723086107,2,4
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,0.13673234939575196,9.279623682109561e-05,0.009633080339179966,999.0,0.0,0.0,43145.82,91.36759999999998,9.558640070637663,2,6
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.11215248823165894,6.307795842037081e-05,0.00794216333377568,999.0,0.0,0.0,35167.4,74.92,8.655634003352962,2,8
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.025330755710601807,5.357974462066296e-06,0.0023147298896558743,999.0,0.0,0.0,7566.44,1225.4864,35.0069478818134,6,6
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.02011065721511841,2.2925977433430943e-06,0.0015141326703242005,999.0,0.0,0.0,6269.92,1387.1936000000005,37.24504799298828,6,6
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.018420438766479492,8.611916640347773e-07,0.0009280041293198955,999.0,0.0,0.0,6010.5,1664.83,40.802328364935256,6,6
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.019919688701629638,7.212422009223473e-06,0.002685595280235552,999.0,0.0,0.0,5996.88,1219.7056,34.92428381513356,6,6
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.020300097465515136,1.989632759864435e-06,0.0014105434271458766,999.0,0.0,0.0,5846.38,1198.0556,34.61293977691002,6,6
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.019149084091186524,1.7793913305922615e-06,0.0013339382784043126,999.0,0.0,0.0,5801.5,1951.55,44.17635113949544,6,6
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.01850006580352783,2.0459007618001123e-06,0.0014303498739120133,999.0,0.0,0.0,5811.66,1457.1244000000004,38.17229885663163,6,6
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.018950119018554687,3.8682138657804895e-06,0.0019667775333729255,999.0,0.0,0.0,5808.52,1584.2895999999998,39.80313555487808,6,6
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.447650842666626,0.005346524061801666,0.07311992930659647,999.0,0.0,0.0,499949.02,98417.71960000003,313.7159855665631,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.018420116901397707,3.4606606985278177e-06,0.0018602851121609875,999.0,0.0,0.0,5814.3,1789.23,42.2992907741962,6,6
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.018670103549957275,3.0216802475649733e-06,0.0017382980893865625,999.0,0.0,0.0,6054.26,1865.5724,43.192272457003234,6,6
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.018970177173614503,4.587627617746648e-06,0.002141874790398974,999.0,0.0,0.0,5812.6,1813.24,42.58215588717884,6,6
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.019600105285644532,1.215678541939269e-06,0.001102578134165225,999.0,0.0,0.0,5799.42,1733.9436000000005,41.64064840993714,6,6
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.01730048894882202,6.915153011220808e-07,0.0008315739901668888,999.0,0.0,0.0,5808.52,1525.3296000000003,39.055468247096975,6,6
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.019610116481781004,5.664914150764844e-06,0.0023801080124155804,999.0,0.0,0.0,5802.12,1812.3856,42.57212233375264,6,6
No-discounting Q-learning,True,False,0.01882965326309204,4.496332161687634e-06,0.002120455649545077,999.0,0.0,0.0,5807.94,1972.4364,44.4121199674143,6,6
"No-discounting, no stochastic approximation Q-learning",True,False,0.019360153675079345,2.032983772613761e-06,0.0014258273993067187,999.0,0.0,0.0,5809.36,1480.4704000000002,38.47688137050611,6,6
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.1434705686569214,7.471066104401417e-05,0.008643532902928882,999.0,0.0,0.0,45164.94,203.7964,14.275727652207435,2,6
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,0.13392054080963134,5.240115435460667e-05,0.007238864161911499,999.0,0.0,0.0,43144.2,81.4,9.02219485491197,2,6
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.11292453050613403,6.942768770124417e-05,0.008332327868083696,999.0,0.0,0.0,35164.56,85.04640000000003,9.22206050728361,2,8
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.02469010353088379,8.800048611874444e-06,0.002966487588356716,999.0,0.0,0.0,7563.48,1340.6896000000002,36.615428442119864,6,6
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.01948007822036743,1.1268570013442058e-06,0.0010615352096582599,999.0,0.0,0.0,6272.56,1723.8464000000001,41.51922927993727,6,6
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.019030513763427733,2.511565966324269e-06,0.0015847920893051772,999.0,0.0,0.0,6017.94,1932.2764000000004,43.95766599809413,6,6
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.01885007381439209,2.1276099230590266e-06,0.0014586328952341046,999.0,0.0,0.0,5999.86,1538.1003999999996,39.218623127284815,6,6
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.018730084896087646,2.6970325359968686e-06,0.0016422644537335844,999.0,0.0,0.0,5840.3,1332.9100000000003,36.50903997642228,6,6
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.019680075645446778,4.427359978740242e-06,0.002104129268543224,999.0,0.0,0.0,5795.84,1476.4544,38.424658749297954,6,6
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.01938966751098633,4.079257286912252e-06,0.0020197171304200626,999.0,0.0,0.0,5802.46,1804.8683999999996,42.4837427729714,6,6
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.018650193214416504,1.726249265834667e-06,0.0013138680549563062,999.0,0.0,0.0,5798.84,1580.3344000000004,39.75341997866347,6,6
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.01978952169418335,2.84349111536244e-06,0.0016862654344326814,999.0,0.0,0.0,6049.44,1672.0864000000004,40.891153077407836,6,6
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.018160381317138673,1.197490276717872e-06,0.001094298988721945,999.0,0.0,0.0,5805.46,1756.1484000000003,41.90642432849646,6,6
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.01864006519317627,2.307776099428338e-06,0.001519136629611813,999.0,0.0,0.0,5800.92,1399.2336000000003,37.40633101494986,6,6
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",True,False,0.019689695835113527,6.397923251887505e-06,0.00252941164144698,999.0,0.0,0.0,5800.22,1605.1915999999997,40.06484244321946,6,6
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",True,False,0.01962083101272583,4.053791718496314e-06,0.0020134030193918735,999.0,0.0,0.0,5830.08,1740.7936,41.72281869672757,6,6
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.2004030466079711,0.002106055366554642,0.04589177885585437,999.0,0.0,0.0,407667.85,19796154.0075,4449.286910000298,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.4666892790794372,0.00710607030598032,0.08429751067487296,999.0,0.0,0.0,499986.34,5997.3644,77.44265233061172,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.017920355796813964,3.7812044458632945e-06,0.0019445319349044629,999.0,0.0,0.0,5807.36,1548.3904,39.349592119868284,6,6
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.020119664669036866,3.567235567624039e-06,0.001888712674713663,999.0,0.0,0.0,5806.6,1783.88,42.2360035988255,6,6
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.020220134258270264,1.3893166350214872e-06,0.0011786927653216028,999.0,0.0,0.0,5806.62,1397.5756,37.38416242207387,6,6
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.017570455074310303,3.642569902865489e-07,0.0006035370662076596,999.0,0.0,0.0,5799.28,1759.1616000000001,41.942360448596595,6,6
cost-based no-discounting Q-learning,True,False,0.018340139389038085,2.185599809399719e-06,0.0014783774245434483,999.0,0.0,0.0,5808.08,2248.8735999999994,47.42229011762295,6,6
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.020225329399108885,2.1288040515855753e-06,0.0014590421692280094,999.0,0.0,0.0,5815.24,1851.1824000000001,43.02536926047236,6,6
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.01949044704437256,4.0303134028590645e-06,0.0020075640470129624,999.0,0.0,0.0,5803.24,1373.4224,37.059713976230306,6,6
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.754413480758667,0.006448137572071438,0.08030029621409523,999.0,0.0,0.0,500000.0,0.0,0.0,6,6
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.6332551264762878,0.005069139616941237,0.07119789053715873,999.0,0.0,0.0,500000.0,0.0,0.0,2,9
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.13141077518463135,0.00011587597133668623,0.010764570188200095,999.0,0.0,0.0,45720.64,1492339.0304000003,1221.6132900390369,6,6
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.16927704811096192,6.288184746154003e-05,0.007929807529917735,999.0,0.0,0.0,45509.78,1094392.4716,1046.1321482489677,6,6
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.01786009073257446,1.3382240472026296e-06,0.0011568163411720244,999.0,0.0,0.0,5094.0,0.0,0.0,6,6
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1240544247627258,0.0044694529623546435,0.06685396743914787,0.0,0.0,0.0,400000.0,0.0,0.0,6,6
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.0005900216102600098,2.6204860188840934e-07,0.0005119068293043269,7.0,0.0,0.0,134.0,0.0,0.0,6,6
Don't care Q-learning,True,False,0.2789748096466064,0.0025447369796412654,0.05044538610855571,14999.0,0.0,0.0,86865.9,214303389.74999997,14639.10481381973,6,8
Value Iteration,True,False,0.0014003205299377441,2.3971268053060156e-07,0.0004896046165331793,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.8,True,False,0.0015396666526794433,2.6879866488229715e-07,0.0005184579682889416,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.6,True,False,0.001410391330718994,2.420052884360757e-07,0.0004919403301581155,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.5,True,False,0.0015396642684936523,2.690820365160107e-07,0.0005187311794330573,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.3,True,False,0.0014882421493530274,3.037138128775041e-07,0.0005511023615241583,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.1,True,False,0.001549675464630127,2.863877997413055e-07,0.0005351521276621308,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.01,True,False,0.0015500497817993164,3.46348481093628e-07,0.0005885137900624147,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.0001,True,False,0.0015601420402526856,2.6511615976119174e-07,0.0005148943190220608,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Discounted Value Iteration - gamma = 0.00001,False,True,0.001280355453491211,2.2017257720108316e-07,0.0004692255078329429,6.0,0.0,0.0,228.0,0.0,0.0,2,2
Discounted Value Iteration - gamma = 0.000001,False,True,0.0012598991394042968,1.9253591744927693e-07,0.00043878914919272667,6.0,0.0,0.0,228.0,0.0,0.0,2,2
Stochastic Value Iteration,True,True,0.015310487747192382,4.317964075198687e-07,0.0006571121727071175,64.0,0.0,0.0,4352.0,0.0,0.0,6,10
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.013559856414794923,3.927139750567222e-06,0.0019817012263626476,54.0,0.0,0.0,3672.0,0.0,0.0,6,12
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.011170144081115723,6.177059742412895e-07,0.0007859427296192067,46.0,0.0,0.0,3128.0,0.0,0.0,6,13
Random Action Value Iteration,True,False,0.014680111408233642,3.4373329349080006e-06,0.0018540045671216671,100.0,0.0,0.0,1414.0,0.0,0.0,6,6
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.015370075702667236,1.933169972807036e-06,0.0013903848290336873,100.0,0.0,0.0,1414.0,0.0,0.0,6,6
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.015600085258483887,1.6407904064408285e-06,0.0012809334121806756,100.0,0.0,0.0,1414.0,0.0,0.0,6,6
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.014910435676574707,2.4603229286412896e-06,0.0015685416566483944,100.0,0.0,0.0,1414.0,0.0,0.0,6,6
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.014990100860595703,1.40828461203455e-06,0.0011867116802469545,100.0,0.0,0.0,1414.0,0.0,0.0,6,6
Q-factor Value Iteration,True,False,0.0011100077629089356,9.71822354301821e-08,0.00031174065411842275,7.0,0.0,0.0,266.0,0.0,0.0,6,6
Q-factor Stochastic Value Iteration,True,True,0.013920168876647949,3.3352161781067485e-06,0.0018262574238334388,64.0,0.0,0.0,4352.0,0.0,0.0,6,13
Model-free Dijkstra,True,False,9.361743927001954e-05,8.923494899590881e-08,0.0002987221936781879,11.0,0.0,0.0,28.0,0.0,0.0,6,6
