Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7668876910209657,0.013434876036361511,0.11590891267008552,999.0,0.0,0.0,459580.15,2873559.8275,1695.1577588826358,2,19
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.7251579809188842,0.0047510688172746765,0.0689279973397942,999.0,0.0,0.0,459478.1,2345873.53,1531.6244742103072,2,24
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7519589853286743,0.002477632870115008,0.049775826162053886,999.0,0.0,0.0,459354.4,2861594.1800000006,1691.6247160644114,2,17
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.1068538045883178,0.0012226621059280892,0.0349665855629069,999.0,0.0,0.0,305508.81,438708.05389999994,662.350401147308,2,16
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.165913712978363,0.005644246551442364,0.07512820077336049,999.0,0.0,0.0,343117.72,5593124.261599999,2364.978702145116,11,11
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.3075439405441284,0.00020851534954611004,0.01444006057972438,999.0,0.0,0.0,82462.27,41210.737100000006,203.00427852634044,18,18
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.288227859,6.555462358348905e-05,0.008096581,999,0,0,78510.82,42092.66760000001,205.16497654326872,18,18
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.18132274150848388,4.076601785584443e-05,0.006384827,999,0,0,49067.67,19429.2611,139.38888442053047,18,18
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.14340917587280275,4.7355573421873484e-05,0.006881539,999,0,0,38642.96,26161.0984,161.74392847955684,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",TRUE,FALSE,0.138486168,3.959733383350681e-05,0.006292641,999,0,0,37334.64,23204.6704,152.33079268486722,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",TRUE,FALSE,0.13750399112701417,2.0760009855871434e-05,0.004556315,999,0,0,37364.68,27911.4576,167.06722479289587,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",FALSE,TRUE,1.5803507804870605,0.006061097,0.07785305,999,0,0,500000,0,0,11,11
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",TRUE,FALSE,0.13084466695785524,0.000105548,0.010273641554743071,999,0,0,37407.42,25698.623600000003,160.3079024876815,18,18
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",TRUE,FALSE,0.3197795414924622,0.000646085,0.025418209,999,0,0,90904.65,36760.2475,191.72962082057117,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",TRUE,FALSE,0.12638132333755492,1.8382009756913933e-05,0.004287425,999,0,0,37353.39,26159.9179,161.7402791514841,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",TRUE,FALSE,0.1268461012840271,3.8838302748325754e-05,0.006232038,999,0,0,37389.65,35965.9475,189.6469021629407,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",TRUE,FALSE,0.1249741244316101,1.4827424822050261e-05,0.00385064,999,0,0,37379.41,34380.8019,185.4206080779588,18,18
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",TRUE,FALSE,0.1255717968940735,3.126970334084831e-05,0.005591932,999,0,0,37376.05,28882.3475,169.94807295171074,18,18
No-discounting Q-learning,TRUE,FALSE,0.12806288480758668,3.5927305929197926e-05,0.005993939,999,0,0,37414.57,22312.965099999998,149.3752492884949,18,18
"No-discounting, no stochastic approximation Q-learning",TRUE,FALSE,0.12612841606140138,3.342631996893033e-05,0.00578155,999,0,0,36862.73,36225.397099999995,190.32970629935832,18,18
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.697511661052704,0.002798420421544705,0.052900098502221195,999.0,0.0,0.0,459380.32,2546364.6776,1595.7332726994196,2,12
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6566392302513122,0.006938832945992743,0.08329965753826808,999.0,0.0,0.0,459331.08,1904070.5535999995,1379.8806301995835,2,19
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6722621774673463,0.03578641770925836,0.18917298356070394,999.0,0.0,0.0,459318.82,2199289.7276,1483.0002453135332,2,11
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.1019348883628846,0.0027412174579832197,0.0523566371913172,999.0,0.0,0.0,305494.87,574995.1131000001,758.284322071873,2,25
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.1392933201789857,0.002375503248768445,0.048739134673980875,999.0,0.0,0.0,343269.46,5618420.3484000005,2370.3207269059603,11,11
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.2909796905517578,0.0002861527874427793,0.016916051177588088,999.0,0.0,0.0,82419.3,39366.13,198.4089967718198,18,18
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.2733511018753052,0.0001842163433477481,0.013572632145156962,999.0,0.0,0.0,78540.12,39226.52559999999,198.05687465978048,18,18
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17385048389434815,0.000125181169119719,0.011188439083255492,999.0,0.0,0.0,49029.75,23554.2475,153.47393101110038,18,18
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.13749077320098876,7.322381662504539e-05,0.008557091598495683,999.0,0.0,0.0,38676.89,27653.997900000002,166.29491242969522,18,18
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1344804859161377,7.836637907180375e-05,0.008852478696489688,999.0,0.0,0.0,37381.64,30068.290400000005,173.4021061002432,18,18
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.13067076921463014,6.400732525577838e-05,0.008000457815386466,999.0,0.0,0.0,37352.33,26024.2611,161.32036790188647,18,18
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.3130813217163086,0.00031810855440098746,0.01783559795468006,999.0,0.0,0.0,90921.88,36649.2056,191.4398223985804,18,18
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.13418233633041382,5.916500593741033e-05,0.007691879220152273,999.0,0.0,0.0,37387.94,30083.2164,173.44513945337297,18,18
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.4081498026847838,0.003927540541554179,0.06267009287973155,999.0,0.0,0.0,445337.72,5185584.781599999,2277.1879109111746,11,11
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.580186860561371,0.004484249636562088,0.06696454014298976,999.0,0.0,0.0,491738.68,996600.0576,998.2985813873522,11,11
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.5992596817016602,0.0060441846743527535,0.07774435461403453,999.0,0.0,0.0,497949.44,193520.2464,439.90936157349506,11,11
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.5909459733963012,0.00419984223638587,0.06480618979994017,999.0,0.0,0.0,499081.47,80594.48909999999,283.8916855069905,11,11
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",FALSE,TRUE,1.5261319017410278,0.000819346,0.028624225,999,0,0,500000,0,0,11,11
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",TRUE,FALSE,0.12903554916381835,3.349509718859735e-05,0.005787495,999,0,0,37406.99,24847.549899999998,157.6310562674754,18,18
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",TRUE,FALSE,0.12876226425170897,2.8889137709802526e-05,0.005374862,999,0,0,37387.74,27706.772399999998,166.4535142314514,18,18
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",TRUE,FALSE,0.13006002426147462,5.813060859177313e-05,0.007624343,999,0,0,37405.81,23245.3339,152.4642053073442,18,18
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",TRUE,FALSE,0.12929075956344604,3.603364968668643e-05,0.006002803,999,0,0,37394.63,28403.853100000004,168.53442704682033,18,18
cost-based no-discounting Q-learning,TRUE,FALSE,0.13154289722442628,3.8418763661525194e-05,0.006198287,999,0,0,37339.62,24239.4156,155.69012685459538,18,18
"cost-based Q-learning (No discounting, no stochastic approximation)",TRUE,FALSE,0.1292031478881836,3.4484462617092505e-05,0.005872347,999,0,0,36861.71,38953.365900000004,197.36607079232238,18,18
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",TRUE,FALSE,0.12982667684555055,6.166153060505053e-05,0.007852486,999,0,0,36857.11,38002.15790000001,194.94142171431912,18,18
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",TRUE,FALSE,1.810106747150421,0.000646206,0.025420574,999,0,0,500000,0,0,18,18
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",FALSE,TRUE,1.7131019091606141,0.000647592,0.025447821395456014,999,0,0,500000,0,0,2,15
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",TRUE,FALSE,3.948530607223511,0.056309610001193375,0.23729646015310338,999,0,0,1303577.02,856045386.4995999,29258.253305684535,18,18
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,3.9191738033294676,0.022999408516961375,0.1516555588066635,999.0,0.0,0.0,1285957.09,924914390.9219,30412.405214351264,18,18
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",TRUE,FALSE,0.11905464172363281,0.000168659,0.012986862603227263,999,0,0,32129,0,0,18,18
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",TRUE,FALSE,1.1863316869735718,0.006075347,0.077944511,0,0,0,400000,0,0,18,18
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",TRUE,FALSE,0.060044457912445066,2.425145149720151e-05,0.004924576,112,0,0,17050,0,0,18,18
Don't care Q-learning,TRUE,FALSE,1.2781029343605042,0.022155105196739414,0.14884591091709376,14999,0,0,385613.84,1524892962,39049.877875281505,18,30
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",TRUE,TRUE,0.080342207,6.757113677795133e-05,0.008220166,101.53,36.8891,6.073639765,18425,105692.16,325.10330665805293,18,6034
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",TRUE,TRUE,0.081959555,0.000186621,0.013660928353306661,99.31,33.4539,5.7839346469336945,18243.44,109845.02639999999,331.4287651969877,18,3616
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",TRUE,TRUE,0.080850561,0.000105579,0.010275143,99.16,35.27440000000001,5.939225538738196,18248.2,93020.22000000002,304.9921638337615,18,11105
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",TRUE,TRUE,0.40079148769378664,0.001682972,0.041024042,740.21,101.54589999999999,10.076998561079582,87550.81,149634.3939,386.82605121682275,18,799
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",TRUE,TRUE,0.12593619108200074,0.000266885,0.016336614314347825,218.52,75.4296,8.685021588919627,27905.74,130603.3924,361.3909135548374,18,651
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",TRUE,TRUE,0.088621488,1.9372282148901834e-05,0.004401395,170,77.14,8.782938005018593,21391.46,99934.86839999998,316.12476714107675,18,1474
Value Iteration,TRUE,FALSE,0.024290454387664796,1.9999119499942706e-05,0.004472038,28,0,0,42560,0,0,18,18
Discounted Value Iteration - gamma = 0.8,TRUE,FALSE,0.028552913665771486,3.2111136105186235e-05,0.005666669,28,0,0,42560,0,0,18,18
Discounted Value Iteration - gamma = 0.6,TRUE,FALSE,0.028177754878997804,2.949044956978355e-05,0.005430511,28,0,0,42560,0,0,18,18
Discounted Value Iteration - gamma = 0.5,TRUE,FALSE,0.027371912002563475,1.4520475972722126e-05,0.003810574,28,0,0,42560,0,0,18,18
Discounted Value Iteration - gamma = 0.3,True,False,0.0288808274269104,1.1927653047848709e-06,0.0010921379513526993,28.0,0.0,0.0,42560.0,0.0,0.0,18,18
Discounted Value Iteration - gamma = 0.1,True,False,0.02734320640563965,5.287828373730008e-06,0.00229952785887234,25.0,0.0,0.0,38000.0,0.0,0.0,18,18
Discounted Value Iteration - gamma = 0.01,False,True,0.013665215969085693,5.400474026663459e-06,0.002323891999784727,13.0,0.0,0.0,19760.0,0.0,0.0,11,11
Discounted Value Iteration - gamma = 0.0001,False,True,0.007797117233276367,5.122566909449233e-07,0.0007157210985746636,8.0,0.0,0.0,12160.0,0.0,0.0,11,11
Discounted Value Iteration - gamma = 0.00001,False,True,0.0059282279014587404,4.195891214010316e-07,0.0006477569925527872,6.0,0.0,0.0,9120.0,0.0,0.0,11,11
Discounted Value Iteration - gamma = 0.000001,False,True,0.005856549739837647,4.5590494025304905e-07,0.0006752073313087241,6.0,0.0,0.0,9120.0,0.0,0.0,11,11
Stochastic Value Iteration,TRUE,TRUE,0.3806141996383667,0.000112671,0.010614679510975226,97,0,0,419816,0,0,18,30
Discounted Stochastic Value Iteration - gamma = 0.8,TRUE,TRUE,0.33511526107788087,0.00105074,0.032415117,83,0,0,359224,0,0,18,37
Discounted Stochastic Value Iteration - gamma = 0.6,TRUE,TRUE,0.29169180393218996,0.000995297,0.031548323,70,0,0,302960,0,0,18,29
Random Action Value Iteration,TRUE,FALSE,0.11322603464126586,0.000337394,0.018368288495588428,149,199,14.106735979665885,60000,31840000,5642.694391866354,18,18
Random Action Discounted Value Iteration - gamma = 0.8,TRUE,FALSE,0.12250683069229126,9.838455994769788e-05,0.009918899,152,96,9.797958971132712,61200,15360000,3919.183588453085,18,18
Random Action Discounted Value Iteration - gamma = 0.6,TRUE,FALSE,0.1332096290588379,0.000286676,0.016931513696976902,150.5,24.75,4.974937186,60600,3960000,1989.974874,18,18
Random Action Discounted Value Iteration - gamma = 0.5,TRUE,FALSE,0.12614277362823487,0.000129867,0.011395939258086947,150.5,24.75,4.974937186,60600,3960000,1989.974874,18,18
Random Action Discounted Value Iteration - gamma = 0.5,TRUE,FALSE,0.12781668901443483,0.000296902,0.017230847438527503,151,49,7,60800,7840000,2800,18,18
Q-factor Value Iteration,TRUE,FALSE,0.036789281368255614,4.6923915026872006e-05,0.006850103,28,0,0,42560,0,0,18,18
Q-factor Stochastic Value Iteration,TRUE,TRUE,0.4432192635536194,0.001598951,0.03998688,97,0,0,419816,0,0,18,27
Model-free Dijkstra,True,False,0.003639078140258789,7.678081021822434e-07,0.0008762465989561633,279.0,0.0,0.0,1074.0,0.0,0.0,18,18
