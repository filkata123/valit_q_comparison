Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6190599918365478,0.0027948922110726925,0.052866740121485574,999.0,0.0,0.0,477611.63,1735698.1531000002,1317.458975869837,2,20
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.645623276233673,0.0054999868769349345,0.07416189639521722,999.0,0.0,0.0,477832.14,1433718.7004000002,1197.3799315171439,2,14
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6498163771629333,0.005603287671600873,0.07485511119222837,999.0,0.0,0.0,477379.44,1592186.8464,1261.8188643383012,2,19
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.3480121088027954,0.002341034273598757,0.04838423579637026,999.0,0.0,0.0,409460.61,2581717.0579,1606.7722482978102,9,9
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.451967442035675,0.010704272036610262,0.10346145193554102,999.0,0.0,0.0,453644.27,1582835.9570999998,1258.108086413882,9,9
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.3270905566215515,0.0006315713958632102,0.02513108425562276,999.0,0.0,0.0,87024.97,51740.8691,227.466193312325,22,22
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.28102234125137326,0.00028543303646831077,0.01689476358130858,999.0,0.0,0.0,83038.6,46469.2,215.56715890877254,22,22
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.18377843379974365,0.0001437319808028178,0.011988827332263061,999.0,0.0,0.0,53048.24,17179.8424,131.07189782710861,22,22
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1463949775695801,0.00010037736294398202,0.01001885038035712,999.0,0.0,0.0,42683.04,22290.058399999998,149.2985545810809,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1443863320350647,9.937460833845649e-05,0.009968681374106432,999.0,0.0,0.0,41182.84,28281.5344,168.17114615771638,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.13868094444274903,7.462815734006653e-05,0.008638759016205194,999.0,0.0,0.0,41057.33,25317.241100000003,159.1139249091669,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.4924349236488341,0.0031891590920245733,0.056472640207666694,999.0,0.0,0.0,500000.0,0.0,0.0,9,9
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.13691211462020875,5.1602927227867225e-05,0.007183517747445692,999.0,0.0,0.0,41146.65,20577.4675,143.44848378424916,22,22
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.32373259544372557,0.0003595085480500074,0.01896071064200937,999.0,0.0,0.0,95826.75,34960.0275,186.97600781918518,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.14102185487747193,0.0002831729923291903,0.016827744719040347,999.0,0.0,0.0,41187.7,34754.87,186.42658072281432,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14341042757034303,0.00012031940982780612,0.010969020458901795,999.0,0.0,0.0,41253.8,29688.360000000008,172.30310502135475,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14111124038696288,8.91068141053438e-05,0.009439640570770891,999.0,0.0,0.0,41144.53,27593.98910000001,166.114385590171,22,22
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.13583203315734863,3.6429762424995716e-05,0.006035707284568704,999.0,0.0,0.0,41104.99,20288.289900000003,142.43696816486934,22,22
No-discounting Q-learning,True,False,0.14061426639556884,7.426473310258641e-05,0.008617698828723734,999.0,0.0,0.0,41125.33,31936.861100000002,178.7088724714025,22,22
"No-discounting, no stochastic approximation Q-learning",True,False,0.13640209913253784,5.9235479424904715e-05,0.007696458888664625,999.0,0.0,0.0,40276.28,56838.48159999999,238.40822469034075,22,22
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6132395792007446,0.0011556739470854381,0.03399520476604661,999.0,0.0,0.0,477694.76,1869582.0224,1367.326596830472,2,17
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6039740872383117,0.001399785774765695,0.0374137110531112,999.0,0.0,0.0,477534.62,1731176.7155999998,1315.7418879096308,2,18
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.598976538181305,0.0024408381640383765,0.04940483947993735,999.0,0.0,0.0,477403.85,1745248.0475,1321.0783653894268,2,30
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.3876081776618958,0.0018662685081920413,0.043200329954666335,999.0,0.0,0.0,409711.72,3669179.1416,1915.5101517872465,9,9
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.474983539581299,0.0029651913629726557,0.05445357070911563,999.0,0.0,0.0,453450.19,1458140.3339000002,1207.534816847945,9,9
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.305186505317688,0.00024230835222167576,0.015566256846836228,999.0,0.0,0.0,87035.66,56800.1244,238.32776674151924,22,22
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.293576717376709,0.0002405560954593966,0.015509870904020982,999.0,0.0,0.0,83082.3,39102.83,197.74435516595665,22,22
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1876942205429077,0.00010489494881060184,0.010241823510029932,999.0,0.0,0.0,53040.43,12359.985099999998,111.17546986633336,22,22
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.15122369050979614,9.11944033324005e-05,0.00954957608129285,999.0,0.0,0.0,42668.66,19654.724399999996,140.19530805273047,22,22
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14880305528640747,9.623046013231828e-05,0.009809712540758688,999.0,0.0,0.0,41192.95,22133.9475,148.77482145847125,22,22
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.1453334665298462,9.277944583870976e-05,0.009632208772587405,999.0,0.0,0.0,41074.02,30900.9796,175.7867446652335,22,22
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.3403783416748047,0.0004617151959327203,0.021487559096666154,999.0,0.0,0.0,95816.41,33537.901900000004,183.13356300798608,22,22
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1478641390800476,0.0001125761886139287,0.010610192675626995,999.0,0.0,0.0,41186.26,29846.7924,172.76224240267317,22,22
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5217836594581604,0.00446030026954565,0.06678547948128882,999.0,0.0,0.0,478370.15,1763380.5274999996,1327.9233891682154,9,9
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5558005595207214,0.0022374150255933447,0.047301321605144865,999.0,0.0,0.0,496190.32,457012.37760000007,676.0269059734236,9,9
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.5745367431640624,0.002550592065807814,0.05050338667661619,999.0,0.0,0.0,499068.64,63332.9304,251.66034729372842,9,9
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.579037744998932,0.003600302579817543,0.060002521445498794,999.0,0.0,0.0,499672.6,38267.520000000004,195.62085778362186,9,9
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5277347874641418,0.00471167335306024,0.06864162988347698,999.0,0.0,0.0,500000.0,0.0,0.0,9,9
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14315166234970092,9.419545853358498e-05,0.009705434484534167,999.0,0.0,0.0,41145.85,36089.3875,189.97207031561243,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14004866123199464,6.964271297104007e-05,0.008345220965980473,999.0,0.0,0.0,41231.75,28336.7075,168.33510477615772,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.14083088874816896,7.785278527003357e-05,0.008823422537203666,999.0,0.0,0.0,41163.12,28056.165600000004,167.49974805951203,22,22
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.14201083421707153,7.385916507120669e-05,0.008594135504587224,999.0,0.0,0.0,41147.02,29472.959600000006,171.67690467852688,22,22
cost-based no-discounting Q-learning,True,False,0.14580153226852416,9.92119651350265e-05,0.009960520324512495,999.0,0.0,0.0,41130.35,39364.747500000005,198.4055127762331,22,22
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.13753257274627687,5.4189557120230346e-05,0.007361355657773258,999.0,0.0,0.0,40236.12,63691.84559999999,252.3724343108811,22,22
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.13757076978683472,4.930127894414796e-05,0.007021486946804641,999.0,0.0,0.0,40301.55,60406.4675,245.77727213882085,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.831209623813629,0.004816379884951374,0.06940014326319056,999.0,0.0,0.0,500000.0,0.0,0.0,22,22
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7546630263328553,0.005940115988627639,0.07707214794351873,999.0,0.0,0.0,500000.0,0.0,0.0,2,13
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.732934558391571,0.040999833568494494,0.20248415633943928,999.0,0.0,0.0,1607381.17,834134375.2011,28881.38457901733,22,22
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.841073732376099,0.03503152392218303,0.18716710160223948,999.0,0.0,0.0,1578437.96,886680978.2184,29777.18889046446,22,22
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.12776000738143922,7.88257361440344e-05,0.008878385897449739,999.0,0.0,0.0,35473.0,0.0,0.0,22,22
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1857778573036193,0.004083850227218245,0.06390500940629182,0.0,0.0,0.0,400000.0,0.0,0.0,22,22
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.06134026527404785,1.8967126207007823e-05,0.004355126428360929,119.0,0.0,0.0,16993.0,0.0,0.0,22,22
Don't care Q-learning,True,False,1.607185471057892,0.07346904000035281,0.27105172938085603,14999.0,0.0,0.0,502145.75,6227448766.6275,78914.1860924099,22,46
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.3072319269180298,0.0003608618365655047,0.018996363772193475,999.0,0.0,0.0,71356.24,2691588.2423999994,1640.60605947924,25,1305
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.2874636960029602,0.0003632990322246939,0.019060404828457708,999.0,0.0,0.0,68277.28,1684987.1416,1298.0705456946475,26,2014
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.29559132575988767,0.0003511761408370602,0.018739694256765776,999.0,0.0,0.0,69111.77,1896027.1371000002,1376.963012248332,26,477
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.46390413522720336,0.0008739382834010656,0.029562447182211855,999.0,0.0,0.0,111952.22,364982.9515999999,604.1381891587387,22,665
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.2574918818473816,0.00026271924123518034,0.016208616265282496,999.0,0.0,0.0,60720.57,974421.7050999999,987.128008466987,22,386
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.25701154470443727,0.00021646022353635885,0.014712587248215687,999.0,0.0,0.0,62273.77,979413.3171000001,989.6531296873668,25,275
Value Iteration,True,False,0.024329419136047362,4.903004274842715e-06,0.0022142728546506447,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.8,True,False,0.02817018985748291,5.661663675732599e-06,0.002379425072519116,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.6,True,False,0.029080119132995606,4.3516371966006775e-06,0.002086057812382168,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.5,True,False,0.029670112133026123,5.662264159644792e-06,0.0023795512517373507,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.3,True,False,0.03038243532180786,1.7732585802212954e-05,0.004211007694390139,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Discounted Value Iteration - gamma = 0.1,False,True,0.026438326835632325,1.1889632146198891e-05,0.0034481345893394143,25.0,0.0,0.0,31700.0,0.0,0.0,9,9
Discounted Value Iteration - gamma = 0.01,False,True,0.012595999240875243,2.4678009269507586e-06,0.0015709235904240405,13.0,0.0,0.0,16484.0,0.0,0.0,9,9
Discounted Value Iteration - gamma = 0.0001,False,True,0.007438781261444091,3.4705686590484683e-06,0.0018629462308527502,8.0,0.0,0.0,10144.0,0.0,0.0,9,9
Discounted Value Iteration - gamma = 0.00001,False,True,0.005756776332855224,1.5252884293829536e-06,0.0012350256796451456,6.0,0.0,0.0,7608.0,0.0,0.0,9,9
Discounted Value Iteration - gamma = 0.000001,False,True,0.005582916736602783,8.89281849794088e-07,0.0009430174175454492,6.0,0.0,0.0,7608.0,0.0,0.0,9,9
Stochastic Value Iteration,True,True,0.32885746717453,0.0006277112860274144,0.025054167039185605,102.0,0.0,0.0,349248.0,0.0,0.0,22,32
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.28132181882858276,0.0005795834302760511,0.02407453904597243,87.0,0.0,0.0,297888.0,0.0,0.0,22,32
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.23019182443618774,0.00028140416178359256,0.016775105417957663,73.0,0.0,0.0,249952.0,0.0,0.0,22,32
Random Action Value Iteration,True,False,0.11546579122543335,0.0003358232014950942,0.018325479570671384,172.0,616.0,24.819347291981714,60896.0,76324864.0,8736.410246777563,22,22
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.117360520362854,0.00020765249697944914,0.014410152566140621,157.5,318.75,17.853571071357123,55792.0,39494400.0,6284.457017117708,22,22
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.12053066253662109,0.00019799098714784125,0.014070927018069607,157.0,301.0,17.349351572897472,55616.0,37295104.0,6106.97175365991,22,22
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1181006383895874,0.0002702736515095467,0.016440001566591977,157.5,318.75,17.853571071357123,55792.0,39494400.0,6284.457017117708,22,22
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.11264074563980103,0.00016471483197330483,0.012834127628058902,160.5,414.75,20.3654118544163,56848.0,51389184.0,7168.624972754538,22,22
Q-factor Value Iteration,True,False,0.03351017713546753,9.048561003800157e-06,0.0030080826125291433,30.0,0.0,0.0,38040.0,0.0,0.0,22,22
Q-factor Stochastic Value Iteration,True,True,0.3547135853767395,0.0008747035118266183,0.029575386926067735,102.0,0.0,0.0,349248.0,0.0,0.0,22,32
Model-free Dijkstra,True,False,0.003056361675262451,5.491710540979966e-07,0.0007410607627570067,273.0,0.0,0.0,981.0,0.0,0.0,22,22
