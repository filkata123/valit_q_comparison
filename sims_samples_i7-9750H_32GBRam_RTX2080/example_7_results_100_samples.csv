Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6408392071723938,0.003640980571552035,0.060340538376385366,999.0,0.0,0.0,490608.58,717766.5835999999,847.2110620146552,2,15
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.636898455619812,0.0024241835842288085,0.049235998864944425,999.0,0.0,0.0,490471.28,733511.8016000001,856.4530352564583,2,17
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6470332193374633,0.002946615405561465,0.05428273579658145,999.0,0.0,0.0,489841.72,789358.1616,888.458306056058,2,12
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5041609501838684,0.0035029776444472356,0.05918595816954589,999.0,0.0,0.0,477871.82,807963.6076000001,898.8679589350152,8,8
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5445976209640504,0.0035714552689705436,0.05976165383396399,999.0,0.0,0.0,487173.94,510134.5964,714.2370729666726,8,8
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.24203455686569214,0.00020439992774014967,0.014296850273404616,999.0,0.0,0.0,70604.76,58727.1024,242.33675412532867,29,29
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.24261115074157716,0.00023927793249945347,0.015468611201379828,999.0,0.0,0.0,68063.66,52632.5644,229.41788160472584,29,29
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17434100866317748,0.0001191426976091577,0.010915250689249317,999.0,0.0,0.0,49287.78,42095.75159999999,205.17249230830137,29,29
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14824076890945434,0.00011796286011460212,0.0108610708548744,999.0,0.0,0.0,42929.26,50745.572400000005,225.26777932052335,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1470111584663391,8.170881353918843e-05,0.009039292756581592,999.0,0.0,0.0,42128.14,43152.98039999999,207.73295453538418,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.13335687637329102,2.221087426969462e-05,0.00471284142208229,999.0,0.0,0.0,42123.76,50156.50239999999,223.9564743426722,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5614120435714722,0.007938584849318408,0.08909873651920328,999.0,0.0,0.0,500000.0,0.0,0.0,8,8
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.14478081464767456,0.00010848368130311314,0.010415549976026861,999.0,0.0,0.0,42192.24,34631.94239999999,186.09659427297424,29,29
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.2641021680831909,0.00028566671556584417,0.016901677892027293,999.0,0.0,0.0,76182.4,51698.8,227.3737012057463,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.14759076595306397,7.528792367841109e-05,0.008676861395597552,999.0,0.0,0.0,42163.62,52105.57559999999,228.2664574570692,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14089109659194946,0.00010377659507764179,0.010187079811096102,999.0,0.0,0.0,42126.62,37638.05560000001,194.00529786580574,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14227261066436767,9.704439939603162e-05,0.009851111581747089,999.0,0.0,0.0,42195.38,40914.615600000005,202.27361567935648,29,29
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.1442832088470459,0.00014646030844178313,0.012102078682680225,999.0,0.0,0.0,42209.14,48434.4204,220.07821427846966,29,29
No-discounting Q-learning,True,False,0.14849129676818848,9.520977809247598e-05,0.009757549799641095,999.0,0.0,0.0,42150.94,32783.63639999999,181.06252069381998,29,29
"No-discounting, no stochastic approximation Q-learning",True,False,0.14271109104156493,0.00010010849727621009,0.010005423393150841,999.0,0.0,0.0,41381.54,49613.66840000001,222.74125886328292,29,29
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6948986124992371,0.004139733188849795,0.06434075837950462,999.0,0.0,0.0,490473.38,727608.5356000001,852.9997277842473,2,13
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.7004227256774902,0.004250120217835411,0.06519294607421428,999.0,0.0,0.0,490541.28,583804.9216000001,764.0712804444361,2,13
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6841877889633179,0.0033283963878491026,0.057692255874156134,999.0,0.0,0.0,489906.86,739837.8603999999,860.1382798131937,2,13
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5715507578849792,0.004097654906093629,0.06401292764819953,999.0,0.0,0.0,477873.0,793071.24,890.5454732915102,8,8
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5750447177886964,0.003911491057248804,0.0625419144034527,999.0,0.0,0.0,487087.38,502364.2156000001,708.7765625357544,8,8
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.2424313807487488,0.0001947302251165809,0.013954577210241122,999.0,0.0,0.0,70590.16,66833.57440000001,258.5219031339511,29,29
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.23774091005325318,0.0002302076348114213,0.015172594860847675,999.0,0.0,0.0,68061.48,52286.76959999998,228.66300444103322,29,29
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17240116834640504,0.00012732952422663286,0.011284038471515101,999.0,0.0,0.0,49259.54,50198.7884,224.05086119004318,29,29
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.1506206226348877,0.00010021537339171117,0.010010762877608837,999.0,0.0,0.0,42971.92,52316.7136,228.72847133664843,29,29
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.14809051513671875,9.288941161062214e-05,0.009637915314559584,999.0,0.0,0.0,42137.44,63531.44640000001,252.05445126004025,29,29
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.15002596855163575,0.000497975055448569,0.02231535470138373,999.0,0.0,0.0,42183.9,52368.51,228.8416701564643,29,29
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.26509100914001466,0.00025711356644487755,0.01603476119076544,999.0,0.0,0.0,76229.1,63368.19,251.73039149057868,29,29
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.14760051012039185,9.488181715493625e-05,0.009740729806073888,999.0,0.0,0.0,42144.5,39351.95,198.37325928662864,29,29
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.571777219772339,0.003082039999752032,0.05551612378176301,999.0,0.0,0.0,493455.0,784369.64,885.646453162886,8,8
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7562481689453124,0.17147322934871803,0.41409326165577487,999.0,0.0,0.0,499390.84,62560.494399999996,250.12095953758052,8,8
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.5908343696594238,0.0035473744774195885,0.0595598394677117,999.0,0.0,0.0,499934.18,11360.487599999995,106.58558814398874,8,8
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.6027524852752686,0.005793179718528702,0.07611294054580142,999.0,0.0,0.0,499982.6,3703.8000000000006,60.85885309468131,8,8
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5801682782173156,0.0067616070699701315,0.08222899166334299,999.0,0.0,0.0,500000.0,0.0,0.0,8,8
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.14180432319641112,9.288761187158342e-05,0.009637821946455715,999.0,0.0,0.0,42177.64,32776.8304,181.04372510529052,29,29
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.14449085235595704,0.00011192867808781558,0.010579635064018777,999.0,0.0,0.0,42153.54,49895.1084,223.3721298640455,29,29
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.14320056438446044,0.00011541035458151327,0.010742921138196691,999.0,0.0,0.0,42179.8,51235.79999999999,226.35326372729858,29,29
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.15017156362533568,7.628483599398236e-05,0.008734119073723599,999.0,0.0,0.0,42181.92,44645.51360000002,211.2948499135746,29,29
cost-based no-discounting Q-learning,True,False,0.13916122674942016,6.189118605504405e-05,0.007867095147196584,999.0,0.0,0.0,42151.92,39762.23359999999,199.40469803893788,29,29
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.1491109037399292,7.781859450735736e-05,0.008821484824413481,999.0,0.0,0.0,41332.7,45760.27,213.91650240222233,29,29
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1393012547492981,7.270190310770772e-05,0.008526541098693404,999.0,0.0,0.0,41389.74,51087.0124,226.02436240370196,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.8721365571022033,0.008305616200987487,0.09113515348638794,999.0,0.0,0.0,500000.0,0.0,0.0,29,29
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7869857501983644,0.008995060786391468,0.09484229429105703,999.0,0.0,0.0,500000.0,0.0,0.0,2,10
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,5.903315944671631,0.0634862251696687,0.25196473001130276,999.0,0.0,0.0,1977511.38,876421218.9355999,29604.412153184192,29,29
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,7.097044382095337,0.03647339206112238,0.19098008289118104,999.0,0.0,0.0,1968821.22,1426944826.6315997,37774.92325116756,29,29
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.12442062139511108,3.9614519796595005e-05,0.006294006656859762,999.0,0.0,0.0,37740.0,0.0,0.0,29,29
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.151260187625885,0.005141323129488791,0.07170302036517563,0.0,0.0,0.0,400000.0,0.0,0.0,29,29
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.049400293827056886,1.6545234322364875e-05,0.004067583351618609,138.0,0.0,0.0,13632.0,0.0,0.0,29,29
Don't care Q-learning,True,False,1.923945825099945,0.044994966425405264,0.21212016977507175,14999.0,0.0,0.0,578376.96,3422807594.5983996,58504.76557168997,29,41
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.30070133686065675,0.0003373702757578258,0.018367642084868317,999.0,0.0,0.0,69187.38,2899152.7355999993,1702.6898530266747,31,31076
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.2845916676521301,0.001470297184411555,0.038344454415359136,965.39,27340.817900000005,165.35059086680036,66825.07,87888939.10510002,9374.910085174151,29,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.28779146671295164,0.0003713644983969288,0.019270819868312007,999.0,0.0,0.0,68898.92,2819078.3135999995,1679.0111118155232,30,13753
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.3852579092979431,0.0005426100857429163,0.023293992481816343,999.0,0.0,0.0,88527.94,294999.43639999995,543.1385057239082,29,252
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.2413717293739319,0.00022725692116420642,0.015075042990459643,999.0,0.0,0.0,56749.68,609214.5375999999,780.521964841477,29,726
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.25947419881820677,0.0002259409444480468,0.015031332091602753,999.0,0.0,0.0,60132.68,905914.0376,951.795165778856,30,558
Value Iteration,True,False,0.02103048324584961,3.3810164883561803e-06,0.0018387540586919666,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.8,True,False,0.024260129928588867,1.4726450848684184e-06,0.0012135258896572494,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.6,True,False,0.022109935283660887,6.158158854867679e-06,0.0024815637922220896,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.5,True,False,0.023560450077056885,5.649214121712022e-06,0.002376807548311815,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.3,True,False,0.021459980010986326,4.353568586884648e-06,0.0020865206893018453,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Discounted Value Iteration - gamma = 0.1,False,True,0.021829390525817872,8.364688827100509e-06,0.0028921771776812895,25.0,0.0,0.0,28950.0,0.0,0.0,8,8
Discounted Value Iteration - gamma = 0.01,False,True,0.010890350341796876,9.604656239844188e-07,0.0009800334810527744,13.0,0.0,0.0,15054.0,0.0,0.0,8,8
Discounted Value Iteration - gamma = 0.0001,False,True,0.006870369911193847,1.2513900562453271e-06,0.00111865546807108,8.0,0.0,0.0,9264.0,0.0,0.0,8,8
Discounted Value Iteration - gamma = 0.00001,False,True,0.0047986221313476565,5.075242168913974e-07,0.0007124073391616606,6.0,0.0,0.0,6948.0,0.0,0.0,8,8
Discounted Value Iteration - gamma = 0.000001,False,True,0.0049599862098693845,3.5997244219174717e-07,0.0005999770347202859,6.0,0.0,0.0,6948.0,0.0,0.0,8,8
Stochastic Value Iteration,True,True,0.34346627235412597,0.00031837586851556803,0.017843090217660392,105.0,0.0,0.0,328440.0,0.0,0.0,29,42
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.26693211317062376,0.00042312506533364165,0.020570004018804704,86.0,0.0,0.0,269008.0,0.0,0.0,29,43
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.2231016182899475,0.00024661213635965285,0.015703889211263967,71.0,0.0,0.0,222088.0,0.0,0.0,29,41
Random Action Value Iteration,True,False,0.09192061424255371,5.71700061118463e-05,0.007561084982451017,149.5,24.75,4.9749371855331,48611.5,2582142.75,1606.9047109271912,29,29
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.10446998119354248,6.274422183653315e-05,0.007921125036037062,151.0,49.0,7.0,49096.0,5112121.0,2261.0,29,29
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.1076006031036377,6.648344373561486e-05,0.008153738022257943,150.0,0.0,0.0,48773.0,0.0,0.0,29,29
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.10340054750442505,6.717276971136812e-05,0.008195899566940052,149.5,24.75,4.9749371855331,48611.5,2582142.75,1606.9047109271912,29,29
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.10041054248809815,4.137601358504526e-05,0.00643241895285477,150.0,0.0,0.0,48773.0,0.0,0.0,29,29
Q-factor Value Iteration,True,False,0.025589871406555175,4.719825494134966e-06,0.002172515936451322,26.0,0.0,0.0,30108.0,0.0,0.0,29,29
Q-factor Stochastic Value Iteration,True,True,0.36504435777664185,0.0006205357897005058,0.024910555788671313,105.0,0.0,0.0,328440.0,0.0,0.0,29,42
Model-free Dijkstra,True,False,0.002697741985321045,6.22658510991414e-07,0.0007890871377683291,165.0,0.0,0.0,579.0,0.0,0.0,29,29
