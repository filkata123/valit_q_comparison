Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.8493428134918213,0.006013577970288588,0.07754726281622445,999.0,0.0,0.0,497527.92,232236.71359999996,481.90944543555065,2,19
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.8082109117507934,0.009048212367864177,0.09512209190227146,999.0,0.0,0.0,497473.72,202532.08160000003,450.0356448104972,2,10
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.8076364088058472,0.0030855761498751006,0.05554796260777798,999.0,0.0,0.0,497273.68,203180.21759999997,450.75516369754433,2,16
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.7196408820152282,0.007931705543915831,0.0890601231972864,999.0,0.0,0.0,494497.42,478649.8635999999,691.8452598666843,5,5
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6757153367996216,0.005240945001260592,0.07239437133686977,999.0,0.0,0.0,496667.12,193729.70559999996,440.1473680484753,5,5
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.3750484108924866,0.0004444379122405451,0.021081696142401473,999.0,0.0,0.0,102583.22,24079.271600000007,155.1749709199264,33,33
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.35161877870559693,0.0005952256732625472,0.024397247247641425,999.0,0.0,0.0,98320.02,25773.7996,160.54220504278618,33,33
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.23965850353240967,0.00014059891349095323,0.01185744127082033,999.0,0.0,0.0,66064.48,26282.0896,162.11751786898293,33,33
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.20070961236953735,0.00012931104207684142,0.011371501311473407,999.0,0.0,0.0,55113.62,23705.415600000004,153.96563122983,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.185227689743042,0.00014437744404644946,0.012015716543196641,999.0,0.0,0.0,53440.52,21967.889600000006,148.21568607944303,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.19202937602996825,0.0001174363619033784,0.01083680589026944,999.0,0.0,0.0,53565.26,27426.692399999996,165.61006128855817,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.6133163905143737,0.005559705887941271,0.07456343532819065,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.1933987021446228,0.00013960422169972733,0.011815423043620881,999.0,0.0,0.0,53479.42,27289.303600000007,165.19474446846064,33,33
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.38890852928161623,0.0006029347588116706,0.024554729866395814,999.0,0.0,0.0,112260.72,19921.481599999996,141.1434787724888,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1874398422241211,0.0001979364305542276,0.014068988256240304,999.0,0.0,0.0,53450.76,20672.862399999995,143.78060508983816,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.19289904117584228,0.00013481058091836077,0.011610795877904355,999.0,0.0,0.0,53460.44,25127.886399999992,158.5177794444522,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.18720920085906984,0.0001431948034684183,0.01196640311323408,999.0,0.0,0.0,53453.16,24248.094399999994,155.7179963909117,33,33
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.17760944843292237,0.00010174869339605265,0.010087055734754947,999.0,0.0,0.0,53466.28,20170.6416,142.02338399010213,33,33
No-discounting Q-learning,True,False,0.18871865034103394,0.00016989904106014256,0.013034532636812974,999.0,0.0,0.0,53502.4,19744.4,140.51476790714918,33,33
"No-discounting, no stochastic approximation Q-learning",True,False,0.19520895719528197,0.00010509214072760075,0.010251445787185374,999.0,0.0,0.0,52986.62,33862.1356,184.01667207076648,33,33
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7881858038902283,0.003801191870292831,0.0616538066164031,999.0,0.0,0.0,497537.48,146436.44960000002,382.670157707653,2,14
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.7985386872291564,0.0015979490903526936,0.039974355408845474,999.0,0.0,0.0,497459.06,210691.8764,459.0118477773749,2,15
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7905773973464967,0.0030973530082699435,0.055653867864416603,999.0,0.0,0.0,497241.62,219982.37560000006,469.02278793252685,2,11
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.693036835193634,0.003439794759964405,0.05864976351158123,999.0,0.0,0.0,494561.42,304238.50360000005,551.5781935501077,5,5
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6915117120742797,0.0020709268252747048,0.045507437032585175,999.0,0.0,0.0,496619.52,163980.1696,404.9446500449166,5,5
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.3718991494178772,0.00029869974715563216,0.01728293224992889,999.0,0.0,0.0,102629.92,20982.793600000005,144.8543875759378,33,33
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.362023332118988,0.0002540079147639687,0.01593762575680483,999.0,0.0,0.0,98332.7,20849.309999999998,144.3929014875731,33,33
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.24039140701293946,0.000151280531063685,0.012299615077866664,999.0,0.0,0.0,66064.64,17688.550400000004,132.9983097636959,33,33
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.20407160520553588,0.0004163217211616654,0.020403963368955193,999.0,0.0,0.0,55147.7,25022.11,158.18378551545666,33,33
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.19922704458236695,0.0003311184316752304,0.0181966599043679,999.0,0.0,0.0,53449.26,29607.97239999999,172.0696730978472,33,33
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.19931150913238527,0.0002431385922179971,0.015592901981927454,999.0,0.0,0.0,53560.96,26046.838400000004,161.39032932614023,33,33
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.414373893737793,0.0007311390539526655,0.02703958309502322,999.0,0.0,0.0,112277.2,22929.6,151.42522907362564,33,33
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1934715223312378,0.00031528671778207804,0.017756314870548958,999.0,0.0,0.0,53426.64,21279.67040000001,145.87553050460522,33,33
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6667528057098389,0.002919438012688625,0.054031824073305396,999.0,0.0,0.0,495501.8,372925.96,610.6766411121356,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6824542856216431,0.002044336714266842,0.04521434190903194,999.0,0.0,0.0,499346.14,53494.5804,231.28895434066885,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6854403948783874,0.0029857754333225957,0.054642249526557704,999.0,0.0,0.0,499799.82,20526.567599999995,143.27095867620903,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.6776604437828064,0.001873678511470979,0.04328600826446092,999.0,0.0,0.0,499960.88,5780.985600000001,76.03279292515829,5,5
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.6054389238357545,0.00754344904424272,0.08685303128989062,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.19013904809951782,0.000173586290529164,0.013175215008840046,999.0,0.0,0.0,53462.38,22818.295599999998,151.0572593422772,33,33
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1941214394569397,0.0001363027758077976,0.01167487797828301,999.0,0.0,0.0,53468.56,24259.846400000002,155.7557267004973,33,33
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.18810933589935303,0.00015617624465064637,0.01249704943779316,999.0,0.0,0.0,53457.08,23624.513600000002,153.7026792219316,33,33
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.18914865732192992,0.00012060033939343954,0.010981818583160056,999.0,0.0,0.0,53419.34,30111.764400000004,173.52741685393696,33,33
cost-based no-discounting Q-learning,True,False,0.18940901517868042,0.00016861589693405106,0.012985218401476775,999.0,0.0,0.0,53450.46,24891.988400000002,157.77195061226823,33,33
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.19689900875091554,7.884447542721773e-05,0.008879441166380784,999.0,0.0,0.0,52986.02,32614.119599999998,180.59379723567474,33,33
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.19348918914794921,0.00011555957323421354,0.010749863870496852,999.0,0.0,0.0,53000.0,29636.08,172.1513287779098,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.894187777042389,0.004410772413845797,0.06641364629235318,999.0,0.0,0.0,500000.0,0.0,0.0,33,33
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7841018271446227,0.007738253863432481,0.08796734543813677,999.0,0.0,0.0,500000.0,0.0,0.0,2,10
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,7.075192446708679,0.08064948116932212,0.2839885229535203,999.0,0.0,0.0,2317991.82,560583724.7276,23676.649356013193,33,33
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,6.888844649791718,0.015395072848912212,0.12407688281429467,999.0,0.0,0.0,2295033.48,314497062.4496,17734.065028909758,33,33
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1753295588493347,8.541901236009722e-05,0.009242240656902266,999.0,0.0,0.0,47708.0,0.0,0.0,33,33
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1929496026039124,0.004661440560506996,0.06827474321084626,0.0,0.0,0.0,400000.0,0.0,0.0,33,33
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.07190956115722656,2.8061302805690503e-05,0.005297292025713752,130.0,0.0,0.0,19900.0,0.0,0.0,33,33
Don't care Q-learning,True,False,2.407799165248871,0.0693793531134782,0.26339960727662104,14999.0,0.0,0.0,719287.25,5212780772.2475,72199.58983434393,33,49
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",False,True,0.4037388825416565,0.0017431820673549456,0.04175143191981499,999.0,0.0,0.0,89561.9,3897199.5499999993,1974.1326069947781,38,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.4006805396080017,0.0007408027051184206,0.027217691032091988,999.0,0.0,0.0,92818.16,3408184.5543999993,1846.1269063636983,37,1924
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.39159995555877686,0.000776764053257034,0.027870487137060126,999.0,0.0,0.0,92256.24,3323580.6224,1823.0690119685542,39,33330
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.5543508291244507,0.0011472787754695448,0.03387150388556057,999.0,0.0,0.0,129751.86,375990.72039999993,613.1808219440657,33,73
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.32796918153762816,0.00040104904683786914,0.02002620899815712,999.0,0.0,0.0,75502.25,715461.1275,845.8493527218661,33,237
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.3381300282478332,0.0005151180945593466,0.022696213220697117,999.0,0.0,0.0,79994.61,1391996.3179000006,1179.8289358631616,35,4826
Value Iteration,True,False,0.021650171279907225,5.789147437553766e-06,0.002406064720150679,28.0,0.0,0.0,30184.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.8,True,False,0.16234917879104616,0.0001657502003862362,0.012874400971937925,224.0,0.0,0.0,241472.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.6,True,False,0.07115959882736206,5.541478093447836e-05,0.0074441104864502354,101.0,0.0,0.0,108878.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.5,True,False,0.05678021192550659,2.777532250826766e-05,0.005270229834482332,75.0,0.0,0.0,80850.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.3,True,False,0.0361241602897644,2.367372272180433e-05,0.00486556499512691,45.0,0.0,0.0,48510.0,0.0,0.0,33,33
Discounted Value Iteration - gamma = 0.1,False,True,0.02133636474609375,4.673149251175346e-06,0.002161746805519866,25.0,0.0,0.0,26950.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.01,False,True,0.01110264539718628,2.211345901804407e-06,0.0014870594815959471,13.0,0.0,0.0,14014.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.0001,False,True,0.006436142921447754,4.314444504643688e-07,0.0006568443121961009,8.0,0.0,0.0,8624.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.00001,False,True,0.004737796783447265,2.0593451281456514e-07,0.00045380008022758784,6.0,0.0,0.0,6468.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.000001,False,True,0.005325973033905029,1.2966888566268154e-06,0.0011387224669017536,6.0,0.0,0.0,6468.0,0.0,0.0,5,5
Stochastic Value Iteration,True,True,0.30099010467529297,0.0005432454348488136,0.023307626109254746,104.0,0.0,0.0,284336.0,0.0,0.0,33,46
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.67183025598526,0.0022384860253885732,0.047312641285269345,247.0,0.0,0.0,675298.0,0.0,0.0,33,45
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.319268479347229,0.0005838677524989407,0.02416335557200077,113.0,0.0,0.0,308942.0,0.0,0.0,33,52
Random Action Value Iteration,True,False,0.10337978839874268,0.00022258223863175317,0.014919190280700665,160.5,414.75,20.3654118544163,52003.0,43002939.0,6557.662617122049,33,33
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.12790975570678711,0.00027710223063081684,0.016646387915425283,159.5,384.75,19.615045245933032,51681.0,39892419.0,6316.044569190436,33,33
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.14793960571289064,0.000130448472609578,0.011421404143518344,199.5,24.75,4.9749371855331,64561.0,2566179.0,1601.9297737416582,33,33
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.12144954442977905,0.00025327622449787556,0.015914654394547045,160.5,414.75,20.3654118544163,52003.0,43002939.0,6557.662617122049,33,33
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.12194902181625367,0.00017898870580422107,0.013378666069688004,157.0,301.0,17.349351572897472,50876.0,31208884.0,5586.491206472986,33,33
Q-factor Value Iteration,True,False,0.028170700073242187,1.0198234680819953e-05,0.003193467501137275,28.0,0.0,0.0,30184.0,0.0,0.0,33,33
Q-factor Stochastic Value Iteration,True,True,0.3465148067474365,0.0006753951171916925,0.025988365034986187,104.0,0.0,0.0,284336.0,0.0,0.0,33,48
Model-free Dijkstra,True,False,0.002586171627044678,3.7555814594156797e-07,0.0006128279905010605,280.0,0.0,0.0,964.0,0.0,0.0,33,33
