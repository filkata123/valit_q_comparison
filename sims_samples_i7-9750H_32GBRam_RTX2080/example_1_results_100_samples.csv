Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7012005686759948,0.0038878328605373717,0.062352488807884576,999.0,0.0,0.0,471695.01,1644922.7298999995,1282.5454104631146,2,14
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6894481897354126,0.002026050251612901,0.04501166794968723,999.0,0.0,0.0,471978.03,1579496.6290999996,1256.7802628542506,2,15
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6064960956573486,0.008308662501988237,0.09115186504942308,999.0,0.0,0.0,471556.86,1534194.1004000006,1238.6258920271289,2,16
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.2191854476928712,0.0004761133490683278,0.021820021747659367,999.0,0.0,0.0,386121.32,3036108.9576,1742.4433872008583,12,12
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.339301643371582,0.0020437067341835245,0.04520737477650659,999.0,0.0,0.0,446095.05,1469403.5075,1212.1895509778988,12,12
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,0.28449579238891604,3.166064886013374e-05,0.0056267796171641325,999.0,0.0,0.0,87139.26,7423.2523999999985,86.15829849759103,6,30
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,0.2911456274986267,0.0004949819177350661,0.022248189088891396,999.0,0.0,0.0,83218.13,6480.8731000000025,80.50387009330672,5,28
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.19340113878250123,0.00021678717074912013,0.014723694195042226,999.0,0.0,0.0,55798.92,9612.413599999998,98.04291713326363,28,28
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16765250205993654,0.0003457562859004156,0.01859452300814451,999.0,0.0,0.0,46469.09,11551.021900000002,107.47568050494029,28,28
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16643403053283692,0.0004409665028659674,0.020999202434044188,999.0,0.0,0.0,45147.02,11281.6996,106.21534540733745,28,28
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.1644873332977295,0.00033855915019817074,0.018399976907544496,999.0,0.0,0.0,45129.83,14072.161100000001,118.62614003667152,28,28
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5914053058624267,0.007384820505871537,0.08593497836080216,999.0,0.0,0.0,500000.0,0.0,0.0,12,12
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.16729405164718628,0.0004013664070474704,0.020034131052967342,999.0,0.0,0.0,45176.46,11617.648399999996,107.78519564392874,28,28
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.344015805721283,0.001291987192932601,0.035944223359708316,999.0,0.0,0.0,93633.76,8604.602400000002,92.76099611366838,2,38
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.16599647998809813,0.00041137962352934216,0.02028249549560759,999.0,0.0,0.0,45123.91,10608.601900000001,102.99806745759845,28,28
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.15265556812286377,2.6446627609880126e-05,0.005142628472860948,999.0,0.0,0.0,45155.33,13508.641099999999,116.22667981147873,28,28
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.1509729242324829,1.3395685937462075e-05,0.003660011740071618,999.0,0.0,0.0,45167.09,16342.061900000004,127.83607432958821,28,28
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.15176658630371093,4.0025414527917746e-05,0.0063265641961429385,999.0,0.0,0.0,45179.15,13460.4875,116.01934106001464,28,28
No-discounting Q-learning,True,False,0.15421368598937987,1.7255689577586964e-05,0.004153996819640931,999.0,0.0,0.0,45159.59,10812.241900000005,103.98193064181875,28,28
"No-discounting, no stochastic approximation Q-learning",True,False,0.15345377206802369,2.7742966638987814e-05,0.0052671592570367395,999.0,0.0,0.0,44981.82,13811.807599999998,117.52364698221375,28,28
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6781101274490355,0.0067471711032896795,0.08214116570447293,999.0,0.0,0.0,471626.11,1578972.0779000001,1256.5715570153577,2,16
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6500186538696289,0.0042347607676444105,0.065075039513199,999.0,0.0,0.0,471738.43,1843112.4651000001,1357.6127817238612,2,13
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.643753845691681,0.003778529155965833,0.0614697417919242,999.0,0.0,0.0,471566.74,1698989.0523999995,1303.4527426799943,2,16
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.314095160961151,0.003230332989375136,0.05683601841592298,999.0,0.0,0.0,386456.94,2179556.6164,1476.3321497549257,12,12
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.426016752719879,0.004085359144226975,0.0639168142528003,999.0,0.0,0.0,446009.52,1224896.5896,1106.7504640161667,12,12
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.3077433681488037,0.0002985291453332138,0.01727799598718595,999.0,0.0,0.0,87126.71,7806.125899999998,88.35228293598303,28,28
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,0.2955187821388245,0.0007735435275627594,0.027812650495103113,999.0,0.0,0.0,83235.15,6988.287499999998,83.59597777405321,9,28
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.19579408645629884,0.0002051385501098139,0.014322658625751501,999.0,0.0,0.0,55791.33,8367.4211,91.47360876230914,28,28
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16538066625595094,0.00011192879567522027,0.010579640621269716,999.0,0.0,0.0,46457.35,8650.5475,93.00831952035259,28,28
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.15241046667098998,8.743126615406139e-05,0.00935046876654114,999.0,0.0,0.0,45148.23,13859.9571,117.72831902307958,28,28
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.1572408390045166,7.891879841427e-05,0.008883625296818298,999.0,0.0,0.0,45148.43,11530.3251,107.37935136701097,28,28
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.32101083278656006,0.00027498925387815234,0.016582799940846913,999.0,0.0,0.0,93636.57,8743.8851,93.50874344145578,2,36
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.15608111143112183,9.948001954132337e-05,0.009973967091449789,999.0,0.0,0.0,45149.82,12815.647600000002,113.20621714375939,28,28
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5011056303977965,0.003919070397093236,0.0626024791609185,999.0,0.0,0.0,476719.76,2280395.7023999994,1510.0979115275934,12,12
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5572969126701355,0.004055718112533731,0.06368452019552107,999.0,0.0,0.0,495884.37,394782.07309999986,628.3168572464054,12,12
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.5731060647964477,0.00468708705627912,0.06846230390718033,999.0,0.0,0.0,498930.46,91038.32840000001,301.72558459633484,12,12
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.566456015110016,0.006003518239323119,0.07748237373314733,999.0,0.0,0.0,499512.66,41861.6044,204.60108601862308,12,12
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5731947064399718,0.004494051017330065,0.06703768356178534,999.0,0.0,0.0,500000.0,0.0,0.0,12,12
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.15443278789520265,2.227749097621654e-05,0.004719903704125387,999.0,0.0,0.0,45179.73,12441.1171,111.53975569275737,28,28
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.15463286638259888,1.5988451911397304e-05,0.003998556228365096,999.0,0.0,0.0,45160.63,12610.233099999998,112.29529420238408,28,28
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.15468846321105956,2.3688529650280548e-05,0.004867086361498072,999.0,0.0,0.0,45170.72,13849.041599999999,117.68195103753166,28,28
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.15564717054367067,4.042535848427064e-05,0.006358093934841687,999.0,0.0,0.0,45183.07,12246.3451,110.66320571897418,28,28
cost-based no-discounting Q-learning,True,False,0.15668911457061768,2.0263899052201853e-05,0.004501544074226293,999.0,0.0,0.0,45165.15,12121.767500000002,110.09889872292094,28,28
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.15573694944381714,1.6856775948662064e-05,0.00410570042120246,999.0,0.0,0.0,44975.94,15102.656399999996,122.89286553742653,28,28
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1550646209716797,2.1427011532432514e-05,0.004628932007756489,999.0,0.0,0.0,45003.55,14309.4275,119.62201929410823,28,28
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.8963060593605041,0.014513070580476435,0.12047020619421399,999.0,0.0,0.0,500000.0,0.0,0.0,28,28
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7191406893730163,0.0007782077625882437,0.027896375438186297,999.0,0.0,0.0,500000.0,0.0,0.0,2,14
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.284620857238769,0.04113851890180058,0.2028263269445083,999.0,0.0,0.0,1439262.94,816238803.0764,28569.893298302675,28,28
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.560518338680267,0.052827144047003796,0.22984156292325328,999.0,0.0,0.0,1419306.67,509161988.1411001,22564.618058834945,28,28
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.14197906494140625,1.9548960473821353e-05,0.004421420639774206,999.0,0.0,0.0,40915.0,0.0,0.0,28,28
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1423112440109253,0.0006136656428097695,0.024772275688958605,0.0,0.0,0.0,400000.0,0.0,0.0,28,28
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.06930150747299195,8.096441964761428e-06,0.0028454247424174526,221.0,0.0,0.0,19909.0,0.0,0.0,28,28
Don't care Q-learning,True,False,1.7944808220863342,0.03404524106212545,0.18451352541785507,14999.0,0.0,0.0,531625.93,1919420028.6051002,43811.18611273952,28,40
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.0942608904838562,0.00015220623258421144,0.012337189006585392,185.03,77.2891,8.791421955520052,21008.39,111079.79789999999,333.2863602069547,28,14955
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.09200562715530396,0.00014493923334413805,0.012039071116333604,186.04,112.9984,10.630070554798777,20782.43,171945.6651,414.66331535355283,28,2868
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.09642135620117187,0.0001986874409538814,0.014095653264530928,189.49,112.36989999999997,10.600466970846142,20995.41,149093.5019,386.1262771425949,28,172
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.4480390524864197,0.0015826903923021915,0.039783041516482764,999.0,0.0,0.0,98494.15,38785.9275,196.9414316491073,28,530
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.1452652931213379,0.0002481168201552464,0.01575172435497925,366.66,107.32440000000003,10.359749031709216,33525.69,141822.1939,376.592875530061,28,261
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.12439588546752929,0.00028426733598134886,0.016860229416628614,319.94,75.9564,8.715296896836046,26863.96,97204.25839999999,311.77597469978343,28,71
Value Iteration,True,False,0.019156723022460936,2.0196028388318153e-05,0.004493999153128331,27.0,0.0,0.0,28890.0,0.0,0.0,28,28
Discounted Value Iteration - gamma = 0.8,True,False,0.02286846876144409,2.3215388422039494e-05,0.004818234990329913,27.0,0.0,0.0,28890.0,0.0,0.0,28,28
Discounted Value Iteration - gamma = 0.6,True,False,0.022871673107147217,3.086682600311974e-05,0.005555792113022206,27.0,0.0,0.0,28890.0,0.0,0.0,28,28
Discounted Value Iteration - gamma = 0.5,True,False,0.02309459924697876,3.65586581731975e-05,0.006046375622899846,27.0,0.0,0.0,28890.0,0.0,0.0,28,28
Discounted Value Iteration - gamma = 0.3,True,False,0.020048542022705076,6.946865890768094e-07,0.0008334786074500109,27.0,0.0,0.0,28890.0,0.0,0.0,28,28
Discounted Value Iteration - gamma = 0.1,False,True,0.01935126304626465,4.337860206851473e-06,0.00208275303549208,25.0,0.0,0.0,26750.0,0.0,0.0,12,12
Discounted Value Iteration - gamma = 0.01,False,True,0.00939821481704712,2.657656710368883e-07,0.0005155246560901702,13.0,0.0,0.0,13910.0,0.0,0.0,12,12
Discounted Value Iteration - gamma = 0.0001,False,True,0.0055881977081298825,3.9325070474660604e-07,0.0006270970457166945,8.0,0.0,0.0,8560.0,0.0,0.0,12,12
Discounted Value Iteration - gamma = 0.00001,False,True,0.0042682051658630375,2.40354780061125e-07,0.0004902599107219813,6.0,0.0,0.0,6420.0,0.0,0.0,12,12
Discounted Value Iteration - gamma = 0.000001,False,True,0.004569852352142334,8.867781836841004e-07,0.0009416890058209772,6.0,0.0,0.0,6420.0,0.0,0.0,12,12
Stochastic Value Iteration,True,True,0.2963150668144226,0.0012691297638269304,0.0356248475621571,98.0,0.0,0.0,280672.0,0.0,0.0,28,47
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.24974801301956176,0.0010807898558517707,0.03287536852799936,82.0,0.0,0.0,234848.0,0.0,0.0,28,39
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.20869647741317748,0.0008631824818331264,0.029379967355889394,69.0,0.0,0.0,197616.0,0.0,0.0,28,41
Random Action Value Iteration,True,False,0.09261804819107056,0.0002289589347064691,0.015131389054097747,150.0,100.0,10.0,45149.0,8940100.0,2990.0,28,28
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.09983669757843018,0.00016173702067296747,0.012717587061741212,150.0,100.0,10.0,45149.0,8940100.0,2990.0,28,28
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.1030285120010376,0.00019078237698124664,0.013812399392619901,150.5,24.75,4.9749371855331,45298.5,2212674.75,1487.5062184743967,28,28
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.0957834267616272,3.238388792362343e-05,0.005690684310662772,150.5,24.75,4.9749371855331,45298.5,2212674.75,1487.5062184743967,28,28
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.09633706331253052,2.8649646912134592e-05,0.005352536493302459,150.0,0.0,0.0,45149.0,0.0,0.0,28,28
Q-factor Value Iteration,True,False,0.023921236991882325,9.070061163151878e-06,0.003011654223703624,27.0,0.0,0.0,28890.0,0.0,0.0,28,28
Q-factor Stochastic Value Iteration,True,True,0.29520230770111083,5.028454650366711e-05,0.007091159743206122,98.0,0.0,0.0,280672.0,0.0,0.0,28,41
Model-free Dijkstra,True,False,0.0027890253067016603,8.434992196043822e-07,0.0009184221358418918,290.0,0.0,0.0,1037.0,0.0,0.0,28,28
