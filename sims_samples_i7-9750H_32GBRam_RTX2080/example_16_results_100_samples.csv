Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.07362019300460815,2.1449690845491884e-05,0.004631381094823863,999.0,0.0,0.0,23598.16,7.0943999999999985,2.6635314903338383,2,3
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,0.05318026781082153,1.6381671277139277e-05,0.004047427735876118,999.0,0.0,0.0,17398.16,13.814399999999996,3.716772793701546,2,5
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.04043020486831665,1.5388651084157348e-05,0.003922837121798119,999.0,0.0,0.0,13159.32,103.13759999999998,10.15566836796082,2,5
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.014800066947937012,2.459522296453543e-06,0.001568286420413549,999.0,0.0,0.0,4879.68,1007.6576000000001,31.743622981632075,5,5
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.014930076599121093,5.266227176525717e-06,0.002294826175666845,999.0,0.0,0.0,4575.46,1121.9884000000002,33.49609529482504,5,5
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.013440067768096925,2.323936654255476e-06,0.001524446343514745,999.0,0.0,0.0,4508.5,988.35,31.438034289694386,5,5
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.014990172386169433,8.441307338921434e-06,0.0029053928028618496,999.0,0.0,0.0,4507.0,973.48,31.200641019056004,5,5
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.014730353355407715,6.039650302432165e-06,0.0024575699994979117,999.0,0.0,0.0,4473.46,1563.6684000000002,39.543247211123166,5,5
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.014439575672149658,1.8263689883667665e-06,0.0013514321989529354,999.0,0.0,0.0,4458.8,950.7199999999997,30.833747744962817,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.013899986743927001,3.1296360644489597e-06,0.0017690777440375423,999.0,0.0,0.0,4458.26,1147.9724,33.88174139562487,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.01430037260055542,1.4087828991421248e-06,0.0011869216061484956,999.0,0.0,0.0,4459.02,1001.1596000000001,31.641106175353606,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.4236915850639342,0.0033629420222106255,0.057990878784603926,999.0,0.0,0.0,499258.82,1468869.1276,1211.969111652603,2,2
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.013369967937469482,1.8533882999861358e-06,0.0013613920449253903,999.0,0.0,0.0,4458.58,903.5436000000002,30.05900197944037,5,5
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.013950016498565674,1.347146484130235e-06,0.0011606663965714847,999.0,0.0,0.0,4514.02,847.2396000000002,29.107380507355867,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.014549615383148194,2.0481251243552376e-06,0.0014311272215827766,999.0,0.0,0.0,4461.54,957.7484,30.94751040067682,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.013377106189727784,1.925162394371682e-06,0.0013875022141862268,999.0,0.0,0.0,4461.02,1155.8796000000002,33.99822936565962,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.013099627494812012,7.298154689351577e-07,0.0008542923790688746,999.0,0.0,0.0,4456.34,782.4844000000002,27.972922621706875,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.01414034366607666,2.799896877263564e-06,0.001673289238973216,999.0,0.0,0.0,4463.44,959.6064000000001,30.977514425789558,5,5
No-discounting Q-learning,True,False,0.014920072555541992,2.4146208328147626e-06,0.001553905026961031,999.0,0.0,0.0,4457.18,915.4475999999999,30.256364619696132,5,5
"No-discounting, no stochastic approximation Q-learning",True,False,0.014449996948242188,2.848252250805671e-06,0.0016876765835922684,999.0,0.0,0.0,4459.84,996.4543999999999,31.566665962689182,5,5
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.0690804147720337,2.2770456021839885e-05,0.00477183989901588,999.0,0.0,0.0,23597.6,14.16,3.7629775444453557,2,3
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,0.0525406813621521,2.2549369651977715e-05,0.004748617656958466,999.0,0.0,0.0,17398.5,14.11,3.7563279941985894,2,5
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,0.03733023881912231,8.989691956065827e-06,0.002998281500470866,999.0,0.0,0.0,13160.48,63.04959999999999,7.940377824763756,2,5
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.01424027681350708,2.078319075025092e-06,0.0014416376365179607,999.0,0.0,0.0,4876.62,966.4555999999998,31.087869016708105,5,5
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.014469733238220215,4.748764622968338e-06,0.0021791660384120203,999.0,0.0,0.0,4576.58,914.1036,30.23414625882464,5,5
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.01420008659362793,3.1018567773116955e-06,0.0017612088965570482,999.0,0.0,0.0,4513.32,1007.6176,31.742992927573795,5,5
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.013620471954345703,3.1743057104449684e-06,0.0017816581351215975,999.0,0.0,0.0,4507.32,932.3376000000001,30.53420377216344,5,5
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.013710095882415771,3.385800411814444e-06,0.0018400544589262688,999.0,0.0,0.0,4471.62,1254.2156000000002,35.41490646606313,5,5
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.013189704418182372,1.6133905940648673e-06,0.0012701931325845167,999.0,0.0,0.0,4463.06,978.5964,31.28252547349718,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.013330080509185792,2.0626146034544485e-06,0.001436180560881691,999.0,0.0,0.0,4459.08,904.8335999999998,30.080452124261694,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.013350443840026855,2.387365788513307e-06,0.0015451102836086836,999.0,0.0,0.0,4455.32,1016.7376,31.886323086865943,5,5
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.013979752063751221,2.259728681855223e-06,0.0015032393960561382,999.0,0.0,0.0,4516.18,1004.6475999999998,31.69617642555644,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.013540074825286866,2.930910556750633e-06,0.0017119902326680001,999.0,0.0,0.0,4460.66,914.4044,30.239120357576542,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",True,False,0.01436011552810669,2.971571793403882e-06,0.0017238247571617835,999.0,0.0,0.0,4458.94,1003.0764000000001,31.67138140340582,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",True,False,0.012630422115325928,9.29226624811008e-07,0.0009639640163465688,999.0,0.0,0.0,4458.54,1214.9484000000002,34.85610993785738,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",True,False,0.013449954986572265,2.7084942939836767e-06,0.0016457503741405246,999.0,0.0,0.0,4468.8,1316.0,36.27671429443411,5,5
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,0.12319067955017089,7.525286849127041e-05,0.008674841121961279,999.0,0.0,0.0,44166.1,1770358.0299999998,1330.548018675012,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.4367002415657044,0.005167669514520702,0.07188650439770111,999.0,0.0,0.0,499264.6,1170340.2000000002,1081.8226287150774,2,2
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.01444995880126953,1.6450858657663044e-06,0.0012826090073620661,999.0,0.0,0.0,4455.4,1092.76,33.05692060673529,5,5
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.01481997013092041,1.2680470927307397e-06,0.0011260759711186185,999.0,0.0,0.0,4454.8,1034.8,32.16830738475371,5,5
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.01413999080657959,2.1814355047581555e-06,0.0014769683492743355,999.0,0.0,0.0,4455.04,1098.9183999999998,33.14993815982165,5,5
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.014190003871917725,2.173322949028034e-06,0.0014742194372033065,999.0,0.0,0.0,4454.52,801.0095999999998,28.302112995322446,5,5
cost-based no-discounting Q-learning,True,False,0.014119939804077148,1.9848029272452548e-06,0.0014088303401209297,999.0,0.0,0.0,4460.14,928.9804,30.479179778990115,5,5
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.013700134754180908,1.0092660482484916e-06,0.0010046223411055975,999.0,0.0,0.0,4457.98,752.7596000000002,27.43646478684891,5,5
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.015230011940002442,1.8358288758690828e-06,0.0013549276275392286,999.0,0.0,0.0,4459.86,953.2203999999999,30.874267602649297,5,5
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.6313419270515441,0.006220269141101035,0.07886868289188703,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.488860924243927,0.005557939368127397,0.07455158863583926,999.0,0.0,0.0,500000.0,0.0,0.0,2,5
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.06823072910308838,3.58482015396703e-05,0.0059873367651795155,999.0,0.0,0.0,23970.54,357946.06840000005,598.2859420043229,5,5
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.07672017097473144,2.8709677095639565e-05,0.0053581411978072735,999.0,0.0,0.0,23948.0,355188.24,595.9767109543794,5,5
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.013380317687988282,4.197113630971217e-06,0.002048685830226591,999.0,0.0,0.0,4020.0,0.0,0.0,5,5
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1119883370399475,0.0047211747507295235,0.06871080519634101,0.0,0.0,0.0,400000.0,0.0,0.0,5,5
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.00022000551223754882,1.7161558486691317e-07,0.00041426511422869435,5.0,0.0,0.0,44.0,0.0,0.0,5,5
Don't care Q-learning,True,False,0.18968356370925904,0.0001081338633474104,0.010398743354242877,14999.0,0.0,0.0,60069.5,1449.55,38.07295628132914,5,5
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.000350344181060791,2.480203583843377e-07,0.0004980164238098355,6.21,8.1659,2.85760389137473,68.02,603.8796000000001,24.57396182954633,5,11
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.00034967660903930666,2.2709052970526503e-07,0.00047654016588873706,6.45,10.6075,3.2569157188972513,69.57,738.5850999999998,27.176922195127243,5,71
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.0003499913215637207,2.2749273563817948e-07,0.00047696198552733685,6.5,8.19,2.8618176042508368,69.91,619.4019,24.887786161087128,5,13
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.021220316886901857,2.7110642650086444e-06,0.0016465309790613246,999.0,0.0,0.0,5307.99,2675.1299,51.72165793939711,5,9
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.013029930591583251,5.7053401015417654e-05,0.007553370175982219,609.99,127824.50990000002,357.52553740956745,3267.21,3607003.0458999993,1899.211164115249,5,11
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.0024699950218200683,3.8283384459361966e-06,0.0019566140257946114,111.86,8793.7004,93.77473220436302,648.95,273396.6075000001,522.8734144130873,5,11
Value Iteration,True,False,0.001450035572052002,3.267836005932167e-07,0.0005716498933728727,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.8,True,False,0.0011900806427001954,1.54451844673531e-07,0.00039300361916085584,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.6,True,False,0.001230001449584961,1.770914434473525e-07,0.00042082234190612137,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.5,True,False,0.0012000060081481933,1.5898300909498174e-07,0.0003987267348635927,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.3,True,False,0.0012102556228637696,1.6537078452074637e-07,0.0004066580683089251,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.1,True,False,0.001219627857208252,1.7194156895925516e-07,0.0004146583762077587,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.01,True,False,0.0016300368309020996,7.131653349631506e-07,0.0008444911692629773,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.0001,True,False,0.001230006217956543,1.7683937924175547e-07,0.0004205227452133303,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.00001,True,False,0.0011904501914978027,1.4959780957610748e-07,0.00038677876050283253,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Discounted Value Iteration - gamma = 0.000001,False,True,0.001269652843475342,1.9814199696952523e-07,0.00044513143785799403,6.0,0.0,0.0,72.0,0.0,0.0,2,2
Stochastic Value Iteration,True,True,0.01568964958190918,5.954979656871728e-06,0.002440282700195149,75.0,0.0,0.0,750.0,0.0,0.0,5,9
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.013150041103363036,3.466986513393522e-06,0.0018619845631458714,61.0,0.0,0.0,610.0,0.0,0.0,5,11
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.009799768924713135,3.4009541456612164e-07,0.0005831770010606743,51.0,0.0,0.0,510.0,0.0,0.0,5,12
Random Action Value Iteration,True,False,0.013010308742523194,3.08461490811851e-07,0.0005553930957545754,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.013159642219543457,3.928042229063067e-07,0.0006267409535895247,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.013119933605194091,2.470736211478197e-07,0.0004970650069636965,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.014699642658233642,3.571283219633869e-06,0.001889783908184708,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.013870368003845215,8.736882857874662e-07,0.0009347129429870254,100.0,0.0,0.0,707.0,0.0,0.0,5,5
Q-factor Value Iteration,True,False,0.0008899974822998047,1.1745734545911547e-07,0.00034272050633003484,6.0,0.0,0.0,72.0,0.0,0.0,5,5
Q-factor Stochastic Value Iteration,True,True,0.009989998340606689,2.4839145691544214e-07,0.0004983888611470387,75.0,0.0,0.0,750.0,0.0,0.0,5,11
Model-free Dijkstra,True,False,4.979610443115235e-05,4.711658223186531e-08,0.0002170635442257988,6.0,0.0,0.0,9.0,0.0,0.0,5,5

