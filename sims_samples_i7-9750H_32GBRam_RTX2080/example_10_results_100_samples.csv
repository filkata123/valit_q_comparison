Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7116672134399413,0.004819836725646974,0.06942504393694666,999.0,0.0,0.0,499982.39,1011.8578999999996,31.80971392515185,2,12
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6971254754066467,0.003833481052307724,0.06191511166353271,999.0,0.0,0.0,499986.15,942.9675,30.707775888201347,2,13
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7061588478088379,0.002805431046763578,0.05296631992845621,999.0,0.0,0.0,499990.65,673.1875,25.945857087404146,2,12
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5864300274848937,0.0030638944302423796,0.05535245640658037,999.0,0.0,0.0,499983.04,1135.5184000000002,33.697453909754074,4,4
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5632880640029907,0.0037273085162391677,0.0610516872513706,999.0,0.0,0.0,499990.72,573.8615999999998,23.955408575100527,4,4
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.3742388010025024,0.0023140014664869567,0.048104069126082845,999.0,0.0,0.0,432278.95,1502013.6675000002,1225.5666719930011,4,4
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,0.7297602701187134,0.0013852203478770433,0.037218548438608445,999.0,0.0,0.0,217933.5,20250.37,142.30379474912115,5,66
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.4709241724014282,0.0006483953460304065,0.025463608268083426,999.0,0.0,0.0,141213.58,18743.543599999997,136.90706190697395,64,64
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.3886441802978516,0.0005399155719211648,0.023236083403215025,999.0,0.0,0.0,115313.44,22602.486399999998,150.34123319967813,64,64
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.37565247535705565,0.0004635512550326894,0.021530240477818388,999.0,0.0,0.0,110684.12,27414.1656,165.5722368031549,64,64
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.3722822189331055,0.0004619216182481978,0.021492361858302073,999.0,0.0,0.0,110674.31,31211.033900000002,176.66644814451894,64,64
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5326056504249572,0.004072771888242521,0.06381827236961622,999.0,0.0,0.0,500000.0,0.0,0.0,4,4
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.3664849066734314,0.0004481094936442048,0.021168596874715263,999.0,0.0,0.0,110684.14,28348.5204,168.37018857268052,64,64
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.8524091506004333,0.0020179378615738357,0.04492146326171751,999.0,0.0,0.0,250052.33,32293.661100000005,179.7043713992512,2,68
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.4597429752349853,0.004653333326580195,0.0682153452426959,999.0,0.0,0.0,473724.32,4007544.5376000004,2001.8852458620102,4,4
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.3624640417098999,0.00031343450089455014,0.017704081475596246,999.0,0.0,0.0,110700.71,32215.565899999998,179.48695189344545,64,64
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.36859606266021727,0.0004027933217853615,0.020069711552121557,999.0,0.0,0.0,110671.76,32357.32240000001,179.8814120469372,64,64
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.37184293508529664,0.00044055784016074426,0.020989469744630146,999.0,0.0,0.0,110695.87,24310.233099999998,155.91739190994699,64,64
No-discounting Q-learning,True,False,0.37472420930862427,0.00046225992448100327,0.021500230800644983,999.0,0.0,0.0,110421.94,31629.3764,177.84649673243496,64,64
"No-discounting, no stochastic approximation Q-learning",True,False,0.3770233368873596,0.000442525516234008,0.021036290458015833,999.0,0.0,0.0,110272.67,46252.70110000001,215.06441151431824,64,64
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7264396095275878,0.0052643635817654745,0.07255593415955358,999.0,0.0,0.0,499981.01,1487.3299000000004,38.56591629924019,2,14
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.788007197380066,0.005981317779899904,0.07733897969264855,999.0,0.0,0.0,499983.74,967.6124000000002,31.10646878062504,2,22
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7470215153694153,0.005235813176445907,0.07235891912159763,999.0,0.0,0.0,499986.96,696.2184000000002,26.385950807200416,2,19
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.632942066192627,0.005510510785557378,0.07423281474898671,999.0,0.0,0.0,499977.96,1164.1184000000003,34.119179357071296,4,4
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6163985466957091,0.0056366087562070046,0.07507735181935365,999.0,0.0,0.0,499987.19,871.8538999999998,29.527172231691946,4,4
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.4285462379455567,0.0033728706056479047,0.05807642039285742,999.0,0.0,0.0,432335.0,1872767.18,1368.4908403054803,4,4
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,0.7680748009681702,0.0013266126700617576,0.036422694437146706,999.0,0.0,0.0,217942.77,22348.85710000001,149.4953413989881,4,66
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.49911625146865846,0.0008796360439797524,0.029658658836497518,999.0,0.0,0.0,141211.08,26396.97360000001,162.4714547235914,64,64
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.37928185939788817,0.00011104295249444932,0.010537691990870169,999.0,0.0,0.0,115307.17,23525.761100000003,153.381097596803,64,64
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.36624903917312623,0.00014580701820215722,0.012075057689392512,999.0,0.0,0.0,110680.36,31709.3504,178.0711947508636,64,64
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.37241124868392944,0.0015352938589029408,0.03918282607090689,999.0,0.0,0.0,110660.81,23466.533900000002,153.18790389583637,64,64
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.8327042531967163,0.00045043118573355516,0.021223364147409694,999.0,0.0,0.0,250041.69,24510.853900000002,156.55942609756846,2,68
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.4341949820518494,0.0007064011668346382,0.026578208495582206,999.0,0.0,0.0,473698.77,2817679.9371,1678.5946315593887,4,4
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5262948441505433,0.0035541954933378922,0.059617073840787356,999.0,0.0,0.0,499830.81,8743.273899999998,93.50547524075795,4,4
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5159844636917115,0.001403415243060522,0.03746218417365066,999.0,0.0,0.0,499999.01,34.169900000000005,5.8455025446919455,4,4
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.537869038581848,0.00346807557504078,0.05889036911958338,999.0,0.0,0.0,500000.0,0.0,0.0,4,4
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.5423503232002258,0.005074213489363791,0.07123351380750349,999.0,0.0,0.0,499999.29,49.90590000000002,7.064410803457003,4,4
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.539851474761963,0.0038923153515679587,0.06238842321751655,999.0,0.0,0.0,500000.0,0.0,0.0,4,4
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.3769460415840149,0.0004755395794567164,0.021806870005957216,999.0,0.0,0.0,110684.14,29032.040400000005,170.38791154304346,64,64
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.3722021245956421,0.00037538762592705555,0.019374922604414594,999.0,0.0,0.0,110672.78,23141.091599999996,152.12196291134293,64,64
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.3795122003555298,0.0004977974241619677,0.022311374322572953,999.0,0.0,0.0,110667.76,28022.482399999994,167.39917084621416,64,64
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.38180467367172244,0.0005258861312286797,0.022932207290810008,999.0,0.0,0.0,110684.47,34505.3291,185.75610111110754,64,64
cost-based no-discounting Q-learning,True,False,0.3888420820236206,0.0009076882809034712,0.030127865521863164,999.0,0.0,0.0,110378.39,23336.837900000006,152.7639941216516,64,64
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.37920365571975706,0.0004549482332278046,0.021329515541329218,999.0,0.0,0.0,110312.32,38233.617600000005,195.53418524646784,64,64
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.3789674353599548,0.00046290822194652604,0.02151530204172198,999.0,0.0,0.0,110253.72,30434.501600000003,174.45486980878465,64,64
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.6958976483345032,0.0043747346821866075,0.06614177713205631,999.0,0.0,0.0,500000.0,0.0,0.0,64,64
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7008979058265685,0.0045004221377006335,0.06708518567985508,999.0,0.0,0.0,500000.0,0.0,0.0,2,15
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,8.320693378448487,0.050225766140602535,0.22411105760448888,999.0,0.0,0.0,2902847.87,133321958.91309999,11546.512846444159,64,64
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,9.193963220119477,0.46943748209363234,0.6851550788643637,999.0,0.0,0.0,2912101.16,61286513.614400014,7828.570342942574,64,64
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.33962594509124755,0.0003548773377024417,0.01883818828078862,999.0,0.0,0.0,100159.0,0.0,0.0,64,64
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1089601278305055,0.0020428616086957166,0.045198026601785575,0.0,0.0,0.0,400000.0,0.0,0.0,64,64
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.17431366443634033,5.723960714760778e-05,0.007565686165022165,247.0,0.0,0.0,52783.0,0.0,0.0,64,64
Don't care Q-learning,True,False,5.879072315692902,0.2883043692051904,0.5369398189789899,14999.0,0.0,0.0,1930038.49,27845515407.9699,166869.75582162844,68,94
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",False,True,0.7441265511512757,0.002301262252696523,0.04797147332213722,999.0,0.0,0.0,183575.95,7937818.087500001,2817.413368233352,92,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.2869019675254822,0.008981725921708239,0.09477196801643532,131.0,2373.28,48.71632170022692,51477.72,170184256.16160002,13045.46879807698,438,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.7565889883041382,0.0013455387347762326,0.03668158577237675,999.0,0.0,0.0,188671.6,5474815.360000001,2339.832335873663,89,12006
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,1.1482696771621703,0.0019573829124914254,0.04424232037869878,999.0,0.0,0.0,285282.22,951810.7916000001,975.6079087420316,74,9781
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.6380303049087525,0.0009205316732400434,0.03034026488414436,999.0,0.0,0.0,158633.38,2066396.1156000001,1437.4964749869823,65,1309
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.6837566804885864,0.001661384478391915,0.04076008437665353,999.0,0.0,0.0,164760.13,3151881.7531,1775.353979661521,81,3883
Value Iteration,True,False,0.031409993171691894,1.5682175307824765e-05,0.003960072639210646,43.0,0.0,0.0,50912.0,0.0,0.0,64,64
Discounted Value Iteration - gamma = 0.8,True,False,0.03471107482910156,1.1130461213042508e-05,0.0033362345860329587,43.0,0.0,0.0,50912.0,0.0,0.0,64,64
Discounted Value Iteration - gamma = 0.6,True,False,0.034470009803771975,7.171173486426596e-06,0.0026779046821025195,43.0,0.0,0.0,50912.0,0.0,0.0,64,64
Discounted Value Iteration - gamma = 0.5,False,True,0.03433902263641357,3.2047634186938007e-06,0.0017901853028929157,43.0,0.0,0.0,50912.0,0.0,0.0,13,13
Discounted Value Iteration - gamma = 0.3,False,True,0.036269571781158444,1.2125239978666969e-05,0.003482131528053897,43.0,0.0,0.0,50912.0,0.0,0.0,11,11
Discounted Value Iteration - gamma = 0.1,False,True,0.0231595778465271,9.761823286788739e-06,0.0031243916666750886,25.0,0.0,0.0,29600.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.01,False,True,0.011300036907196045,2.256540391607587e-06,0.0015021785485113237,13.0,0.0,0.0,15392.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.0001,False,True,0.006729531288146973,1.623723169927871e-06,0.0012742539660239912,8.0,0.0,0.0,9472.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.00001,False,True,0.005439846515655517,1.3081444520310015e-06,0.0011437414270852489,6.0,0.0,0.0,7104.0,0.0,0.0,4,4
Discounted Value Iteration - gamma = 0.000001,False,True,0.004688119888305664,6.426198285680584e-07,0.0008016357206163273,6.0,0.0,0.0,7104.0,0.0,0.0,4,4
Stochastic Value Iteration,True,True,0.39399033546447754,0.0007220181903222965,0.02687039616980547,132.0,0.0,0.0,405768.0,0.0,0.0,65,87
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.3170173168182373,0.0003298752009254713,0.018162466818289615,109.0,0.0,0.0,335066.0,0.0,0.0,66,84
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.2612294363975525,0.0003929152247338322,0.019822089313032372,87.0,0.0,0.0,267438.0,0.0,0.0,65,83
Random Action Value Iteration,True,False,0.16038105249404908,0.0002979275736332682,0.01726057860076736,253.5,462.75,21.511624764298954,86275.5,53179692.75,7292.4407950973455,64,64
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.1852616000175476,0.0004189813068006232,0.020469032874091124,242.0,536.0,23.15167380558045,82377.0,61597656.0,7848.417420091773,64,64
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.16755000352859498,0.0002599069076079274,0.016121628565623495,236.0,554.0,23.53720459187964,80343.0,63666234.0,7979.112356647198,64,64
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.15299697160720827,0.00022211655514717564,0.014903575247140387,208.5,352.75,18.78163997099295,71020.5,40538382.75,6366.975950166609,2,10
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.15878061294555665,0.0004136020090649254,0.020337207504102558,212.5,518.75,22.776083947860748,72376.5,59615268.75,7721.092458324793,2,12
Q-factor Value Iteration,True,False,0.043070068359375,1.7173732115816162e-05,0.00414412018597629,43.0,0.0,0.0,50912.0,0.0,0.0,64,64
Q-factor Stochastic Value Iteration,True,True,0.4281580781936645,0.0006261901700316002,0.02502379207937119,132.0,0.0,0.0,405768.0,0.0,0.0,66,85
Model-free Dijkstra,True,False,0.0032069754600524904,1.3013664175275613e-06,0.0011407744814500196,331.0,0.0,0.0,1157.0,0.0,0.0,64,64

