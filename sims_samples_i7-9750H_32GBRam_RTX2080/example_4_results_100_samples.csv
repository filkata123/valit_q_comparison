Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6332609510421754,0.0017849194718687841,0.04224830732548683,999.0,0.0,0.0,484388.7,978611.8100000002,989.2481033593141,2,12
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6544179344177246,0.002322767867889434,0.04819510211514687,999.0,0.0,0.0,484132.96,1226209.1383999998,1107.3432793853945,2,13
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6287429165840148,0.0027840916665539395,0.05276449247888147,999.0,0.0,0.0,483546.45,1315110.4875,1146.7826679454133,2,12
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.50505850315094,0.0031405526658585363,0.05604063406010443,999.0,0.0,0.0,464502.2,1384651.3999999997,1176.7121143253348,3,3
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5123174357414246,0.0018221574040947583,0.04268673569265701,999.0,0.0,0.0,482066.89,711630.3979000002,843.5818857111622,3,3
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,0.3202969026565552,0.00026752138569029286,0.016356081000358638,999.0,0.0,0.0,92862.92,37986.6936,194.90175371196636,7,32
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,0.3099598264694214,0.0005879098568773543,0.02424685251485962,999.0,0.0,0.0,88947.55,32555.96749999999,180.43272291909798,10,32
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.21793325901031493,0.0001804042828134925,0.0134314661453429,999.0,0.0,0.0,60735.72,32402.5216,180.00700430816573,32,32
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.18162195205688478,0.00010337237219787312,0.010167220475521967,999.0,0.0,0.0,51389.43,30958.525100000003,175.95034839408532,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17127135276794433,0.00014108568403717073,0.011877949487902814,999.0,0.0,0.0,50018.72,39651.5816,199.1270488909028,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.17544145822525026,9.282474265966698e-05,0.00963455980622192,999.0,0.0,0.0,50050.37,27201.333100000003,164.92826652820918,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5970867919921874,0.005032247109956506,0.07093833314898586,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.17125126361846923,0.0001369230883438149,0.011701413946349172,999.0,0.0,0.0,50035.63,40723.9331,201.8017172870439,32,32
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.3634588623046875,0.00022984198333679158,0.015160540337890058,999.0,0.0,0.0,100294.34,29306.1444,171.19037472942222,3,36
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1758412480354309,0.00010387815831947476,0.01019206349663672,999.0,0.0,0.0,50029.7,30163.390000000003,173.6761065892485,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.17614654779434205,0.00013444787890654766,0.011595166187103472,999.0,0.0,0.0,50057.36,38403.610400000005,195.9683913288059,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.17781131982803344,0.00014973306328426475,0.012236546215508065,999.0,0.0,0.0,50058.94,32789.736399999994,181.07936492046792,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.15972371101379396,6.917967544950443e-06,0.002630202947483415,999.0,0.0,0.0,50070.98,36014.6196,189.77518172827553,32,32
No-discounting Q-learning,True,False,0.17538842916488648,0.00014632897080320504,0.012096651222681632,999.0,0.0,0.0,50029.54,32122.748399999997,179.22820202189163,32,32
"No-discounting, no stochastic approximation Q-learning",True,False,0.1823742699623108,9.996749812518715e-05,0.009998374774191411,999.0,0.0,0.0,50537.95,32979.78749999999,181.6033796491684,32,32
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6739904642105103,0.0037196999726899094,0.060989343107545516,999.0,0.0,0.0,484306.54,1438909.5884000002,1199.5455757910995,2,15
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6628812289237975,0.0028984810966322756,0.05383754356053288,999.0,0.0,0.0,484185.55,1095180.2875,1046.5086179769378,2,12
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6773573517799378,0.007805610494578305,0.08834936612437186,999.0,0.0,0.0,483605.72,1220331.2815999999,1104.6860556737374,2,12
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5356792879104615,0.004049664912752837,0.06363697755827846,999.0,0.0,0.0,464064.91,1767781.8819000004,1329.5795884037932,3,3
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.5352197885513306,0.002707730588970776,0.05203585868389966,999.0,0.0,0.0,482170.4,704858.6399999999,839.5585983122321,3,3
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,0.32453131675720215,0.00028028732286884407,0.01674178374214779,999.0,0.0,0.0,92848.27,36145.3571,190.11932332090814,5,34
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.3077836275100708,0.0003039786274649714,0.017434982863913904,999.0,0.0,0.0,88906.33,35606.7611,188.69753866969225,32,32
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.21322126626968385,0.00012230112111944323,0.011058983729052287,999.0,0.0,0.0,60737.82,36672.207599999994,191.49988929500714,32,32
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17724080085754396,0.00011194481674692726,0.010580397759391055,999.0,0.0,0.0,51377.58,25509.043599999997,159.71550832652412,32,32
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17807091951370238,0.00014206700984829582,0.011919186626959736,999.0,0.0,0.0,49981.83,40849.001099999994,202.11135816672945,32,32
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.17528037548065187,0.00011452134425596797,0.010701464584624292,999.0,0.0,0.0,50059.06,28721.276399999995,169.47352713624625,32,32
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.3517017841339111,0.00036239938052517567,0.019036790184408077,999.0,0.0,0.0,100293.9,31069.41,176.2651695599559,2,34
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1721306037902832,8.204192660136868e-05,0.00905769985158311,999.0,0.0,0.0,50017.77,31532.8971,177.57504638884373,32,32
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.568649275302887,0.00465532841073952,0.06822996710199647,999.0,0.0,0.0,490439.69,882837.9739000001,939.5945795394948,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.579914495944977,0.0030831436673664655,0.05552606295575498,999.0,0.0,0.0,498717.53,139453.72910000003,373.4350400002657,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.5776717472076416,0.0024674329390139524,0.049673261811702604,999.0,0.0,0.0,499850.82,19207.8276,138.59230714581528,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.5885175156593323,0.0024045778207115003,0.04903649478410442,999.0,0.0,0.0,499993.22,1621.3715999999997,40.26625882795668,3,3
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.6126308560371398,0.005224134693435616,0.07227817577551066,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.17573795080184937,0.00015447281400030876,0.012428709265257947,999.0,0.0,0.0,50073.36,29264.5704,171.06890541533258,32,32
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.17815104722976685,0.00013932048716398525,0.011803409980339801,999.0,0.0,0.0,50073.2,36117.08,190.0449420531891,32,32
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.177952401638031,0.00012721020889381977,0.01127875032500586,999.0,0.0,0.0,50084.01,38680.1699,196.67274823930234,32,32
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.18091120958328247,0.000107821894456373,0.010383732202651078,999.0,0.0,0.0,50082.98,29725.4596,172.41072936450328,32,32
cost-based no-discounting Q-learning,True,False,0.17444199323654175,0.00016417451558638164,0.012813060352093158,999.0,0.0,0.0,50050.1,38354.35,195.84266644426592,32,32
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.18106175184249879,0.00017019790345268001,0.013045991853925097,999.0,0.0,0.0,50589.47,35202.38910000001,187.6229972577989,32,32
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.18389471054077147,0.0001028931493145592,0.010143626043706421,999.0,0.0,0.0,50583.11,31498.2379,177.47742926918903,32,32
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.9004796528816223,0.004372333808361299,0.0661236251907085,999.0,0.0,0.0,500000.0,0.0,0.0,32,32
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7132649111747742,0.0082892156639328,0.09104512981995687,999.0,0.0,0.0,500000.0,0.0,0.0,2,13
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,5.820675036907196,0.07167840598004444,0.26772823157083087,999.0,0.0,0.0,1943243.21,846602639.0259001,29096.436878523462,32,32
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,7.017825846672058,0.0501550473187285,0.22395322573860932,999.0,0.0,0.0,1932810.03,1910788307.7491,43712.56464392246,32,32
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.15871142387390136,0.00013521229203311121,0.01162808204447798,999.0,0.0,0.0,45868.0,0.0,0.0,32,32
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1883616852760315,0.004271383638987407,0.06535582329821427,0.0,0.0,0.0,400000.0,0.0,0.0,32,32
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.07236137866973877,1.3635994253922944e-05,0.003692694714422375,222.0,0.0,0.0,21781.0,0.0,0.0,32,32
Don't care Q-learning,True,False,1.7988091397285462,0.021112374959858075,0.14530098058808164,14999.0,0.0,0.0,557091.02,985837124.8596,31398.043328519692,32,38
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.3427822494506836,0.0005930385301984643,0.024352382433726363,999.0,0.0,0.0,79179.5,3691060.75,1921.2133535867379,34,3156
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.3249918293952942,0.0017327306789052899,0.04162608171453674,952.74,26245.012400000003,162.00312466122375,77797.69,101374504.1139,10068.490657188891,33,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.3367717671394348,0.0006357053056229404,0.025213197052792423,999.0,0.0,0.0,79980.61,2968661.1179000004,1722.9803010771773,34,4394
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",False,True,0.48502799987792966,0.001250639527610565,0.03536438218901279,999.0,0.0,0.0,113688.9,257963.35,507.9009253781686,32,100002
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.25973235607147216,0.00020371993462210867,0.014273049240512998,999.0,0.0,0.0,64952.62,145842.41559999998,381.8931992062702,32,111
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.2747212600708008,0.00024851454698418817,0.015764344166002853,999.0,0.0,0.0,64493.1,738876.39,859.5791935592671,32,6115
Value Iteration,True,False,0.024008159637451173,9.067860952973207e-06,0.003011288918880619,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.8,True,False,0.02784958839416504,7.770309387660744e-06,0.002787527468503359,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.6,True,False,0.027078726291656495,1.070904954428329e-05,0.0032724684176143382,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.5,True,False,0.02601881980895996,3.309601895261949e-06,0.001819231127499183,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.3,True,False,0.028280203342437745,1.823696673893096e-05,0.004270476172387684,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.1,False,True,0.020860254764556885,3.9829265919877345e-06,0.001995727083543172,25.0,0.0,0.0,25750.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.01,False,True,0.0100093412399292,1.3810145044089952e-06,0.0011751657348684887,13.0,0.0,0.0,13390.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.0001,False,True,0.005749955177307129,3.519189724784155e-07,0.0005932275891076,8.0,0.0,0.0,8240.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.00001,False,True,0.004238858222961426,2.0781156174507485e-07,0.0004558635341251534,6.0,0.0,0.0,6180.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.000001,False,True,0.004810371398925781,1.4958795108213964e-06,0.0012230615319031976,6.0,0.0,0.0,6180.0,0.0,0.0,3,3
Stochastic Value Iteration,True,True,0.29911208629608155,0.0003401103067755911,0.01844207978443839,108.0,0.0,0.0,287496.0,0.0,0.0,32,47
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.256356987953186,0.0002484133684947892,0.01576113474641941,92.0,0.0,0.0,244904.0,0.0,0.0,32,44
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.21342153549194337,0.00031739200160927787,0.017815498915530765,77.0,0.0,0.0,204974.0,0.0,0.0,32,47
Random Action Value Iteration,True,False,0.09369072437286377,5.0193226076976325e-05,0.0070847177838624115,151.0,49.0,7.0,45448.0,4380649.0,2093.0,32,32
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.10530372858047485,8.969950761681391e-05,0.00947098239977321,150.5,24.75,4.9749371855331,45298.5,2212674.75,1487.5062184743967,32,32
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.10827148199081421,6.974459180184452e-05,0.008351322757614181,151.0,49.0,7.0,45448.0,4380649.0,2093.0,32,32
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1111406946182251,9.216507073349477e-05,0.009600264097070182,153.0,141.0,11.874342087037917,46046.0,12605541.0,3550.4282840243372,32,32
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.10620099544525147,0.00010877593728380363,0.010429570330737678,152.0,96.0,9.797958971132712,45747.0,8582496.0,2929.589732368681,32,32
Q-factor Value Iteration,True,False,0.030780220031738283,5.915440361832225e-06,0.0024321678317567284,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Q-factor Stochastic Value Iteration,True,True,0.33294817209243777,0.0007572334301055151,0.02751787473816819,108.0,0.0,0.0,287496.0,0.0,0.0,32,43
Model-free Dijkstra,True,False,0.0026067399978637693,6.117824664443105e-07,0.0007821652424164031,243.0,0.0,0.0,846.0,0.0,0.0,32,32
