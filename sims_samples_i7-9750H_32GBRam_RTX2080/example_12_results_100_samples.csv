Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7027903866767884,0.004691243310875796,0.06849265151004008,999.0,0.0,0.0,499997.54,220.86839999999995,14.8616419012167,2,14
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.7311071729660035,0.01163748378192238,0.10787716988279948,999.0,0.0,0.0,499999.44,18.246400000000005,4.271580503747998,3,3
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6822166872024535,0.0017449020434700969,0.04177202465131535,999.0,0.0,0.0,499999.32,30.657599999999988,5.536930557628476,3,3
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6460767960548401,0.001940305701155575,0.04404890124799454,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.621445472240448,0.004145518858034455,0.06438570383271783,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.6375311827659607,0.00232321094365272,0.04819969858466669,999.0,0.0,0.0,491228.88,150522.5856,387.9724031422854,3,3
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,2.1592103242874146,0.06979249212746425,0.2641826870320314,999.0,0.0,0.0,457397.3,853936.59,924.0868952647256,3,3
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",False,True,2.183585534095764,0.015676804804727933,0.1252070477438388,999.0,0.0,0.0,471612.76,1158562.2223999999,1076.365282977856,3,3
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.676718671321869,0.08099482156150892,0.2845958916806582,999.0,0.0,0.0,477421.46,927522.1484000001,963.0795130206021,3,3
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.5363591766357423,0.004461031190007043,0.0667909514081589,999.0,0.0,0.0,482691.36,1670948.2304000002,1292.651627624396,3,3
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.454867959022522,0.0008185129988527251,0.028609666178631393,999.0,0.0,0.0,135879.98,59629.7196,244.1919728410416,91,91
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.585555067062378,0.00423864620185318,0.06510488615959005,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.46089920282363894,0.000490759342211112,0.022153088773602475,999.0,0.0,0.0,136334.28,48364.64160000001,219.91962531797842,91,91
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.911330120563507,0.001747729113892836,0.04180585023525817,999.0,0.0,0.0,266116.84,71742.4144,267.84774481036794,3,95
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.6032906246185303,0.005110706964250176,0.07148920872586419,999.0,0.0,0.0,494453.7,581342.5099999999,762.4582021330742,3,3
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.45729952812194824,0.0008161755411266769,0.02856878613323774,999.0,0.0,0.0,136325.1,51160.75,226.1874222851483,91,91
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.4597229504585266,0.0008463150138217029,0.029091493839638126,999.0,0.0,0.0,136334.3,62683.86999999998,250.36746993169857,91,91
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.46212207317352294,0.0009865159789779001,0.03140885192072292,999.0,0.0,0.0,136301.62,54400.615599999976,233.2393954716912,91,91
No-discounting Q-learning,True,False,0.47359897375106813,0.00048820971299080043,0.022095468155049362,999.0,0.0,0.0,136318.62,49724.215599999996,222.98927238770926,91,91
"No-discounting, no stochastic approximation Q-learning",True,False,0.4661596393585205,0.000712540734434833,0.026693458645046974,999.0,0.0,0.0,136297.5,61598.75,248.19095471027947,91,91
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6101347494125366,0.002336271288091643,0.04833499030817781,999.0,0.0,0.0,499997.24,171.02240000000006,13.077553287981665,2,16
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6104375290870667,0.0043324040940925165,0.06582100040330986,999.0,0.0,0.0,499998.74,82.29239999999999,9.071515860097472,3,3
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.5965703439712524,0.0017453228025303585,0.041777060721529445,999.0,0.0,0.0,499999.96,0.15839999999999993,0.3979949748426479,3,3
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6363150334358216,0.007141751957377318,0.08450888685444459,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6591603255271912,0.0015861457895144843,0.03982644585591946,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.665763189792633,0.0016659138215871904,0.040815607573417186,999.0,0.0,0.0,491285.56,227139.80639999997,476.59186564606824,3,3
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.5524315333366394,0.0011233332218015392,0.033516163590147655,999.0,0.0,0.0,457519.56,1153161.6464,1073.8536429141543,3,3
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.5956629633903503,0.00243101001793022,0.04930527373344783,999.0,0.0,0.0,471700.68,761019.7775999999,872.3644752051747,3,3
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.60911363363266,0.0021857517289281246,0.0467520237949987,999.0,0.0,0.0,477687.5,1097447.55,1047.5913086695593,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",False,True,1.617464153766632,0.001361322065519806,0.036896098242494506,999.0,0.0,0.0,482581.32,1753310.0975999995,1324.126163777455,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.4754184174537659,0.00031871265566525724,0.017852525190158876,999.0,0.0,0.0,135826.34,44393.60440000001,210.6978984233113,91,91
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.9251311945915223,0.0010356521606802915,0.03218155000431601,999.0,0.0,0.0,266131.8,78829.8,280.766450987293,2,93
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.6534344887733459,0.002217217702683337,0.04708734121484602,999.0,0.0,0.0,494419.82,641682.8076000001,801.0510642899116,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6508837175369262,0.0023460202900803095,0.04843573360733075,999.0,0.0,0.0,499979.82,1406.0876,37.49783460414748,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6582692384719848,0.0017624100726760614,0.04198106802686256,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6586394286155701,0.001979994653788134,0.044497130848945014,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.6540889024734498,0.0022680721198474886,0.04762428078036968,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5975310802459717,0.0075526014291369846,0.08690570423819707,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.4669522285461426,0.0005883285016761874,0.024255483950566467,999.0,0.0,0.0,136356.52,52634.76960000001,229.42268763136747,91,91
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.4662990617752075,0.0007081162615008679,0.026610453989003417,999.0,0.0,0.0,136276.8,55288.00000000001,235.13400434645774,91,91
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.4595494747161865,0.0009095636422000552,0.030158972830652823,999.0,0.0,0.0,136270.22,69694.23160000001,263.99665073633037,91,91
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.47404908657073974,0.0004759287613478365,0.02181579155904815,999.0,0.0,0.0,136308.16,56132.294400000006,236.92254937004205,91,91
cost-based no-discounting Q-learning,True,False,0.47260102748870847,0.0007442143759849158,0.027280292813401323,999.0,0.0,0.0,136330.48,48404.409600000006,220.01002158992668,91,91
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.46137513637542726,0.0008269992785599014,0.028757595145628943,999.0,0.0,0.0,136277.68,59727.497599999995,244.3920980719303,91,91
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.4674355864524841,0.0010728853425705153,0.03275492852336142,999.0,0.0,0.0,136275.24,73084.3824,270.3412332590055,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.6697699952125549,0.0052916800128496744,0.07274393454336708,999.0,0.0,0.0,500000.0,0.0,0.0,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7008608508110046,0.005842958249511929,0.07643924547973985,999.0,0.0,0.0,500000.0,0.0,0.0,2,19
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",False,True,8.804706299304963,0.08269624853270713,0.28756955425202285,999.0,0.0,0.0,2982350.86,19731452.580399998,4442.009970767737,69,91
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,8.944918546676636,0.07818251923801145,0.2796113717966625,999.0,0.0,0.0,2984330.3,12048362.030000003,3471.075053927818,91,91
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.43189364194869995,0.000528899171313077,0.022997807967566758,999.0,0.0,0.0,124464.0,0.0,0.0,91,91
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",False,True,1.164088659286499,0.003692388058507367,0.06076502331528695,0.0,0.0,0.0,400000.0,0.0,0.0,2,91
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.19380791664123534,0.00018769925905289707,0.013700337917471127,242.0,0.0,0.0,56334.0,0.0,0.0,91,91
Don't care Q-learning,True,False,11.83496997833252,1.9895473680962703,1.4105131577182364,14999.0,0.0,0.0,3891202.48,212553046510.9296,461034.7562938499,91,105
Value Iteration,True,False,0.03523963689804077,1.530301961267355e-05,0.003911907413612131,56.0,0.0,0.0,48832.0,0.0,0.0,91,91
Discounted Value Iteration - gamma = 0.8,True,False,0.037618653774261476,1.1139028875112443e-05,0.003337518370752803,56.0,0.0,0.0,48832.0,0.0,0.0,91,91
Discounted Value Iteration - gamma = 0.6,False,True,0.037914705276489255,1.5110391163943861e-05,0.0038872086596867758,56.0,0.0,0.0,48832.0,0.0,0.0,9,9
Discounted Value Iteration - gamma = 0.5,False,True,0.039109601974487304,1.4873311948031184e-05,0.0038565933086120428,56.0,0.0,0.0,48832.0,0.0,0.0,9,9
Discounted Value Iteration - gamma = 0.3,False,True,0.035948092937469485,4.684853304132731e-06,0.0021644521949289457,45.0,0.0,0.0,39240.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.1,False,True,0.02088658332824707,2.7700254302999385e-06,0.001664339337484979,25.0,0.0,0.0,21800.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.01,False,True,0.010237019062042236,1.491901181287858e-06,0.0012214340675156634,13.0,0.0,0.0,11336.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.0001,False,True,0.006049919128417969,9.735713649661194e-07,0.0009866972002423638,8.0,0.0,0.0,6976.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.00001,False,True,0.004502091407775879,6.634330066162875e-07,0.0008145139695648489,6.0,0.0,0.0,5232.0,0.0,0.0,3,3
Discounted Value Iteration - gamma = 0.000001,False,True,0.004621565341949463,8.730507208554172e-07,0.0009343718322249538,6.0,0.0,0.0,5232.0,0.0,0.0,3,3
Stochastic Value Iteration,True,True,0.37490991830825804,0.0006848481702934978,0.026169603938414845,162.0,0.0,0.0,293220.0,0.0,0.0,92,117
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.29546907901763914,0.000538252303356353,0.023200265157026827,130.0,0.0,0.0,235300.0,0.0,0.0,95,122
Discounted Stochastic Value Iteration - gamma = 0.6,False,True,0.3313488531112671,0.0006918838237759019,0.026303684604554966,97.0,0.0,0.0,175570.0,0.0,0.0,100002,100002
Random Action Value Iteration,True,False,0.16443940877914429,0.0003584006657088765,0.01893147288799465,273.5,672.75,25.937424698685874,82350.0,60547500.0,7781.227409605762,91,91
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.20300888776779175,0.0004924271947908152,0.022190700637672874,271.5,612.75,24.753787588973125,81750.0,55147500.0,7426.136276691938,91,91
Random Action Discounted Value Iteration - gamma = 0.6,False,True,0.16443735837936402,0.00028200540041081577,0.01679301641786894,214.5,514.75,22.688102609076854,64650.0,46327500.0,6806.430782723057,2,11
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.1465492582321167,0.0003000516376255518,0.017321998661400242,191.0,369.0,19.209372712298546,57600.0,33210000.0,5762.811813689564,2,11
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.14278958320617677,0.00032803894959633907,0.018111845560194552,189.0,479.0,21.88606862823929,57000.0,43110000.0,6565.820588471786,2,13
Q-factor Value Iteration,True,False,0.04680991411209107,2.1253874110124118e-05,0.00461019241573756,56.0,0.0,0.0,48832.0,0.0,0.0,91,91
Q-factor Stochastic Value Iteration,True,True,0.42758827924728393,0.0008293677325990018,0.028798745330291767,162.0,0.0,0.0,293220.0,0.0,0.0,95,119
Model-free Dijkstra,True,False,0.0024624228477478026,4.835706538131035e-07,0.0006953924458987914,306.0,0.0,0.0,867.0,0.0,0.0,91,91
