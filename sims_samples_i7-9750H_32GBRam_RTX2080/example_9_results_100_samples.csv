Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6655035424232483,0.002175926130921715,0.04664682337439191,999.0,0.0,0.0,499991.9,560.9899999999999,23.685227463547818,2,9
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6825983214378357,0.0024665708665004954,0.04966458362354904,999.0,0.0,0.0,499978.08,2220.7936,47.125296816041384,12,17
"Normal Q-learning (reward, alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.6805917882919312,0.0036425828702931775,0.06035381404926434,999.0,0.0,0.0,499984.42,1305.3036,36.12898559328784,10,19
"Normal Q-learning (reward, alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.653099594116211,0.0028830584855690177,0.05369411965540564,999.0,0.0,0.0,499980.88,1218.5056000000002,34.90709956441526,19,19
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.6364256429672241,0.0026077765139266144,0.05106639319480684,999.0,0.0,0.0,499988.62,502.37559999999996,22.413736859345878,19,19
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.4560613465309142,0.0032180587683265627,0.05672793640109398,999.0,0.0,0.0,440925.68,3371759.7376000006,1836.2352075918816,19,19
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.5563195371627807,0.001266767495903446,0.035591677340404256,999.0,0.0,0.0,165514.32,19264.1376,138.79530827805382,73,73
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.40378143787384035,0.0006087843201822807,0.024673555077902348,999.0,0.0,0.0,121097.46,18791.0284,137.080372045016,73,73
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.3512723898887634,0.0008855431132823186,0.029758076437873444,999.0,0.0,0.0,106045.1,26322.43,162.24188731643872,73,73
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.3550226879119873,0.00046451252339088563,0.021552552595710924,999.0,0.0,0.0,103286.04,23965.998399999997,154.8095552606492,73,73
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.3540023708343506,0.0002936663551069159,0.01713669615494527,999.0,0.0,0.0,103368.36,22147.470399999995,148.8202620613201,73,73
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.6691528677940368,0.007473180683877416,0.08644756031188744,999.0,0.0,0.0,500000.0,0.0,0.0,19,19
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.3488420748710632,0.0004607268971156997,0.021464549776682943,999.0,0.0,0.0,103352.86,32018.5004,178.93714091825655,73,73
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.6197516798973084,0.001338907570953296,0.036591085949357886,999.0,0.0,0.0,184776.3,26428.509999999995,162.56847787932318,73,73
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.4498174858093262,0.005720548262845658,0.07563430612391217,999.0,0.0,0.0,435549.38,8992371.7356,2998.7283530856876,19,19
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.34858466625213624,0.0004754021163260177,0.021803717947313887,999.0,0.0,0.0,103256.1,32700.11,180.83171735069044,73,73
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.3503423762321472,0.0004360098383679712,0.020880848602678272,999.0,0.0,0.0,103383.36,29602.79040000001,172.0546145850207,73,73
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.3516057777404785,0.0007310616143484821,0.02703815108968219,999.0,0.0,0.0,103344.02,26350.1996,162.32744561533642,73,73
No-discounting Q-learning,True,False,0.3571325397491455,0.0004870484544338978,0.02206917430340106,999.0,0.0,0.0,103194.14,24857.460399999996,157.6624888805197,73,73
"No-discounting, no stochastic approximation Q-learning",True,False,0.3498358964920044,0.0003914386579668872,0.01978480876750865,999.0,0.0,0.0,102395.36,35998.31040000001,189.73220707091352,73,73
"cost-based  Q-learning (alpha = 0.001, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.697024290561676,0.0022115780890235954,0.04702741848138802,999.0,0.0,0.0,499996.3,192.10999999999996,13.860375175297383,2,6
"cost-based  Q-learning (alpha = 0.01, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.7192027258872986,0.003013811175856807,0.05489818918558978,999.0,0.0,0.0,499977.26,1727.0124000000003,41.55733870208727,12,27
"cost-based  Q-learning (alpha = 0.01, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.712452871799469,0.0031664168532304932,0.05627092369270735,999.0,0.0,0.0,499991.46,540.6284,23.251417161110847,10,17
"cost-based  Q-learning (alpha = 0.1, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.7135993218421937,0.003639101008636197,0.06032496173754441,999.0,0.0,0.0,499974.64,1401.8303999999998,37.441025626977684,19,19
"cost-based  Q-learning (alpha = 0.3, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.704989721775055,0.004991735690929653,0.07065221646154955,999.0,0.0,0.0,499982.52,1247.9696000000001,35.326613197418176,19,19
"cost-based  Q-learning (alpha = 0.3, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.4793563079833985,0.003729114379479688,0.061066475086414544,999.0,0.0,0.0,441129.98,3113935.2396000004,1764.6345909564395,19,19
"cost-based  Q-learning (alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.5578406167030334,0.000908078156465598,0.030134335175437302,999.0,0.0,0.0,165524.82,25824.0876,160.69874797272067,73,73
"cost-based  Q-learning (alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.4170312237739563,0.0005653040952152595,0.02377612447846073,999.0,0.0,0.0,121102.84,30099.294399999995,173.49148221166362,73,73
"cost-based  Q-learning (alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.35718339681625366,0.0003904173020082169,0.019758980287662035,999.0,0.0,0.0,106054.12,25380.225600000005,159.31172461561016,73,73
"cost-based  Q-learning (alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.35746891021728516,0.0004521425066367101,0.021263642835523507,999.0,0.0,0.0,103282.36,25583.07039999999,159.94708625042216,73,73
"cost-based  Q-learning (alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.3485212540626526,0.0002897225737855195,0.01702123890278024,999.0,0.0,0.0,103351.8,28506.200000000004,168.83779197798106,73,73
"cost-based  Q-learning (alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.6307127213478089,0.0010082697278993409,0.03175326326378662,999.0,0.0,0.0,184770.18,19138.2476,138.3410553667999,73,73
"cost-based  Q-learning (alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",False,True,1.469901831150055,0.004436806216567282,0.06660935532316224,999.0,0.0,0.0,435598.04,8345703.678400001,2888.893158010521,19,19
"cost-based  Q-learning (alpha = 0.999, gamma = 0.1 and termination goal, initial values = 0)",False,True,1.7016193127632142,0.005925566991455407,0.07697770450887326,999.0,0.0,0.0,499798.7,12721.71,112.79055811547347,19,19
"cost-based  Q-learning (alpha = 0.999, gamma = 0.01 and termination goal, initial values = 0)",False,True,1.7197523379325867,0.003415193068276375,0.058439653218310385,999.0,0.0,0.0,499993.98,653.0795999999997,25.555422125255525,19,19
"cost-based  Q-learning (alpha = 0.999, gamma = 0.001 and termination goal, initial values = 0)",False,True,1.6988563060760498,0.005333050739478723,0.07302773952053235,999.0,0.0,0.0,499999.96,0.15839999999999993,0.3979949748426479,19,19
"cost-based  Q-learning (alpha = 0.999, gamma = 0.0001 and termination goal, initial values = 0)",False,True,1.7111971855163575,0.004670719571352037,0.06834266289333506,999.0,0.0,0.0,499999.62,14.295600000000002,3.7809522610051562,19,19
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.72749107837677,0.006316661653570033,0.07947742857925157,999.0,0.0,0.0,500000.0,0.0,0.0,19,20
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.35650202989578245,0.0004952522633526485,0.02225426393643808,999.0,0.0,0.0,103389.58,28944.5036,170.13084258887335,73,73
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.34213297605514525,0.00045184191608574906,0.02125657347941453,999.0,0.0,0.0,103252.94,25514.7564,159.7333916249198,73,73
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.3444122505187988,0.00040086857183709983,0.020021702520942115,999.0,0.0,0.0,103395.88,22954.705600000005,151.50810407367655,73,73
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.3559937596321106,0.00045162622151494246,0.02125149927687321,999.0,0.0,0.0,103379.38,27673.815599999987,166.35448776633586,73,73
cost-based no-discounting Q-learning,True,False,0.3575832748413086,0.0004624021555058789,0.021503538208998977,999.0,0.0,0.0,103183.12,25867.385599999994,160.83340946457608,73,73
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.3585409188270569,0.000411683484746726,0.020289984838504093,999.0,0.0,0.0,102415.34,27406.484400000005,165.54903926027478,73,73
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.3555754661560059,0.0004840388229558812,0.02200088232221338,999.0,0.0,0.0,102391.08,30591.953600000004,174.9055562296407,73,73
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.6468274593353271,0.006977133037181602,0.0835292346258578,999.0,0.0,0.0,500000.0,0.0,0.0,73,73
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.716343343257904,0.005626335470383737,0.07500890260751544,999.0,0.0,0.0,500000.0,0.0,0.0,2,16
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,8.878984293937684,0.09206896750853982,0.3034286860343626,999.0,0.0,0.0,2954434.58,50197550.26360001,7085.022954345314,73,73
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,10.904655034542083,0.11899004857606413,0.3449493420432399,999.0,0.0,0.0,2956029.58,19400564.903600004,4404.60723602003,73,73
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.31031481027603147,0.0003138128342747848,0.01771476317298046,999.0,0.0,0.0,92530.0,0.0,0.0,73,73
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1236089062690735,0.00308971494750387,0.05558520439383011,0.0,0.0,0.0,400000.0,0.0,0.0,73,73
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.11137068271636963,7.281281477105494e-05,0.008533042527203,169.0,0.0,0.0,32770.0,0.0,0.0,73,73
Don't care Q-learning,True,False,6.39983743429184,0.29689145126427363,0.5448774644489104,14999.0,0.0,0.0,2093664.29,33224644218.6859,182276.28539852874,73,91
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",False,True,0.7836878538131714,0.002994292604888074,0.05472012979597247,999.0,0.0,0.0,192658.7,21492382.97,4635.987809518054,115,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.26211938619613645,0.009135541283236416,0.0955800255452802,58.57,476.92510000000004,21.83861488281709,26018.1,43861333.10999999,6622.788922349858,499,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.7965763068199158,0.002698577529588073,0.05194783469585689,999.0,0.0,0.0,194245.71,21620281.765899997,4649.7614740866,94,100002
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.8644051766395568,0.001958790461392363,0.044258224788081627,999.0,0.0,0.0,212998.26,389775.07239999995,624.3196876600961,77,280
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",False,True,0.5518830561637879,0.006204483262617596,0.078768542341582,953.32,39650.357599999996,199.12397545248035,134887.14,503733042.58039993,22443.997918828987,79,100002
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",False,True,0.627094144821167,0.0018343686299323054,0.04282952988222385,999.0,0.0,0.0,155223.95,3393706.8275000006,1842.2016250942786,81,100002
Value Iteration,True,False,0.014170246124267578,3.617555249547877e-07,0.000601461158309319,31.0,0.0,0.0,20522.0,0.0,0.0,73,73
Discounted Value Iteration - gamma = 0.8,True,False,0.01881016492843628,6.213928237883692e-06,0.002492775208053003,31.0,0.0,0.0,20522.0,0.0,0.0,73,73
Discounted Value Iteration - gamma = 0.6,True,False,0.0181805157661438,4.132903571752421e-06,0.002032954394902262,31.0,0.0,0.0,20522.0,0.0,0.0,73,73
Discounted Value Iteration - gamma = 0.5,False,True,0.020329694747924804,6.481338499133926e-06,0.0025458473047561053,32.0,0.0,0.0,21184.0,0.0,0.0,19,19
Discounted Value Iteration - gamma = 0.3,False,True,0.020160348415374757,1.2562513686197006e-05,0.0035443636503887416,32.0,0.0,0.0,21184.0,0.0,0.0,19,19
Discounted Value Iteration - gamma = 0.1,False,True,0.012520520687103272,8.09305268666094e-07,0.0008996139553531248,20.0,0.0,0.0,13240.0,0.0,0.0,18,18
Discounted Value Iteration - gamma = 0.01,False,True,0.007260076999664307,2.3210040519074934e-07,0.00048176799934278466,13.0,0.0,0.0,8606.0,0.0,0.0,19,19
Discounted Value Iteration - gamma = 0.0001,False,True,0.004950010776519775,1.065355856297856e-06,0.0010321607705671902,8.0,0.0,0.0,5296.0,0.0,0.0,19,19
Discounted Value Iteration - gamma = 0.00001,False,True,0.0032799601554870605,2.015054376158787e-07,0.00044889357047732224,6.0,0.0,0.0,3972.0,0.0,0.0,19,19
Discounted Value Iteration - gamma = 0.000001,False,True,0.0039599061012268065,1.2178984665922598e-06,0.0011035843722127729,6.0,0.0,0.0,3972.0,0.0,0.0,19,19
Stochastic Value Iteration,True,True,0.21546146631240845,0.00030247922947739306,0.01739193001013381,124.0,0.0,0.0,185256.0,0.0,0.0,76,99
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.16801119089126587,0.00014746428809245913,0.012143487476522514,99.0,0.0,0.0,147906.0,0.0,0.0,75,93
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.1318810200691223,0.00010535583805914825,0.010264299199611644,77.0,0.0,0.0,115038.0,0.0,0.0,74,95
Random Action Value Iteration,True,False,0.10145568370819091,0.00014578754632109392,0.012074251377252916,232.0,576.0,24.0,50095.0,26625600.0,5160.0,73,73
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.12097671508789062,0.00025670034191680314,0.01602187073711441,228.0,616.0,24.819347291981714,49235.0,28474600.0,5336.159667776068,73,73
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.1169729208946228,0.0004210106841963978,0.020518544884966815,221.0,709.0,26.627053911388696,47730.0,32773525.0,5724.81659094857,73,73
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.10544058799743652,0.00015451603902679378,0.012430448062189623,197.0,341.0,18.466185312619388,42570.0,15762725.0,3970.2298422131685,2,13
Random Action Discounted Value Iteration - gamma = 0.5,False,True,0.10629110097885132,0.00016181782992782132,0.012720763732096485,199.5,424.75,20.60946384552495,43107.5,19634068.75,4431.034726787864,2,18
Q-factor Value Iteration,True,False,0.019840047359466553,6.473632255045913e-06,0.0025443333616186996,31.0,0.0,0.0,20522.0,0.0,0.0,73,73
Q-factor Stochastic Value Iteration,True,True,0.2286010789871216,0.0002250412289767837,0.015001374236275278,124.0,0.0,0.0,185256.0,0.0,0.0,76,97
Model-free Dijkstra,True,False,0.0017664647102355957,3.7175375848050867e-07,0.0006097161294245944,213.0,0.0,0.0,656.0,0.0,0.0,73,73
