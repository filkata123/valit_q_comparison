Algorithm,Avg Time,Shortest Path,Inconsistent?,Min Path,Max Path
"Normal Q-learning (reward, discounting, stochastic approximation and termination goal)",0.43237224817276,91,False,91,91
No-discounting Q-learning,0.4469246697425842,91,False,91,91
"No-discounting, no stochastic approximation Q-learning",0.4688239884376526,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation)",0.47441706180572507,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",0.45286438703536985,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",1.6300118398666381,91,False,91,91
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",1.6393500971794128,0,False,0,0
Fully-random exploration Q-learning,8.358914251327514,91,True,0,91
Fully-greedy exploration Q-learning,0.42544243335723875,91,False,91,91
One-episode random-exploration Q-learning,1.0996609592437745,91,True,0,91
Fully-greedy Q-learning with convergence,0.18231068849563598,91,False,91,91
Don't care Q-learning,11.724980835914613,107,True,101,99
Stochastic Q-learning (converging),0.19741028308868408,98,True,100,99
Value Iteration,0.032920637130737306,91,False,91,91
Random Action Value Iteration,0.15904734611511231,91,False,91,91
Stochastic Value Iteration,0.3488649463653564,105,True,100,99
