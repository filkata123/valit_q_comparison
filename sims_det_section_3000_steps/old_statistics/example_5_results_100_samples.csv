Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Avg convergence action,Var convergence actio ,STD convergence action,Convergence rate,Shortest Path,Longest Path
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0",True,False,0.16611113548278808,0.00033621366428405965,0.018336130024736944,999.0,0.0,0.0,42386.0,0.0,0.0,0.0,0.0,0.0,0.0,31,31
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.25",True,False,0.216821870803833,0.0004036003017480653,0.0200898059161373,999.0,0.0,0.0,56083.2,74818.0,273.528791903156,0.0,0.0,0.0,0.0,31,31
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.5",True,False,0.31991158962249755,0.0008317387608297167,0.028839881428842883,999.0,0.0,0.0,84382.7,442449.87,665.1690536998847,0.0,0.0,0.0,0.0,31,31
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.75",True,False,0.6815619373321533,0.002011601384380038,0.044850879415904855,999.0,0.0,0.0,184741.88,5173605.2656000005,2274.5560590145938,0.0,0.0,0.0,0.0,31,31
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.9",True,False,1.1848943305015565,0.11246906975312693,0.335364085365632,592.9,30524.210000000006,174.7117912448957,314406.6,7522457583.48,86732.10238129823,312121.2121212121,7224288337.9247,84995.81364940687,0.99,31,31
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.5859952425956726,0.016610272436866688,0.12888084588823387,109.57,477.54510000000005,21.852805311904465,174512.14,1160561278.3404,34067.01158511559,173500.0,1155250000.0,33988.96879871468,1.0,31,31
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.6364604449272155,0.02699151011759579,0.16429093133096478,112.24,843.9024000000001,29.049998278829555,180433.06,2057951724.9964,45364.65281467941,179350.0,2040327500.0,45169.98450298605,1.0,31,31
Model-free Dijkstra,True,False,0.002411308288574219,9.542175798878816e-06,0.003089041242663946,238.0,0.0,0.0,3615.0,0.0,0.0,836.0,0.0,0.0,1.0,31,31
Model-free Value Iteration,True,False,0.015384671688079833,2.9672060117599128e-05,0.005447206634376846,20.0,0.0,0.0,23059.0,0.0,0.0,20280.0,0.0,0.0,1.0,31,31
Model-free Synchronous Value Iteration,True,False,0.023075098991394042,1.066684383088159e-05,0.0032660134462187368,33.0,0.0,0.0,36241.0,0.0,0.0,33462.0,0.0,0.0,1.0,31,31