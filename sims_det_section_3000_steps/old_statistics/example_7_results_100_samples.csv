Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Avg convergence action,Var convergence actio ,STD convergence action,Convergence rate,Shortest Path,Longest Path
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0",True,False,0.14499294519424438,0.0003116374883182913,0.017653257158901054,999.0,0.0,0.0,37478.0,0.0,0.0,0.0,0.0,0.0,0.0,29,29
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.25",True,False,0.19013260126113893,0.00039949480410797944,0.01998736611232154,999.0,0.0,0.0,49706.3,77780.51,278.891573913591,0.0,0.0,0.0,0.0,29,29
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.5",True,False,0.2866607165336609,0.0010750289412495648,0.032787633968457755,999.0,0.0,0.0,76018.08,395918.7935999999,629.2207828735475,0.0,0.0,0.0,0.0,29,29
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.75",True,False,0.6024996161460876,0.0012816047768062333,0.03579950805257292,999.0,0.0,0.0,169470.92,4356731.8736000005,2087.27858073617,0.0,0.0,0.0,0.0,29,29
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.9",True,False,1.7970860362052918,0.020736170433070106,0.14400059178027744,967.18,4759.4476,68.98874980748673,504429.66,1255697452.9644003,35435.82160701795,468500.0,2178583333.3333335,46675.29682105228,0.3,29,29
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,2.188181409835815,0.2503399193849315,0.5003398039182286,328.95,5228.8475,72.31077029046226,654295.78,21287434282.0716,145902.13940196903,653050.0,21254947500.0,145790.76616850603,1.0,29,29
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,2.340233416557312,0.5577149039063167,0.7468031225874171,333.63,10635.933100000002,103.13066032950628,659290.64,43955977423.91039,209656.8086752977,657500.0,43924250000.0,209581.12987575957,1.0,29,29
Model-free Dijkstra,True,False,0.002700965404510498,1.0939635633411626e-05,0.0033075120004939706,165.0,0.0,0.0,3856.0,0.0,0.0,579.0,0.0,0.0,1.0,29,29
Model-free Value Iteration,True,False,0.015599853992462158,1.8198448596484694e-05,0.004265963970368795,20.0,0.0,0.0,26437.0,0.0,0.0,23160.0,0.0,0.0,1.0,29,29
Model-free Synchronous Value Iteration,True,False,0.02765266418457031,1.5080097947338802e-05,0.0038833101791305316,34.0,0.0,0.0,42649.0,0.0,0.0,39372.0,0.0,0.0,1.0,29,29
