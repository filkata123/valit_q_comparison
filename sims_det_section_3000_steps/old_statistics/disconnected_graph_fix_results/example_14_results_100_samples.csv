Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Avg convergence action,Var convergence actio ,STD convergence action,Convergence rate,Shortest Path,Longest Path
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0",True,False,0.15851722717285155,0.0007267022473505674,0.02695741544270458,999.0,0.0,0.0,36008.0,0.0,0.0,0.0,0.0,0.0,0.0,24,24
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.25",True,False,0.20805116176605223,0.0008466013557283305,0.02909641482602849,999.0,0.0,0.0,48336.44,44603.1464,211.19457000595446,0.0,0.0,0.0,0.0,24,24
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.5",True,False,0.3215399956703186,0.001944122292509854,0.04409220217351197,999.0,0.0,0.0,72930.51,251563.12990000003,501.5606941338207,0.0,0.0,0.0,0.0,24,24
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.75",True,False,0.6204680442810059,0.004636824169332385,0.0680942300737176,999.0,0.0,0.0,155878.9,2870555.45,1694.27136256268,0.0,0.0,0.0,0.0,24,24
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.9",True,False,0.6317637538909913,0.022714002437538877,0.1507116532904436,358.77,7417.077099999999,86.1224540988005,164896.37,1184442764.2131,34415.73425358088,164600.0,1185840000.0,34436.027645476184,1.0,24,24
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.4093886160850525,0.005145161566348058,0.07172978158581035,70.99,107.14989999999997,10.351323586865593,110731.0,161074559.52,12691.515257052642,109700.0,162410000.0,12744.01820463232,1.0,24,24
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.4182737421989441,0.00508566621016127,0.07131385706972573,70.81,83.53389999999999,9.139688178488367,110997.83,144431786.1211,12017.977621925413,110000.0,144500000.0,12020.815280171308,1.0,24,24
Model-free Dijkstra,True,False,0.0029130125045776366,2.8833358119095464e-05,0.0053696702058036545,278.0,0.0,0.0,3711.0,0.0,0.0,879.0,0.0,0.0,1.0,24,24
Model-free Value Iteration,True,False,0.02475341558456421,5.887345899438401e-05,0.0076729042086021125,26.0,0.0,0.0,30808.0,0.0,0.0,27976.0,0.0,0.0,1.0,24,24
Model-free Synchronous Value Iteration,True,False,0.024565706253051756,4.467662638821823e-05,0.006684057629031802,30.0,0.0,0.0,35112.0,0.0,0.0,32280.0,0.0,0.0,1.0,24,24
