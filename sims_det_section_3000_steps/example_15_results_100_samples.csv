Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Avg convergence action,Var convergence actio ,STD convergence action,Convergence rate,Shortest Path,Longest Path
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0",True,False,0.14585695505142213,6.451624624116334e-05,0.008032200585217189,999.0,0.0,0.0,33372.0,0.0,0.0,0.0,0.0,0.0,0.0,22,22
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.25",True,False,0.18605640888214112,0.00017589436392997872,0.013262517254653383,999.0,0.0,0.0,43939.13,70200.91309999999,264.9545491211653,0.0,0.0,0.0,0.0,22,22
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.5",True,False,0.264676673412323,0.0005166049717436807,0.02272894568042435,999.0,0.0,0.0,64695.34,224847.82440000004,474.1812147270282,0.0,0.0,0.0,0.0,22,22
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.75",True,False,0.5066776251792908,0.00034737915447805685,0.01863811027111002,999.0,0.0,0.0,130567.31,2696707.9539,1642.16562925303,0.0,0.0,0.0,0.0,22,22
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.9",True,False,1.2013410544395446,0.00730716271280141,0.0854819437823065,998.91,0.8018999999999997,0.8954886933959578,315810.96,38687392.7184,6219.919028283246,320000.0,0.0,0.0,0.01,22,22
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.5150347304344177,0.008635184301688076,0.09292569236593332,123.91,579.2819000000001,24.068275800314407,141415.88,616910754.2656001,24837.688182791895,140500.0,619250000.0,24884.73427625861,1.0,22,22
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.5346207737922668,0.011938849826211,0.10926504393542795,124.99,762.8699,27.620099565352767,143220.82,845691895.1875998,29080.78223135684,142300.0,843710000.0,29046.686558022415,1.0,22,22
Model-free Dijkstra,True,False,0.0032154011726379396,7.127170177398055e-07,0.0008442256912341661,291.0,0.0,0.0,4494.0,0.0,0.0,1009.0,0.0,0.0,1.0,22,22
Model-free Value Iteration,True,False,0.015591812133789063,4.013195390371038e-06,0.002003296131472089,15.0,0.0,0.0,22175.0,0.0,0.0,18690.0,0.0,0.0,1.0,22,22
Model-free Synchronous Value Iteration,True,False,0.025500288009643556,3.361477852185999e-05,0.005797825326953201,29.0,0.0,0.0,39619.0,0.0,0.0,36134.0,0.0,0.0,1.0,22,22

