Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Avg convergence action,Var convergence actio ,STD convergence action,Convergence rate,Shortest Path,Longest Path
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0",True,False,0.14004770040512085,0.00037804530134241644,0.019443387085135565,999.0,0.0,0.0,32774.0,0.0,0.0,0.0,0.0,0.0,0.0,18,18
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.25",True,False,0.19033058643341064,0.0004966370243758093,0.02228535448171757,999.0,0.0,0.0,45240.89,79693.27790000002,282.29997856889753,0.0,0.0,0.0,0.0,18,18
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.5",True,False,0.31721922636032107,0.002358318571012495,0.04856252228841182,999.0,0.0,0.0,72474.56,417176.98639999994,645.8923953724799,0.0,0.0,0.0,0.0,18,18
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.75",True,False,0.7597184443473816,0.007400556612352481,0.08602648785317508,999.0,0.0,0.0,180400.35,8306132.7475,2882.036215508056,0.0,0.0,0.0,0.0,18,18
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 0.9",True,False,1.1937046122550965,0.0514304034904506,0.22678272308632905,549.08,10886.653600000003,104.33912784761047,307043.28,2937877556.8616,54202.19143966044,306650.0,2937527500.0,54198.962167185455,1.0,18,18
"cost-based Q-learning true convergence (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.5508868408203125,0.007555475131422645,0.08692223611609773,108.48,152.6696,12.355954030345046,142120.43,173942394.00509998,13188.72222791503,141150.0,167927500.0,12958.684346800026,1.0,18,18
"Fully-random (deterministic with pi) exploration Q-learning (No discounting, no stochastic approximation) w/ term action & term goal, epsilon = 1",True,False,0.5402889966964721,0.007049749343584039,0.08396278546823015,105.93,117.1451,10.823358998019053,136706.64,102489772.15040001,10123.723235569018,135850.0,103027500.0,10150.246302430302,1.0,18,18
Model-free Dijkstra,True,False,0.003590099811553955,1.3649401982382869e-05,0.003694509707983303,279.0,0.0,0.0,5368.0,0.0,0.0,1074.0,0.0,0.0,1.0,18,18
Model-free Value Iteration,True,False,0.022727739810943604,4.259014567466579e-05,0.00652611260051999,18.0,0.0,0.0,31654.0,0.0,0.0,27360.0,0.0,0.0,1.0,18,18
Model-free Synchronous Value Iteration,True,False,0.029842121601104735,6.516895048497418e-06,0.002552820998130777,29.0,0.0,0.0,48374.0,0.0,0.0,44080.0,0.0,0.0,1.0,18,18
