Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",False,True,0.3099598264694214,0.0005879098568773543,0.02424685251485962,999.0,0.0,0.0,88947.55,32555.96749999999,180.43272291909798,10,32
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.21793325901031493,0.0001804042828134925,0.0134314661453429,999.0,0.0,0.0,60735.72,32402.5216,180.00700430816573,32,32
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.18162195205688478,0.00010337237219787312,0.010167220475521967,999.0,0.0,0.0,51389.43,30958.525100000003,175.95034839408532,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.17127135276794433,0.00014108568403717073,0.011877949487902814,999.0,0.0,0.0,50018.72,39651.5816,199.1270488909028,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.17544145822525026,9.282474265966698e-05,0.00963455980622192,999.0,0.0,0.0,50050.37,27201.333100000003,164.92826652820918,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5970867919921874,0.005032247109956506,0.07093833314898586,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.17125126361846923,0.0001369230883438149,0.011701413946349172,999.0,0.0,0.0,50035.63,40723.9331,201.8017172870439,32,32
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.3634588623046875,0.00022984198333679158,0.015160540337890058,999.0,0.0,0.0,100294.34,29306.1444,171.19037472942222,3,36
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.1758412480354309,0.00010387815831947476,0.01019206349663672,999.0,0.0,0.0,50029.7,30163.390000000003,173.6761065892485,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.17614654779434205,0.00013444787890654766,0.011595166187103472,999.0,0.0,0.0,50057.36,38403.610400000005,195.9683913288059,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.17781131982803344,0.00014973306328426475,0.012236546215508065,999.0,0.0,0.0,50058.94,32789.736399999994,181.07936492046792,32,32
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.15972371101379396,6.917967544950443e-06,0.002630202947483415,999.0,0.0,0.0,50070.98,36014.6196,189.77518172827553,32,32
No-discounting Q-learning,True,False,0.17538842916488648,0.00014632897080320504,0.012096651222681632,999.0,0.0,0.0,50029.54,32122.748399999997,179.22820202189163,32,32
"No-discounting, no stochastic approximation Q-learning",True,False,0.1823742699623108,9.996749812518715e-05,0.009998374774191411,999.0,0.0,0.0,50537.95,32979.78749999999,181.6033796491684,32,32
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.6126308560371398,0.005224134693435616,0.07227817577551066,999.0,0.0,0.0,500000.0,0.0,0.0,3,3
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.17573795080184937,0.00015447281400030876,0.012428709265257947,999.0,0.0,0.0,50073.36,29264.5704,171.06890541533258,32,32
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.17815104722976685,0.00013932048716398525,0.011803409980339801,999.0,0.0,0.0,50073.2,36117.08,190.0449420531891,32,32
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.177952401638031,0.00012721020889381977,0.01127875032500586,999.0,0.0,0.0,50084.01,38680.1699,196.67274823930234,32,32
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.18091120958328247,0.000107821894456373,0.010383732202651078,999.0,0.0,0.0,50082.98,29725.4596,172.41072936450328,32,32
cost-based no-discounting Q-learning,True,False,0.17444199323654175,0.00016417451558638164,0.012813060352093158,999.0,0.0,0.0,50050.1,38354.35,195.84266644426592,32,32
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.18106175184249879,0.00017019790345268001,0.013045991853925097,999.0,0.0,0.0,50589.47,35202.38910000001,187.6229972577989,32,32
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.18389471054077147,0.0001028931493145592,0.010143626043706421,999.0,0.0,0.0,50583.11,31498.2379,177.47742926918903,32,32
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.9004796528816223,0.004372333808361299,0.0661236251907085,999.0,0.0,0.0,500000.0,0.0,0.0,32,32
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7132649111747742,0.0082892156639328,0.09104512981995687,999.0,0.0,0.0,500000.0,0.0,0.0,2,13
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,5.820675036907196,0.07167840598004444,0.26772823157083087,999.0,0.0,0.0,1943243.21,846602639.0259001,29096.436878523462,32,32
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.15871142387390136,0.00013521229203311121,0.01162808204447798,999.0,0.0,0.0,45868.0,0.0,0.0,32,32
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1883616852760315,0.004271383638987407,0.06535582329821427,0.0,0.0,0.0,400000.0,0.0,0.0,32,32
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.07236137866973877,1.3635994253922944e-05,0.003692694714422375,222.0,0.0,0.0,21781.0,0.0,0.0,32,32
Don't care Q-learning,True,False,1.7988091397285462,0.021112374959858075,0.14530098058808164,14999.0,0.0,0.0,557091.02,985837124.8596,31398.043328519692,32,38
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.3427822494506836,0.0005930385301984643,0.024352382433726363,999.0,0.0,0.0,79179.5,3691060.75,1921.2133535867379,34,3156
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",False,True,0.3249918293952942,0.0017327306789052899,0.04162608171453674,952.74,26245.012400000003,162.00312466122375,77797.69,101374504.1139,10068.490657188891,33,100002
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.3367717671394348,0.0006357053056229404,0.025213197052792423,999.0,0.0,0.0,79980.61,2968661.1179000004,1722.9803010771773,34,4394
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",False,True,0.48502799987792966,0.001250639527610565,0.03536438218901279,999.0,0.0,0.0,113688.9,257963.35,507.9009253781686,32,100002
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.25973235607147216,0.00020371993462210867,0.014273049240512998,999.0,0.0,0.0,64952.62,145842.41559999998,381.8931992062702,32,111
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.2747212600708008,0.00024851454698418817,0.015764344166002853,999.0,0.0,0.0,64493.1,738876.39,859.5791935592671,32,6115
Value Iteration,True,False,0.024008159637451173,9.067860952973207e-06,0.003011288918880619,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.8,True,False,0.02784958839416504,7.770309387660744e-06,0.002787527468503359,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.6,True,False,0.027078726291656495,1.070904954428329e-05,0.0032724684176143382,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Discounted Value Iteration - gamma = 0.5,True,False,0.02601881980895996,3.309601895261949e-06,0.001819231127499183,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Stochastic Value Iteration,True,True,0.29911208629608155,0.0003401103067755911,0.01844207978443839,108.0,0.0,0.0,287496.0,0.0,0.0,32,47
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.256356987953186,0.0002484133684947892,0.01576113474641941,92.0,0.0,0.0,244904.0,0.0,0.0,32,44
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.21342153549194337,0.00031739200160927787,0.017815498915530765,77.0,0.0,0.0,204974.0,0.0,0.0,32,47
Random Action Value Iteration,True,False,0.09369072437286377,5.0193226076976325e-05,0.0070847177838624115,151.0,49.0,7.0,45448.0,4380649.0,2093.0,32,32
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.10530372858047485,8.969950761681391e-05,0.00947098239977321,150.5,24.75,4.9749371855331,45298.5,2212674.75,1487.5062184743967,32,32
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.10827148199081421,6.974459180184452e-05,0.008351322757614181,151.0,49.0,7.0,45448.0,4380649.0,2093.0,32,32
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.1111406946182251,9.216507073349477e-05,0.009600264097070182,153.0,141.0,11.874342087037917,46046.0,12605541.0,3550.4282840243372,32,32
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.10620099544525147,0.00010877593728380363,0.010429570330737678,152.0,96.0,9.797958971132712,45747.0,8582496.0,2929.589732368681,32,32
Q-factor Value Iteration,True,False,0.030780220031738283,5.915440361832225e-06,0.0024321678317567284,34.0,0.0,0.0,35020.0,0.0,0.0,32,32
Q-factor Stochastic Value Iteration,True,True,0.33294817209243777,0.0007572334301055151,0.02751787473816819,108.0,0.0,0.0,287496.0,0.0,0.0,32,43
