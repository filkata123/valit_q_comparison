Algorithm,Goal reached,Loops encountered,Avg Time,Var Time,STD Time,Avg Iterations,Var Iterations,STD iterations,Avg action count,Var action count,STD action count,Shortest Path,Longest Path
"Normal Q-learning (reward, alpha = 0.3, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.2583369469642639,0.00032766743204512637,0.018101586451057996,999.0,0.0,0.0,77595.32,19911.057600000004,141.10654697780683,31,33
"Normal Q-learning (reward, alpha = 0.6, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.191421320438385,0.00011805057609000756,0.010865108195043783,999.0,0.0,0.0,54723.42,20455.783600000002,143.02371691436355,31,31
"Normal Q-learning (reward, alpha = 0.9, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16787848711013795,0.00014241182462466781,0.01193364255475535,999.0,0.0,0.0,46976.28,21281.5216,145.88187550206501,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.6 and termination goal, initial values = 0)",True,False,0.16177138328552246,0.0001328508671025702,0.01152609505004059,999.0,0.0,0.0,45897.24,18828.3024,137.21626142698977,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = 0)",True,False,0.16379663944244385,0.00010915624615392971,0.010447786662921947,999.0,0.0,0.0,45906.18,18872.1676,137.3760080945723,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = -1e4)",False,True,1.5874560427665712,0.004745759061459984,0.06888946988807494,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.15436097383499145,0.00010907418960002247,0.010443858941982244,999.0,0.0,0.0,46358.96,22647.7184,150.4915891337453,31,31
"Normal Q-learning (reward, alpha = 0.2, gamma = 0.999 and termination goal, initial values = 0)",False,True,0.29783196926116945,0.00038161912221696645,0.019535074154375928,999.0,0.0,0.0,86378.78,21073.831599999998,145.16828717044228,2,35
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.5 and termination goal, initial values = 0)",True,False,0.15246137857437134,8.071895746031146e-05,0.008984372958660579,999.0,0.0,0.0,45891.12,23157.7056,152.17656061299323,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.16902664422988892,4.5627997538593946e-05,0.006754849927170399,999.0,0.0,0.0,46281.68,21636.137600000005,147.09227579992094,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.1485314154624939,6.3786412534852845e-06,0.0025255972072928184,999.0,0.0,0.0,46335.56,16172.206400000001,127.16999017063735,31,31
"Normal Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e7)",True,False,0.16836159706115722,6.218105978755375e-05,0.007885496800300774,999.0,0.0,0.0,46371.76,24458.9824,156.39367762157138,31,31
No-discounting Q-learning,True,False,0.16183073282241822,0.00010742115296158661,0.010364417637358436,999.0,0.0,0.0,46400.2,25765.080000000005,160.51504602372952,31,31
"No-discounting, no stochastic approximation Q-learning",True,False,0.16491106748580933,0.00011557721750293124,0.010750684513226645,999.0,0.0,0.0,46759.24,26489.662399999997,162.75645117782582,31,31
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.9 and termination goal, initial values = +1e4)",False,True,1.5935091304779052,0.007460271021419748,0.08637286044481651,999.0,0.0,0.0,500000.0,0.0,0.0,5,5
"cost-based  Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = +1e4)",True,False,0.15828112840652467,0.00010732561464130299,0.010359807654647986,999.0,0.0,0.0,46370.02,16733.399599999997,129.35764221722656,31,31
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = 0)",True,False,0.1625470209121704,0.0001279293633089992,0.011310586337984393,999.0,0.0,0.0,46251.34,25443.9244,159.51151807941645,31,31
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e4)",True,False,0.16844231605529786,0.0002381907237862379,0.015433428776076879,999.0,0.0,0.0,46340.56,21134.566399999996,145.37732422905574,31,31
"cost-based Q-learning (reward, alpha = 0.999, gamma = 0.999 and termination goal, initial values = -1e7)",True,False,0.16429107904434204,0.00011961715025774423,0.010936962570007461,999.0,0.0,0.0,46362.58,21912.743599999998,148.02953624192708,31,31
cost-based no-discounting Q-learning,True,False,0.17142107963562012,9.567796312726384e-05,0.009781511290555454,999.0,0.0,0.0,46351.72,15650.6416,125.1025243550265,31,31
"cost-based Q-learning (No discounting, no stochastic approximation)",True,False,0.16729120969772338,0.00013080913777423008,0.01143718224801153,999.0,0.0,0.0,46725.98,23811.4796,154.3096873174202,31,31
"cost-based Q-learning (No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.1706312608718872,0.00010065118275722396,0.010032506304868388,999.0,0.0,0.0,46729.4,23420.999999999996,153.03921066184313,31,31
"cost-based Q-learning (No discounting, no stochastic approximation, no term goal) w/ term action",True,False,1.8701869201660157,0.008790479204761231,0.09375755545427382,999.0,0.0,0.0,500000.0,0.0,0.0,31,31
"cost-based Q-learning (No discounting, no stochastic approximation, no termination at all)",False,True,1.7262860655784606,0.0069485475127810045,0.08335794810802989,999.0,0.0,0.0,500000.0,0.0,0.0,2,14
"Fully-random exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,4.756113083362579,0.04968291980181295,0.22289665722440288,999.0,0.0,0.0,1583984.54,887726034.5484002,29794.731657600143,31,31
"Fully-greedy exploration Q-learning(No discounting, no stochastic approximation) w/ term action & term goal",True,False,0.14474086284637452,7.483583270116014e-05,0.008650770642038785,999.0,0.0,0.0,42170.0,0.0,0.0,31,31
"One-episode random-exploration Q-learning(No discounting, no stochastic approximation) w/ term action only",True,False,1.1952937865257263,0.004093023944744227,0.06397674534347794,0.0,0.0,0.0,400000.0,0.0,0.0,31,31
"Fully-greedy Q-learning with convergence (No discounting, no stochastic approximation) w/ term action & term goal (best case)",True,False,0.0666503643989563,2.1927113659666015e-05,0.004682639603862977,203.0,0.0,0.0,18290.0,0.0,0.0,31,31
Don't care Q-learning,True,False,1.802631709575653,0.018832693921844904,0.1372322626857289,14999.0,0.0,0.0,535596.47,962700401.6491001,31027.413711895166,31,41
"Stochastic-problem Q-learning (converging, no discounting, no stochastic approximation)",True,True,0.31641206026077273,0.00046080591588481076,0.021466390378561802,999.0,0.0,0.0,74490.16,2334612.1344,1527.9437602215598,32,3096
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.32023274660110473,0.00033542463295872836,0.018314601632542497,999.0,0.0,0.0,73020.58,2150877.7436000006,1466.587107402762,31,2380
"Stochastic-problem Q-learning (converging, no stochastic approximation, gamma = 0.6)",True,True,0.3070241951942444,0.0004661523510261474,0.02159056161905353,999.0,0.0,0.0,73209.75,2583982.9275,1607.4771934618545,32,827
"Stochastic-problem Q-learning (converging, no discounting, alpha = 0.2)",True,True,0.41856908082962035,0.0005593087911784722,0.023649710171130475,999.0,0.0,0.0,98689.53,199426.7891,446.5722663802579,31,650
"Stochastic-problem Q-learning (converging, alpha = 0.7, gamma = 0.7)",True,True,0.2522825074195862,0.00030315329716751755,0.017411297974806977,999.0,0.0,0.0,59813.14,156748.24040000004,395.9144357054944,31,270
"Stochastic-problem Q-learning (converging, alpha = 0.9, gamma = 0.9)",True,True,0.2560517716407776,0.00029106173270458267,0.017060531430895774,999.0,0.0,0.0,60542.95,667080.5875,816.7500153045606,31,2221
Value Iteration,True,False,0.022350826263427735,7.866733086211751e-06,0.0028047697028832424,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.8,True,False,0.025019400119781494,8.144828971643391e-06,0.002853914674905925,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.6,True,False,0.024018981456756593,6.416299346426513e-07,0.0008010180613710601,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Discounted Value Iteration - gamma = 0.5,True,False,0.027230212688446043,6.496707352761177e-06,0.00254886393374797,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Stochastic Value Iteration,True,True,0.2953133845329285,0.0004769902430881472,0.021840106297546888,105.0,0.0,0.0,279090.0,0.0,0.0,31,44
Discounted Stochastic Value Iteration - gamma = 0.8,True,True,0.24526187896728516,0.00048020478349517366,0.02191357532433203,91.0,0.0,0.0,241878.0,0.0,0.0,31,44
Discounted Stochastic Value Iteration - gamma = 0.6,True,True,0.20859833002090455,0.000220084248873826,0.01483523673130382,75.0,0.0,0.0,199350.0,0.0,0.0,31,41
Random Action Value Iteration,True,False,0.09172922849655152,9.12579141195863e-05,0.009552900822241708,154.0,184.0,13.564659966250536,44950.0,15474400.0,3933.7513902126557,31,31
Random Action Discounted Value Iteration - gamma = 0.8,True,False,0.09189131736755371,4.505448127733871e-05,0.006712263498801184,151.0,49.0,7.0,44080.0,4120900.0,2030.0,31,31
Random Action Discounted Value Iteration - gamma = 0.6,True,False,0.09010730743408203,1.9621926033232742e-05,0.004429664325119088,150.0,0.0,0.0,43790.0,0.0,0.0,31,31
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.09656081199645997,4.475333534287528e-05,0.006689793370715965,150.0,0.0,0.0,43790.0,0.0,0.0,31,31
Random Action Discounted Value Iteration - gamma = 0.5,True,False,0.09499100208282471,5.7908841758967354e-05,0.007609785920705481,150.0,0.0,0.0,43790.0,0.0,0.0,31,31
Q-factor Value Iteration,True,False,0.027927544116973877,3.4155065167112752e-06,0.001848108902827773,33.0,0.0,0.0,33462.0,0.0,0.0,31,31
Q-factor Stochastic Value Iteration,True,True,0.30814948558807376,0.0006499965645341263,0.025495030192845942,105.0,0.0,0.0,279090.0,0.0,0.0,31,44
